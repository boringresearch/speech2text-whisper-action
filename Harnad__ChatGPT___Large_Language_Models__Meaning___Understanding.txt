 Okay, sorry for all of this. Welcome to the
 Dramatics Seminar. The theme has more or less become large language models, GPT, and grounding.
 And today I'm going to be talking about the surprising success for all of us
 who admit it, of ChatGPT. There are those of us who have strong reasons for believing that
 ChatGPT is doing what it's doing without understanding. And there are those who believe
 that ChatGPT does it with understanding. And there are reputable people in both sides.
 In fact, the two of the three godfathers of AI, or I forgot what the Turing prize was for,
 Joshua Benjio and Jeffrey Hinton, both believe that ChatGPT understands.
 It's not clear. I think Joshua Benjio can be said to also believe that eventually
 he agrees that it's not grounded now, but eventually it will be grounded. We'll talk
 about what grounding is shortly. It will be grounded by the same means that it uses now
 to negotiate the landscape of words. I have a different view on this. I think that
 ChatGPT doesn't understand, and that the reason it does not understand is because it's not grounded.
 We'll talk about what that means in a moment. And that you cannot get it grounded by the same
 means that you use for its remarkable success. But I do believe that success is remarkable.
 And in order to say with confidence that it doesn't understand, despite what it can do,
 we need some explanation for how it could do so well without grounding. And that's what this talk
 is going to be about. I'm going to give you six, I don't even want to call them hypotheses. I'm going
 to give you six hunches as to how it would be possible for a computer program and a database,
 basically, a computer program and a database. The database is huge. It's distributed across many
 computers all over the planet. And it got its data from a huge, huge, huge text source,
 lots of things from the internet, lots of things from written work, journal articles, books, etc.
 So all those words in context with the words that come with them are stored and part of this entity
 about which we're asking, does it or doesn't it understand, which we have to admit that it
 certainly behaves in many ways as if it does understand remarkably. Sometimes it does stupid
 things, but we do stupid things as well. So even understanding humans do stupid things. So it's not
 clear that it's mistakes which are getting less and less and being corrected better and better
 when there's mistakes are grounds for saying that it does understand and either it doesn't need
 grounding or it already has grounding. These are the questions I'm going to be talking about. I have
 six hunches, not hypotheses, not substantive. Well, I should say not empirical enough yet.
 We call hypotheses, but they're hunches and it'll plant a seed in your mind as to why it is
 remarkable capacity, surprising to anybody who admits it, that how this capacity emerges from
 the means at the disposal of the CHAD GPT and of LLMs and the database that they have.
 It concerns the question of meaning and understanding. So I'll be speaking a little
 bit about language and my answer, my hunches, will take the form that it doesn't understand,
 it's not grounded, but there are at the scale of LLMs biases or convergent, benign biases
 or convergent constraints at that scale, which partly at least explain its remarkable, unexpected
 and rather astounding success. So the first thing I want to remind you is that the original proposal
 by Alan Turing in 1950 was for what we would today call a chat bot that can be interacted with
 just by texting. The test, the so-called Turing test, was if and when you reach the point
 where a computer can generate chat bot capacities that are indistinguishable from those of a real
 human meaning and understanding person. And when I say indistinguishable, I don't mean just for a 10
 minute Loebner Prize, which used to be taking place for about 15 years, to see which computer
 program can fool 75 percent of the judges for 10 minutes. That's not the Turing test. Anyone that
 you have not met in person and not Zoom, anyone whom you have just texted with, nothing else,
 just texted with, is the Turing test for you and for that individual. You are interacting verbally,
 you're interacting interacting by text, if you want to identify, and you cannot tell it apart
 from a real person. If you have no reason to say this is not a real person, on the basis of its
 performance, then you pass the Turing test. And from the point of view of cognitive science,
 it is one, not necessarily the, but it's one of the viable models for how it is that the
 human mind understands as peace and means. That's Turing's criterion and Turing's
 Turing's method and his criterion. The method is verbal only and the criterion is
 well equivalence in performance capacity and verbal performance capacity and indistinguishability
 from a real person. Now the first question to ask is has chat GPT such as it is passed this?
 And the answer is yes and no. It hasn't because it tells you right away I'm not a person.
 So it just can't work because he tells you the truth. You're not a person and he tells you exactly
 what it is that he is. He's a computer program and a huge database and a lot of parameters
 that are stored in many computers all over the planet. So it's not exactly what Turing had in
 mind. It is not one Turing machine, one immense powerful Turing machine that simply has verbal
 capacity. It is in fact many, many, many computers that have not only lots of capacities, statistical
 learning capacities and so on, but it also has a
 interact with everybody about questions that they have about just about anything they would ask
 from a library reference and it's capable of interacting with them. That took everybody by
 surprise and it sounds and feels as if when you talk to it it's understanding what you're saying.
 It doesn't literally pass the original criterion for Turing tests because he tells you that it's
 not. So you're no longer testing whether it's a person. The idea was Turing's intuition was
 if you're interacting with it you're not being tested. You're simply interacting with it
 for a lifetime, not just for one day, 10 days, but in principle for a lifetime and it never crosses
 your mind that it's not another person. You may never have met them in the old days when you had
 a pen pal overseas whom you simply exchange letters with. You wouldn't see them so if they
 keep on responding and interacting with you in a way that you have no reason to suspect that it's
 not a person, then it's passed the test and of course Chachi Pt fails immediately because he
 tells you I'm not a person. But the other thing that you have to bear in mind is that I believe
 that the original Turing paper did not, it was defining computation, it was making predictions
 about how powerful computation is, but it wasn't implying that the candidate can only be a computer.
 I think now I call the ordinary original Turing test P2 is purely verbal. There's another version
 of it that's very similar but that's much more demanding and that requires grounding.
 Could somebody close the door please? It requires grounding and that's T3, the robotic version of
 the Turing. T2 is verbal interaction indistinguishable in its capacities from a
 real human being for a lifetime if necessary. That's the criterion. T3 is also verbal capacities
 indistinguishable for a lifetime, but also the capacity, being a robot, to interact with all
 of the reference of those words. If you say I have a cat sitting in my lap, come and stroke it,
 T3 can do it. That's a trivial example but I may be using it over and over again. The connection
 between the words and the world is made by T3, not by T2. You have to have robotic capacities to
 interact with the reference of your words. We'll be talking about that and we'll come back to the
 distinction between T2 and T3 because the essence of grounding is already contained there. There is,
 by the way, also a T4 and we'll only discuss it in the discussion section if you ask me a question
 about it. Now, yes, T1 is a little T, not a big T. It's not a Turing test because
 one of the criteria of Turing test is total indistinguishability. T1 is a toy. You know
 about these toys, a chess playing program, a scene describing program, Siri, Alexa. These are things
 that are able to do a little fragment of what people can do but that can't do it all. Turing
 insisted on all and there was a reason for that reason because with a toy, a chess playing program,
 the degrees of freedom are much, much wider. There's countless ways in which you can get
 something to play chess, like a computer program to play chess, whereas if it has to do everything
 that we're able to do, the degrees of freedom are narrowed down to the normal degrees of freedom
 from empirical research program. The empirical research program of cognitive science is reverse
 engineering capacity to speak, hear, and think, and part of that capacity is robotic capacity.
 So the T1 is a little T and it's not one of the Turing tests. But let's not ask questions now.
 We'll cover that in the discussion section. So the symbol grounding problem, I'm going to give it to
 you in the mushroom world because everything's there and the same questions that arrive over
 and over again in this talk about the immense bag of text that is in ChatGPT's head, if you like.
 How is that connected through the world that it refers to? Because those words are talking about
 things. I'll give that in detail. So on the left in this illustration, you see a word cloud,
 a bunch of words. On the right, they happen to be a word cloud of names and features of mushrooms
 which are in the world. So we have the word, the mushroom words on the left,
 what ChatGPT has, and you have the mushrooms on the right. How are they connected? I see there that
 one of the words is morels. I have no idea what morels are because I'm not an expert. But if I
 were a robot or a person who has seen mushrooms, tasted mushrooms, tasted mushrooms that made them
 throw up so they realized that those were toxic, poisonous mushrooms, a person like that does know
 what the word morel refers to. They are not just a bag of words manipulating words. They're actually
 agents that are interacting with the reference and the meaning of the words. And we'll get back to
 that as well. In between the two, we have something that we'll also be discussing,
 which is a dictionary. A dictionary in principle defines every word in the language.
 So in a sense, just as it's all there in GPT, it's all there in a dictionary. The
 difference is that a dictionary is not an LLM. It's a very, very, SM, a very small bag of words.
 And the other thing, and this is true also about ChatGPT, is that the dictionary is completely
 circular. Every word that you want to know the definition of is defined in the dictionary. That's
 part of the definition of a dictionary. But if you don't know, if you're not initialized in some way,
 so that you know the meaning of some words, then whatever you look up, whatever unknown word you
 look up will just give you a string of words that you also don't know the meaning of. And you can
 look up those words too, and you still get nowhere if you just stay in this vicious circle that is a
 dictionary and that is also ChatGPT. So I'll be talking a little bit more about that as well.
 That's called the simple grounding problem. How to connect words to the things that they refer
 to. The hard problem, and I'll only be saying a little bit about this, it's the hard, there are
 two problems in cognitive science. Turing takes care of the first one, or gives a method for taking
 care of the first one, which is how is it that thinking systems, organizers, can do all the
 things they can do? Talk, learn, remember, identify, categorize, etc. All of the things you can do,
 a thinking entity can do. Including, for example, looking at this mushroom and seeing red.
 That's another thing they can, another property of a thinking entity, but it's not something they do.
 Turing takes care, Turing's is really a kind of a behaviorist method. He takes care of everything
 you can do, including everything you can say, but he doesn't take care of what it feels like to see
 red. That's the hard problem. We're not going to dwell on that. It's going to be a little tiny
 component of one of my hunches as well, so I'll set that aside. The easy problem is explaining
 how and why organisms, cognizing thinking organisms, or any system that cognizes and
 thinks does it. The hard problem is how and why do some of those systems feel anything?
 I gave this talk last week at Milla, and I gave it more abstractly. I want to give it
 very concretely now, just pictures. This is the mushroom island. Imagine that the fellow on the
 left was shipwrecked on a mushroom island alone, and the only thing there is to eat, which is
 convenient for me because I'm a vegan and mushrooms are vegan, the only thing they have to eat is
 mushrooms, and that sustains them. Some of the mushrooms are poisonous, and the shipwrecked
 person has no idea which ones are edible and which ones are not, so the only thing he can do
 and cross his fingers that it's going to succeed is take very tiny little tastes of mushrooms
 and see if they make them sick or not. He has to do that over and over
 again, make himself sick multiple times, and nourish himself often enough to be able to
 survive until something inside his head, and we'll talk about what that is in his head,
 detects the features that distinguish the mushrooms that are edible and the mushrooms
 that are inedible. All of that is done non-verbally. That's all robotic capacity. He's grounding
 two simple words on this island. He already had the actually, since he's a normal person,
 he can speak, so he has the word edible and inedible, but he doesn't know which mushrooms
 are edible and inedible, so he hasn't got the mushrooms on this island grounded, so
 if he has to do it alone, he has a lot of risk. It's going to take a lot of time, and he's not
 guaranteed of success. He may not survive. On the other side, the same situation except this time,
 the shipwrecked person lands on the island, and there is someone on the island already, a professor
 who knows what the features of the edible mushrooms are, so he tells the shipwrecked
 person what they are, and the condition for that to help this shipwrecked person think about this
 for a second, there's something that's absolutely essential and it's completely, it's fundamentally
 relevant to chat GPT. The other person does not know which are the edible and inedible mushrooms
 on this island, but he does know all of the categories, all of the names of the features
 of edible mushrooms that the professor tells him are the features of edible ones here on this island.
 He knows red, round, long, short, stem, spotted, all of those words you need in order to ground
 on this island the words edible and inedible. He actually already has the word edible and inedible,
 but now he's grounding the one for edible and inedible mushrooms. When he was alone
 on the island trying to do it directly by trial and error, he wasn't interested in words,
 he was interested in features. Here it's the names of the features that are giving him that,
 teaching him the category, the distinction between the category edible mushrooms and
 inedible mushrooms on this island. I'll take one second for questions if there's something
 unclear either here or in the... Okay, if you're on the online version, please just speak out
 aloud if there's a quick comprehension question you want to ask me. Okay, go on. All of the
 essential elements of symbol grounding are there. In the first case is direct sensory motor grounding.
 The robot uses its sensory capacities and its movement capacities to sample in the environment,
 find the features of what he wants. I should quickly, by the way, because I'm going to be
 talking a lot about categories here, define category. It's not the usual definition,
 but it is in fact a representative case of the usual definition. The usual definition is
 classifying things, putting them into groups. That is useless for this kind of discussion.
 For this kind of how and why in the biological discussion, a category is a kind of thing,
 let me put it another way, to categorize or to learn to categorize is to learn the right
 thing to do with the right kind of thing. That's why the mushrooms are perfect for this.
 You need to eat mushrooms in order to survive, but then you need to not eat
 poisonous mushrooms in order not to die. So there's your feedback, your corrective feedback.
 And if you learn it directly by sensory motor learning, you by trial and error sample what
 there is and you get feedback when you do the right thing and when you do the wrong thing.
 For those of you who are in computer science, this should already be calling into mind
 both unsupervised learning, where you're simply looking at the mushrooms and seeing what correlates
 with what, and supervised, in fact, reinforcement learning, when you're not looking at correlations
 between features and features, but you're looking at correlations between features and outcomes
 for you doing the right thing with the right kind of thing. If you wanted an intuition for it now,
 that is direct sensory motor grounding. On the right, the professor is telling the verbal,
 describing the verbal features of the edible mushrooms. That's indirect verbal grounding.
 And here is the core intuition to take home. You cannot have, you cannot receive,
 and you cannot learn a new category through indirect verbal grounding from someone who knows,
 unless at least the words in the description are already grounded for you.
 They could be grounded by indirect verbal grounding also, but it can't be indirect verbal
 grounding all the way down. There has to be some way to break that circle. That's also what we
 were talking about in the dictionary, right? The dictionary cannot be just going from
 undefined word to undefined strings of undefined words. There has to be a way to break that loop.
 Stephen, there's a question. Yes. Gary, go ahead. No, Steve has a question. He's got his hand.
 Short question. Go ahead, Steve. Can you hear me now? Yes, I can. Oh, well, lovely talk so far. I
 got hung up on the very beginning and I know you're probably going to get to it. So just tell me that
 is the word understanding. So yes, yes, we'll get to that. Okay. That's very good. I'll be patient.
 You'll be patient. Okay. Now I'm now going to, so my hunch is that there's something happening at
 LLM scale that overcomes the problem that I just mentioned that you can't get or in general,
 you can't get or give understanding of anything. I'll get to what understanding means in a second
 without already having some prior grounding. It can't be inheritance from inheritance from
 inheritance. There has to be some place where you have a source. And the source is of course a source
 of the ultimate grounding is always sensory motor features, the stuff that you can pick up by some
 other means than verbal. And that's how you ground the incredible astronomical power of words. And
 by the way, the reason that I showed this illustration is because I want at LLM scale, the
 nuclear power of language itself is bared in a way that is not, it's shown locally when we talk to
 one another, when you hear a talk like this, but it's in its full glory if somehow you could hold
 all of that stuff in our heads and do all of the operations on it that Chad GPT could do,
 but even then, even at LLM scale, it won't break the circle. Just to anticipate, it will break the
 circle for a learner. If someone wants to know what the mushrooms, the edible mushrooms are and
 the inedible mushrooms are, then if someone else knows, they can break the circle by giving the
 person a description or a definition in words that are already grounded for that person. But
 notice that they have to be already grounded for the learner, not the category inedible mushroom.
 That's a new one. That's the one that you're asking about. Well, the features that the one
 who already has it uses to define and describe it, that has to already have been grounded for the
 learner. Chad GPT is both learner and teacher. And you should ask yourself, there's an asymmetry here.
 The asymmetry is between the teacher and the learner. The teacher has to know what the right
 description is. The learner has to understand the description because the words in the description
 have to be grounded for the learner. Normally, they're grounded for the teacher too. I mean,
 when you get told something in words by somebody, that person, not only do you understand the words
 that they're using, but they understand the word that they're using. And we'll get into that as
 well. Understanding is a mirror capacity. Understanding mirrors with meaning. You can
 say something and mean something. And then you can hear that thing that's said to you,
 and you can understand that same something. And that's a mirror capacity, which I'll be speaking
 about in a moment. But here, what you consider, what you should consider is that there's no way
 for Chad GPT to break out of the word-word circle. All Chad GPT has is words. Chad GPT is capable of
 doing things with words that we can't do with words. Immensely powerful things because there's
 so many, because there's so much infrastructure helping out, and so many algorithms, computing
 time, et cetera, et cetera. So even if we had somehow squeezed that many words into our brains,
 we couldn't do anything with them. In fact, it would use up all the capacity we have for
 everything else that we can do for our ordinary everyday robotic lives in order to even begin
 to do with our neurons what Chad GPT, which LLMs are doing with their word database.
 It would be much simpler if words resembled in some way what it is that they referred to.
 And for now, I have to give you a few definitions. In the dictionary, you find two kinds of words,
 content words, which is 95% to 99% of the words in the dictionary. That includes nouns, verbs,
 adjectives, adverbs. Adjectives and adverbs are pretty close if they just have a little bit of
 syntactic difference. And nouns, they're names of individuals, proper names of individuals,
 interesting in dictionary. We don't usually put in George Sanders in the dictionary.
 Individual proper names and category names. Those are important. So the 90 to 95%
 of words that are content words in the dictionary, they're a category name. They're also function
 words. The difference between a content word and a function word is with a content word,
 you can point to the thing that it refers to. Cat refers to cat. Cat's function words,
 like if, and, for, of, the, is. You can't point to an is. Yeah, but some people like
 gerunds, abstract gerunds in the case of is, they say, yeah, but you can point.
 Let's say being is a referent word. You can define the word being in a dictionary. And having gotten
 the definition of the word being, there will be examples of being and examples of non-beings.
 Very abstract. We'll get back to that later. But if and and is something that you have to know what
 to do with it. It's a syntactic word. You're not looking for its referent. You're looking for its
 use. And uses can be grounded. Excuse me. Uses don't need grounding. All you need in order to use.
 Nichtenstein had an incorrect belief about all words, all content words and function words,
 which is that they all are all just a way of using words. The meanings of words are just the way you
 use the words. That's true for function words, but it's not true for content words. Content words
 have reference. And in order to know what a content word refers to, you have to be able to,
 you have to know what its reference is, not the definition of its reference, which can give you,
 right? You say yes. Spontaneous examples always escape me while I'm speaking. All right. What is
 what is the usual example that people use is what is democracy? You can define democracy
 in words. It may be inadequate. You may have arguments about what it is, but you could,
 a string of words will get you started. And then you can start arguing about what the features of
 democracy are and aren't. And once you've got it grounded indirectly, then you can point to examples,
 just like you do with cats. That's a cat. That's not a cat. And you can point to examples of,
 of instances of democracy or not democracy. You can say what's going on in the United States today
 is not, or is, or isn't an instance of democracy, according to my definition. So this distinction
 hold it in your heads. Content words versus function words are very important for the
 grounding of CHPT. But in addition to the distinction between content words and function
 words, there's a distinction between words and propositions. Some people say, what does cat mean?
 Which is your, your elementary school teacher will tell you not the meaning of cat is I'll tell you
 what the meaning of cat is, but cats are not the meaning of cat and pointing to the cat doesn't
 tell you the meaning of cat. What just gives you the referent of that? What does it refer to
 meaning? And Steve Hansen will eventually get to understanding, but we're halfway on the road now,
 because in order to talk about meaning, I have to talk about it's perception production inverse,
 which is about understanding is it's, excuse me, before talking about understanding, I have to
 talk about it's perception production inverse, which is meaning. When you say cat, you're just
 referring, when you say the cat is on the mat, you were saying a proposition, a proposition has a
 subject, the cat, and a predicate is on the mat. And it has a truth value, true or false.
 Cat is not, if I just say cat, I haven't said anything really, I've just said, I've just
 pronounced a word. So my first hunch is about this, and I'll get back to that. I haven't,
 I haven't set it up yet. Iconicity is a resemblance between the word that refers to
 the content word that refers to a thing and the thing. If the word for cat were a cat or something
 that looks like a cat, or a drawing of a cat, you would have an iconic name. I haven't got time to
 explain it in detail now, but there are many reasons why neither formal language like mathematics,
 nor natural language like English and French, why the words of, why the symbols of mathematics and
 the words of English cannot be iconic. You cannot reserve an example of that. By the way, the reason
 I put Marcel Marceau over there on the upper left is in miming, it's not speaking. In miming, there
 is an iconic relation between what you're doing and the object that you're imitating. Miming is
 imitation, and language cannot be that. I haven't got time to discuss that in this talk, but if you
 look at the interaction with chat GPT that's in the, that I sent a link for, this is discussed in
 more detail. I can't do it in this talk. I know already from giving this talk at Mila last week
 that I haven't got the time for that. So I've made the distinction between reference and meaning,
 I've made the distinction between words and propositions. I've told you what it is about
 propositions and it's different from, and my first hunch, oh I forgot to say, so upper left is Marcel
 Marceau miming, and that's not language, not speaking sign language. Sign language is down
 on the right, and in sign language, although it starts with, there's a little bit of similarity
 between gestures in gestural language and content, content gestures or actions, and they're
 referent, it doesn't figure into it. You can say in sign language everything and anything you can
 say in English or French, and the resemblance, slight resemblance of some of the gestures to
 the thing that they refer to has been lost a long time ago when language first evolved. Just as for
 example there are onomatopoetic words, onomatopoeia, where the similarity between bark, which may
 in originally have been an imitation of a dog barking, and barking is gone. It's irrelevant
 because in order to make all the propositions that you're capable of making with all of the
 instances of a word, the iconicity cannot enter into it at the word level, at the word level.
 Now here comes hunch number one. What G.P.T. has at the L.L.M. level is not just words but propositions.
 Now the proposition the cat is on the mat, not only does cat not resemble
 cat, and that doesn't resemble mat, and he is etc., but the cat is on the mat, does not resemble
 a cat being on a mat, and the mat is on the cat does not resemble a mat not being on a cat, because
 we're here on local terrestrial space, we're not in L.L.M. space. It's just one sentence, the cat is
 on the mat, and the mat is on the cat, the cat is not on the mat, those are all just atomic propositions
 which we can do, recombine in different ways, but at the L.L.M. level there may be, first hunch,
 a benign bias that comes out only as a structural, an iconic bias, a universe of propositions,
 not a complete universe because not every proposition that could be said has been said
 and never will, but as you approach that there's a structure that could emerge from that or an
 entity with the powers of GPT with enormous bags of data and correlations in those to detect the same
 way or a similar way to the way that the shipwrecked sailor can detect after lots of trial and error
 learning the features of an edible mushroom. The features of predicate space, if you like,
 propositional space, of true propositions and false propositions may contain a constraint.
 It doesn't give that, it certainly doesn't give you Chad GPT's powers, but it narrows down the options
 and all of my hypotheses will be about how is it that the looks like an unnavigable bunch of options
 up there, how is it reduced to a point where it's a little bit less surprising that it can do what it
 can do. When I get to the sixth hypothesis, Chomsky's thinkability hypothesis, this will
 come back in its clearest form, okay, so the first proposition, the first hypothesis is
 there is, there may be propositional iconicity not at the word level, but at the propositional level
 at LLM scale, which is not accessible to minor computational devices like ourselves.
 Another one now, we were talking about mushrooms, we were talking about their features of mushrooms
 that you have to learn to detect. One of the tools in the repertoire within the capacities
 of LLM's and Chad GPT is something that's almost equally striking, namely deep learning. It's remarkable
 how neural networks using variants of the canonical backpropagation algorithm and then
 lots and lots of ramifications are able to do such remarkable learning, learning of the kind
 that the shipwrecked mariner on the left is doing when doing its sensory motor interactions
 with mushrooms, tasting them, getting positive and negative feedback. These neural nets
 can do that kind of thing. It's not ridiculous to say that a neural net could be the one
 that, you know, our own brains, something like a neural net that enables us to learn the features
 that distinguish members from non-members of the category by trial and error. It's the second phase
 reinforcement learning. And here comes another form of iconicity that is not really iconicity.
 Neural nets, if they're learning a category, their performance changes as you give them trial and
 error training. First, unsupervised learning just to get the lay of the land to find what's
 correlated with what. And then when it comes down to brass tacks, when you're training them which of
 these mushrooms, not what the correlations are among the features of the mushrooms, but which
 of these mushrooms are actually edible and inedible, at that point the neural net will by trial and error
 weight more heavily what is correlated with success in the reinforcement learning and less
 heavily what is not. In effect, it's got lots of candidate features which in the beginning are
 all on a par. And even after unsupervised learning where you get the correlations among them,
 you still don't know what's the edible and the inedible one. But once you start training with
 reinforcement learning, the neural nets are capable of detecting, abstracting the features,
 the relevant features that will distinguish what's the right, what kind, will give you the means to
 do the right thing with the right kind of thing. It will heavily weight more heavily those features
 that are correlated with success and it will down weight or even eliminate the features that are not.
 As a consequence and as a side effect, something happens inside those nets which is uncannily
 similar to what happens to realist people learning real categories.
 Something like I illustrate the rainbow on the upper left. In the rainbow, we don't see the
 rainbow we know is just what's varying across the wavelengths is just wavelength. It's longer and
 shorter wavelengths, but what we see is bands of color. If you look at it very fine-tuned,
 they do great into one another, but if you take a step back, an equal size distance between
 two blues, two wavelengths in the blue range, two wavelengths in the green range,
 they look about equal size differences. But if instead the very same difference is across
 between two of them, between one on one side of the boundary between the categories and one on
 the other, there's an accordion effect or this that's what gives you the rainbow effect, which is
 they look more the bands looked at a certain distance look further apart from one another
 between bands or differences between bands and within bands. That's been named categorical
 perception by people who studied it in color and then studied it in speech, badaga. It's also a
 continuum just like blue, blue, yellow, blue, green. Badaga in phonology space is a continuum,
 but we do not hear it as a continuum. We don't perceive it as a continuum. You perceive it like
 a rainbow. You know, ba's vary in their quality, but they're all ba's and ga's vary in their quality
 and ga's. But when you cross the badaga boundary, the same difference, same physical difference
 perceptibly separates them. It makes in the case of the rainbow, it makes the red and the blue and
 the green pop out. And that's because of the feature detector. And even in a neural net that is
 learning features and could be learning features in a very synthetic space, there is a side effect
 that occurs as a side effect of learning the category, which is that if you look at the,
 and I haven't got time to spell this out either, but what we see here in the middle illustration
 is the representation of the act of the, let's say average activations of pairs of
 hidden units in hidden unit space before the neural net learns the category.
 And then what happens as it's learning the category and what's happening is as it's learning the
 category, these hidden unit representations are sorting themselves out. There's two categories
 here and there, there's some colors, I guess are blue and whatever. They're sorting themselves out
 until at the end, there's no overlap anymore. This, of course, this is much more exaggerated
 than a rainbow, right? And a rainbow is much more exaggerated than what we find in human subjects
 when they're learning a new arbitrary category. It is the same effect. It's this between category
 separation and within category perception. And that's my second candidate for this.
 Whereas there's no way you can give a chat GPT a sensation. You can certainly give it
 neural net data. In fact, it uses neural net data. So if you're trying,
 if you're first of all, supposing you're just trying to do a T3, you're trying to have a robot
 that can do everything we can do, including this kind of perceptual learning.
 That robot, if it uses neural nets in its head, will have this kind of separation effect occurring
 computationally. It's not perceptual, it's computational.
 But it's there. And there is a side effect of learning category. And we learned here,
 the categories can be learned at least two ways. One of the ways is in the direct sensory motorway.
 And the other way, which is to learn to detect the features with your neural nets. Let's simplify.
 The other way is to learn the features using grounded feature names describing the feature.
 And those grounded feature names, one of the, I haven't thought enough about this,
 but one of the questions you want to ask is how can, what happens when we're teaching chat GPT
 something? Chat GPT is ungrounded and it only has words, but there are some combinations of words
 that are not in its word bag. And it is capable, and in learning the word bag, it clearly did this
 a lot because it had indescribable numbers of words, but it is capable of changing its internal
 representations to reflect, for example, what the features of a referent are. It has a word X,
 X refers to things that can be described in other words, eqrs. And that kind of an interaction,
 I think my second hunch is that kind of a verbal interaction, perhaps also in humans, but clearly,
 certainly in sensory motor learning in humans, what's not been tested yet in humans is verbal
 learning, verbal grounding in humans. That can also produce this separation compression effect,
 purely computationally, making things pop out in a space that isn't really perceptual,
 it's just representational. But that is available to a creature like chat GPT, second hunch.
 Related to the second hunch is we're talking about the relation between
 direct feature detection and indirect feature description. The only thing I said about
 direct feature detection is nonverbal, indirect feature description is verbal, but the words
 for the learner have to be grounded. I'm simply singling out that capacity as one of the potential
 biases. These are biases that we're talking about. They're not understanding. I'm not going to end
 up concluding that chat GPT understand, but there are biases that are part of the landscape,
 large language models that could constrain chat GPT to be more likely to say something relevant
 and correct. Now I want to get to circularity. The dictionary, as I said, can define every word.
 What you'll be surprised to hear is that you can shrink a dictionary to something much smaller than
 the dictionary. First, let me define a dictionary. A dictionary is a set of words with the property
 that every word in that dictionary is defined by words, a description, a definition, of words that
 are also in the dictionary and also defined in the dictionary. That's a dictionary. One thing that's
 not part of the definition of a dictionary is that every word in the dictionary is defined,
 but not every word in the dictionary defines another word. There may be a terminal point where
 you can get to it by definition, but that word doesn't go on yet, it could eventually. That
 doesn't go on to define anything else. So these terminal points that are defined, but not defined
 nerves, you can remove them from the dictionary. You've lost nothing. You can get back to them
 from what's left, but you don't need them. And once you've plucked off some of these defined,
 but not defining words, you keep on plucking them. That reveals more defined, but not defining ones,
 and the rule is always recursive, that what you have left has to be able to get you back to where
 you were. And if you keep on going back, you cut it down to something like 10 to 20 percent of the
 dictionaries. As little as 10 percent of the dictionary is left if you remove all of these
 these endpoints that are reachable by definition, but are not needed to go further.
 So we shrank dictionaries down this, and it's called a kernel, the kernel of the dictionary.
 But that kernel is not, so you might want to say, okay, so we have to ground that kernel some other
 way. No, not the whole kernel. It turns out that the kernel is still a dictionary. You can still
 define everything in itself. If you followed what I gave it by way of a definition of the way it was
 shrunk down, that kernel is a dictionary that can define inwards and outwards if you find the rest of
 the dictionaries. It's not the smallest number of words that can, if you know what they mean already,
 you can get all the rest of the dictionary. That smallest number of words is smaller, quite a bit
 smaller, maybe 60 percent of the kernel. Well, no, less. It's actually, no, sorry, that was stupid
 what I said. In fact, it's around a thousand words, and it's all of the, there are a thousand words
 that can define, if you know what those words are, you can define everything else out of them.
 But the same recursive process, you define something and then you use that thing that you define
 to define further things. Now, I have a question for you. A quick question. Is, and this is,
 we call this a minimal grounding set. Is a minimal grounding set a dictionary, according to my
 definition? Anybody out in virtual land or here? No one wants to commit yourself to saying it is
 or isn't a dictionary. Okay. It's definitely not a dictionary. In fact, it's the opposite of what I
 gave as the definition of a dictionary. And in fact, if it were, the way to see it, the proof,
 I'll give you a proof. If the minimal grounding set is the smallest number of words that can,
 from which you can define everything else in a dictionary, and it also contains a word that can
 be defined by, be a dictionary, it has to be defined inward as well. If there's even one word
 that you can define out of the minimal grounding set, it is not a minimal grounding set.
 So the minimal grounding set cannot be a dictionary.
 A dictionary is a set of words in which every word in the dictionary is definable from words,
 combinations of words in the dictionary. A minimal grounding set is the smallest
 number of words from which everything else in the dictionary can be defined. But the minimal
 grounding set, that set is not a dictionary. It can't be a dictionary at all. It's the opposite
 of a dictionary. That's interesting. Another interesting piece of news is that inside the
 kernel, there are lots of minimal grounding sets. So it's a minimal grounding set, although minimal
 is always the smallest cardinality, the smallest number of words. Let's say 1,000 is the magic
 number, somewhere around 1,000, or a minimal grounding set, but there's lots of thousand
 word minimal grounding sets inside the kernel. They're not unique, but they all have this property
 that if you can somehow, by some means, be armed with the reference of all of those words
 and the capacity to form a proposition, which is to say you have you have cat and you have mat,
 and you can also say the function words, the cat is on the mat. If you have that propositional
 capacity, then you've got the capacity to define any other word of those words.
 ChatGPT is not a dictionary. ChatGPT is absolutely enormous.
 But I want to make a prediction, which is that if you try to reduce ChatGPTs in one language,
 let's not complicate by using every language, you try to reduce ChatGPT's bag of words for minimal
 grounding set, it will be the same size as that of a dictionary. Words are words and a dictionary
 contains all the words in a set. Every dictionary definition is circular and it's approximate and so
 you can always make it bigger and it always depends on other words. But that's what we have
 when we when we have when we tell one another things verbally, we use the words we have to
 define the words, the reference and the words we don't have and then we can understand the
 meanings of the propositions that use those words. What am I going to say?
 So I think that that ChatGPT, which you can think of a ChatGPT as a super textbook or super
 encyclopedia or a super dictionary, you can always lengthen the definitions of words.
 If at least the definition put together by lexicographers is something about apples
 that we never thought of is a pomegranate. No, not a pomegranate. Anyway, something
 looked like an apple. Is that an apple or not? That depends on facts of taxonomy. So there's
 lots of words involved in really defining apples so that nobody can be left with any
 uncertainty about what to do with this. If it's an apple or not to do with it, if it's an apple.
 Not to be very long, ChatGPT is capable of doing the full distance and I think its minimal
 grounding set will be of the same order as the minimal grounding set of an ordinary dictionary.
 That has to be tested. I don't know. Yeah, that has to be tested. So dictionary can't break out
 of its circularity. ChatGPT can't break out of its circularity. But the fact that ChatGPT is giving,
 remember the asymmetry between the learner and the teacher. The fact that ChatGPT is the teacher or
 us means that we're not bound by that circle. Whatever ChatGPT tells us, that as long as we
 understand its words, it's going to work for us. ChatGPT, because it doesn't understand and its
 meanings are not grounded, doesn't need to understand. It just needs to be able to analyze
 its bag of words the way that it does for us. But the size of the minimal grounding set,
 even if there are multiple minimal grounding sets, is a constraint on us and it's benign constraint
 on GPT. Because inasmuch as any grounding set will ground its entire vocabulary,
 that reduces, for us, it doesn't tell us what new words mean. It has the definitions of all
 of those words within itself already. So it's not become grounded. It's freed from one of the
 handicaps that we have for words that refer to ungrounded features. It doesn't have any
 grounded features, but it doesn't need any grounded features. It's just doing its loop through the
 word word circle. And just to remind you about the magic of a dictionary definition,
 if your life depends on doing the right thing with a zebra, which is it, if it's a zebra,
 you should approach it and stroke its nose, and you have no idea what a zebra is,
 just one sentence. You already have a grounded word for horse, and you have a grounded word
 for stripes, and a zebra is a striped horse already gets you there. So for us mortals,
 robots in the world, language can reach anywhere directly. If we learn it the sensory motor way,
 this way, on the left, we can reach even further collectively if others can be our teachers.
 And Chad GPT is the universal teacher, if you like. All of the texts are in there.
 So it knows what to say to us. One of the questions you have to ask yourself is not only
 how does it sift all of that information out of its bag of words, but when it's getting a small
 handkerchief of words in interactions with us, how does it sort that?
 One of the benign constraints on that sorting is the fact that it can arrive at the words we said
 from its mineral grounding set via any route, and get back by any route.
 That would be a benign constraint, and it's a constraint, it's a property, if you like,
 of dictionaries as surely as it is a property of a large language model.
 The other thing, and this is related to the question about understanding and meaning,
 we have, as some of you know, certain neurons or certain regions that are active,
 that are involved in imitation, that are active whenever I'm doing something,
 or whenever someone else is doing the same thing. They're important for things like mimicry,
 vocal mimicry, copying, dolly style, copying an image, and they have a reciprocal relation
 with one another. The reason they're called mirror neurons is because there's something you can do.
 And I said the easy problem is explaining how and why a cognitive system can do all the things it
 can do, the things that it can do, and it's also the things that are doing them, and also perceiving
 them being done. That's a reciprocal relation that we have in language, and we're using it,
 especially in the, now I'm on the side of the of the hunches that say, that say less about constraints
 on GPT than constraints on us. We're not accustomed to teachers being able to tell us something that
 makes complete sense to us while having absolutely no idea what they're saying.
 So we project onto them our own state. Our own state is that of understanding the meaning of
 the words that we're hearing. Now this chat GPT, who's producing the words that we're now hearing
 or seeing read, understand does a GPT mean anything by those words? We're just projecting that.
 It just has a circular bag of words. We have grounded words. We can trace our bearings back
 down to grounded words, and we project that over the things that he does, says and does.
 This has a slight connection with the, I'm almost finished, this has a slight connection with the
 symbol grounding problem in the sense that, pardon me, the heart problem, in the sense that
 not only do we not only do we see red when we're seeing red and understand the word red when
 someone says red, that's red when they tell us because the word red is grounded for us.
 What we see when we see red is still implicit in the grounding of the word that refers to the
 feature red. There's another property there, which is that it feels like something to see red,
 as I said. Not just detect the feature red, but to see, to feel what it feels like to see red.
 And this, of course, the chat GPT doubly lacks. Not only is it not grounded,
 it has absolutely no way to feel anything whatsoever. So it may be, this may have more
 bearing on our surprise when it says something, that means something to us, and our projection
 of the understanding onto it that we feel. And this may be because of our mirror neurons. Our
 mirror neurons, after all, evolved in the context of living with other organisms and other members
 of our own species. And we're not accustomed to coming up to people who tell us the cat is on the
 mat and don't know what the cat is on the mat means and all the words that are around it. And chat GPT
 doesn't do that either. So we project onto chat GPT that capacity, but it may also be on chat GPT's
 end reflected in its own way of handling words. The fact that we have mirror capacity is implicit
 in its vocabulary. In fact, the understanding and meaning... I've caught chat GPT, by the way,
 contradicting itself in the same sentence. I ask it the canonical question, "Do you understand?" And
 he says, "Of course not. I'm just a bag of words with an algorithm. I don't understand." And then I
 say, "You understand what algorithm is?" I mean, well, I don't understand. The way it usually goes
 is, "You understand what algorithm is?" And he says, "Can you tell me what algorithm is? Do you
 understand?" And it'll tell you, "Yes, I can tell you. I understand." So we'll use understand to
 refer to what we mean by understanding. And also, in truth, it can be true. And it says, "I don't
 understand." There's an analogy to this with blind people. Do you know that blind people
 use the word "see" and they don't use it just for what it is that we have that they don't have?
 Well, they'll say, "I see." And I see that. And often they'll use it for another sensory modality
 that they have by touching. But they'll use "see" for it because in our language, "see" has come to
 refer to visual perception primarily, even though they don't have it. Anyway, this is another
 potential constraint on the words of ChatGPT that you would not expect if you just thought of it as
 a bunch of text written by people. I want to come to the last one, which has the flavor of
 the earlier ones, which are about ChatGPT's constraints and not our constraints.
 You know that there's two kinds of grammars, ordinary grammar. Every language has its own
 ordinary grammar, its own ordinary grammar rule, which are different from language to language,
 and change across time. And in addition, here I have an example of one of these tree structures
 that the Chomskyan linguists like to use. There's another kind of a structural grammar
 that also constrains what we say, what makes some things correct and some things incorrect to say.
 Ordinary grammar, I mean, you don't say, the example I use is currently, according to current
 English language grammar, between you and I is wrong. Or if you want to ask someone a question,
 it's wrong to say that I don't usually make this mistake, and so I have to figure out a way.
 Begging the question does not mean I beg to ask. Begging the question means invading the question
 today in English, but that could change as well. So these are all aspects of ordinary grammar and
 lexicon that can change from generation to generation. Universal grammar, on the other hand,
 is universal to all languages. Now, people have made a big deal out of the fact that
 Chachi Petit was not taught grammar. He's not taught grammar, and yet he speaks in conformity
 with the rules of grammar in each language. Ordinary grammar. I personally, this is not one
 of the things that surprises me. You know that Chachi Petit says and doesn't say depends on the
 proportion of his big gulp, it says it or doesn't. And apparently, even though there's a lot of
 grammatical errors that people make, ordinary grammar errors that people make, there's not enough
 of them in Chachi Petit's database to make Chachi Petit talk like that. That's why so many students
 use Chachi Petit to help them with their grammar and their style, because although it's not a
 literary style, it's really quite a good one for a student to use, and it'll change students'
 writing to that, and it changes it to that because it's not dominated by agrammatical English.
 There is agrammatical English in French, in German, in Latin, etc. It exists, but it doesn't exist
 in enough proportion, and I make the bet that if somebody fumbled with, fiddled with the vocabulary
 of a LLM and put in a huge proportion of ordinary grammar errors, it would start making ordinary
 grammar errors too. That's a hypothesis, but it never ever hears universal grammar errors, because
 neither do we. Universal grammar, according to Chomsky's first hypothesis, nobody makes a mistake
 in universal grammar, because universal grammar, we don't learn it the way we learn ordinary grammar,
 it's encoded in our brains, it's inborn. That's a controversial hypothesis by Chomsky. I won't
 close with another hypothesis, also controversial, a little bit closer to what we were talking about
 over here. Chomsky said there's another way to think of universal grammar, a grammar that we all
 obey, but that we never violate, or at least the only ones who violate are linguists.
 Here's an example of a violation of universal grammar. John is easy to please Sally.
 You can say John is eager to please, you can say John is easy to please, you can say John is eager
 to please Sally, but you can't say John is easy to please Sally, and that is not a violation of
 ordinary grammar. That's a violation of universal grammar, and you would never have thought of it.
 It takes a Chomsky linguist to get to play with the rules, hypothesize rules, and eventually
 generate something that violates something that looks like it's a rule of law.
 Chomsky's second hypothesis is that the reason children never
 make UG errors is because UG is not what is innate, and in fact nothing is innate.
 Thinking is something that cognizing organisms can do, thinking. The question behind all of
 this is what is thinking, and we think thoughts. Chomsky's second hypothesis about UG is that
 the reason all languages vary in ordinary grammar, but all of them are governed by UG,
 because a sentence like John is easy to please Sally is not an expression of a thought that we
 can think. Thoughts can be thought, language and thought is not quite the same thing, but language
 is a system that can express any thought, and John is easy to please Sally is not a thought we can
 think of, and this awkward expression sounds wrong not because it violates a linguistic grammatical
 rules, because it doesn't follow the rules of thought. That's Chomsky's conjecture, and I think
 that you can think of all of the LLM constraints as being a little bit like that, like something
 bigger constraining language itself, and in the case of thought, even wider. Thought constrains
 language, and then language constrains countless other things, including is the cat on the mat,
 or is the man on the cat? With that, I think I'll stop. If you're interested in the details that I
 left out, the link down here, I'll post this, the link down here with the link in which I discussed
 it in more detail with CHPT. Let me stop sharing, and we're open to questions.
 Stephen, I'm still waiting to hear what a definition of understanding is.
 Understanding is the perceptual side of meaning, and meaning is the capacity to
 formulate a proposition with a truth value. The cat is on the mat is a proposition with a truth
 value. That's what it means. It means the cat is on the mat. If you want to unpack what the cat is
 on the mat means, you have to unpack what cat refers to and what mat refers to. Basically,
 that's the concrete side of understanding, not what it feels like to understand, which is
 a hard problem, which is also relevant and interesting, but simply what is being done
 when you say the cat is on the mat. If you're saying the grounded word cat,
 if you're a robot that is saying the grounded word cat, the grounded word mat, you can point to or
 draw or give evidence for a cat being on a mat. If you can mean that and you have the
 capacity to understand propositions just as you have the capacity to form propositions,
 the meaning is just the mirror side of understanding. The understanding is the
 mirror side of meaning just as receiving, just as making a gesture with your hand like this
 and seeing someone make a gesture with your hand like this is our mirror capacities. Sorry,
 I'm getting a little articulate. Gary, did you want to challenge me again?
 I'll let Steve take a shot. Steve, take a shot.
 Okay, so I think I followed this partly based on our earlier conversations over the last year.
 I do think Mironons get blamed for too much functionality and this might be a case of it,
 I'm not sure, but the thing I have a problem with is the evolution of your thought here
 because you start out saying these chat bots are remarkable but they don't understand
 and yet you have conversations with them where it appears they're understanding despite the fact
 that you know if you ask a chat bot does it dance, of course it will say no, I have nobody I can't.
 So there's a lot of deniability here but there's a sense in which the exegesis that you and others
 are doing in this story, this projection story, feels like behaviorism. It feels like you don't
 have any internal mechanisms to actually point at. You don't have any, there is no algorithm per
 se, this thing that learns whatever capacity it has and yes it's learning to make well-formed
 utterances that like with anybody if I'm trying to understand what you're saying or trying to
 understand what Gary Cottrell is saying, I have to project something of my own understanding into
 this story and certainly this is no less true of a chat bot. So if it was just a big database of words
 then there would be literally no reaction to it. AI did this in the 70s and it just didn't work
 and it didn't make any sense and people kept trying to write parses in front of these large
 knowledge bases and it failed over and over with millions, tens of millions, hundreds of millions
 of dollars behind it. So all of a sudden this little open AI place, it has an accident, this
 thing appears and we don't understand what it is but people like you are making inroads to it but
 I still think I don't understand the distinction between saying it doesn't understand sensory motor
 things which makes sense because it has no sensory motor context and I think that's a very
 good distinction to me but I don't understand the first part because so for instance I spent a lot
 of time programming Python with it and does it make errors? You betcha. Is it stupid about things?
 Yes it is but does it learn when I tell it something? It does and then it corrects itself.
 Now I've had poor graduate students in that so I'm always amazed at how chat she reminds me
 of just a good graduate student who's learned stuff. So there's a sense in which the sensory
 motor part probably is crucial here and you know in the next decade, maybe the next five years,
 we're going to see a lot of robots with chat bots attached to their head somehow. I don't know what
 that will entail but again this is all emergent. I mean the first argument you have to make is it's
 emergent but don't know what it is and there's no simple explanation. That's kind of a question
 embedded in a statement. There's other questions so I'll answer quickly and we can always continue
 some other time. I'm not a behaviorist. You can't be a behaviorist if you actually give
 or if you actually admit that there's some internal mechanism. Sensory motor grounding,
 the first kind that I described where the shipwreck guy is using his eyes and his hands
 to detect the features that distinguish the edible ones from the inedible ones. That's at the bottom
 of it all and it's not behavioristic. You need a mechanism for that. The mechanism, one mechanism
 that looks like it might work, neural nets that learn features. That's all you need. The rest of
 it about understanding and meaning, an important property that I was talking about was the asymmetry
 between the teacher and the learner. That's what allows you to have these amazing experiences with
 chat GPT where he's telling you something, he surprises you and he's able to even say everything
 that he can say but then you interact with him and you correct something and he gets it and he
 understands. You can't help yourself from saying he understands but all it is is an illusory
 interaction between an ungrounded entity that cannot even distinguish an edible from,
 you can't even talk about mushrooms or cats. With you, you can't get rid of your grounding,
 it's there giving content to every proposition you hear. For it, it shows you how amazingly,
 and that was what I explained today, how amazingly much you can do, it's like what you can do
 without grounding, without any of that, not the slightest hint of that. I talk in the chat with
 GPT about bottom-up meeting top-down somewhere in between, if that's what you mean by
 merge. I think that's completely incoherent. You can't take a bunch of words that are handing
 from sky hooks with no grounding at all but amazing LLM scale properties that can be
 used by the kind of analytic capacities and actually grounded discourse. I think, go ahead Casey.
 Yeah, I truly disagree with you but I'll bring it up later. I think Gary had a question.
 Okay, right now Casey's next. Yeah, Casey's next. Thank you. I'm coming at this, I'm looking at this
 from a little bit more. Raise your volume a little bit, just a little bit. Sure, I'm looking at this
 a little bit more from the computer science side. The way chat GPT operates starts with the
 distributional hypothesis of semantics. Semantics here in very, you know, loose terms, very scary
 quotes. I don't know about others but your voice is falling off. It can possibly get closer to a mic.
 Yeah, the distributional hypothesis, is this better? Yeah. Okay, the distributional hypothesis
 is what is the semantics that all of these models are based on and that's just what is the meaning
 of a word based on how it's used in text. It doesn't read from dictionaries. The learning regime
 for these is playing a game of guess the word within a context of other words. Not in a physical
 context but in a context of other words and from that it derives a distributional meaning of a word.
 Of course, that's ungrounded. I'm with you on that but I can't help but think it's deriving
 some degree of meaning especially for more abstract words that's fairly reliable.
 By some degree of meaning. I'm waving my hands very vaguely here. I don't know. I don't know
 because they're vectors and I don't know what they're doing with them but the history here is
 the first distributional models were just let's count word co-occurrences within a certain
 text window. Then down a few years later they came up with the word devec which is the same idea
 playing a game of guess the word with a simple model. The semantics here being a vector at the
 word level but then down the road some more we see more complicated architectures that are designed
 to handle the more complex linguistic phenomena. That's what self-attention is for in order to
 compose. They're composing vectors and who really knows what's happening at the level. When I say
 compose I don't really know how it's happening with the models but it seems to be doing something
 because it can spit out coherent sentences. This is all the distributional hypothesis at work just
 with more complicated models, with more data, that kind of thing. When I think about a vector
 or a tensor and I think about how that's representing meaning I don't know what how
 it's doing it but it must be doing something and it must be doing something fairly reliably with
 with more abstract words because when you talk about it about talk with chat gpt about abstract
 things it seems to do okay. When you get to more concrete things like mushrooms and rainbows it's
 like I can only tell you what I've read about those I don't know anything else and that makes
 me think like a human who tries to learn a foreign language but only tries to learn it through text
 like reading something in your native language then in the foreign language the target language
 and kind of learning from that and you don't kind of experience speaking with native speakers or
 anything like that and you learn the language but you're still carrying your internal groundings
 that you've had when you learned your first language along with you but I'm trying to get
 into the head as it were of a model like chat gpt that just learns learns a language abstractly
 only without having a body and that's just something I can't quite put my head into.
 I'm in agreement with you that understanding here is probably not happening it's it's and and the
 more I try to explain to people the more I find that I'm probably in kind of a radical embodied
 camp of of cognition even though I'm not a cognitive scientist but I can't I can't I can't
 explain why the distributional hypothesis does so well with these but that's what's happening
 under the hood. The distribution distribution hypothesis is a way of putting some substance
 into the notion that there's some other kind of understanding than grounded understanding of the
 sensorimotor sensorimotor robot kind and I don't think there is and there's and and it's not just
 grounding that's missing of course there's something else that we have which is that
 it feels like something to understand and they don't feel anything either but I don't I don't
 I don't know what what progress we can make we start from the same starting point Casey which
 is that we're both puzzled by how we know it I know as well as you do what it is that that
 the heart that the infrastructure is doing and the algorithms are doing and the next work stuff we
 all know what the what the method is which it arrives at its amazing performance but the
 performance is amazing and I'm all I'm trying to do is give a few reasons to think that it may be
 a little bit less amazing than that but it's still amazing and I certainly don't think that the
 punchline will be that there's another kind of understanding and that's the kind of understanding
 it's sort of like a native language understanding that you carry to some other kind of understanding
 I think that's I don't understand that okay Gary yeah okay um so uh what about the new
 uh gpt's that have vision and language I go over that in the ground I go over that in the long
 version and Joshua yes last week when I gave this at milla I brought that up is this the short
 version this is a short this was the short version uh yoshua said he thinks that the way all of the
 means that chat gpt is using to uh to do what it can do so amazingly well those same means apply
 to to a sensorimotor grounding and he can apply them he just adds on the sensory modalities and
 then you'll you'll turn chat gpt into a t3 and the longer version what I said was no you don't turn
 a chat bt you turn a t3 robot into a into a into a t3 robot by by bottom up right you don't take a
 hundred million grounded words already in the in text in your head and then somehow send down
 feelers from that in order to find a cat or a dog that's there most of that most of that vocabulary
 is free-floating sky hooks and but and that's that's top down bottom up is you start with mushrooms
 and their features and you try to eat them and they make you sick and that's the way you eventually
 go by propositions up towards the sky the two don't meet there's no seamless meeting between
 top down and bottom up that's an assertion but I don't understand why it's true uh seems to me
 that there are two paths right the top down and the bottom up yeah yeah the the ones the top the
 bottom I mean the top down is used by creatures like you and me that are bottom up grounded and
 we can go to a dictionary which is not grounded at all and we can pick us a definition of a word we
 don't know words that whether we do or don't know and you come up with it right that's but but all
 right so it's got this whole vocabulary it's got you know English essentially already but now you're
 attaching that directly to perception so I don't see why that isn't another path to you know
 never gotten up there without perception first we need the reference in order to ground the
 propositions but it knows the propositions and then it grounds them that's of course I don't
 I agree that if you put a chat bot into a into an automated word waiter it'll be smarter than an
 automated worder because not only can it might find you and give you your food but you can also
 talk to you about french haute cuisine but that's but but one of those is a trick it's like like
 like a look-up trick okay I think we'll have to agree to disagree there that'll continue a very
 long tradition yeah can you can you hear me yes yes okay if I understand you right and the
 ungrounded learning which is chat ttp it's ungrounded it's producing the language it's
 we understand as if it's grounded it's because of our grounding am I right right chat ttp is
 ungrounded because it has the other kind of training you give the training and so it's
 ungrounded am I right to understand yes yes yes okay so we get the impression that it's grounded
 because we interpret it as grounding because we have grounding experience and so my question but
 yeah it's not I'm not just talking about the user illusion here I asked some of the punches were
 about why it is that it's really doing as well as it is not just seems to be doing as well as it is
 sorry yeah the question is for this ungrounded learning for machines and human you assume there
 has to be feedback there has to be reinforcement in order to ground in order to learn in order to
 produce anything like for for for yeah for humans we have ungrounded learning too as soon as we have
 some basis of grounded basis right this core knowledge and then on top of that we can have
 other ungrounded learning and for chat ttp if I understand you right it's all ungrounded learning
 so you must give feedback you must give uh corrections or some sort of feedback that's
 another thing how do you define I wasn't really talking about how to train chat gpt I was talking
 about what chat gpt can already do and the thing that you said before that I can't agree with it's
 not that uh that you you have to ground you have to ground um the minimal grounding set and then
 you can start doing things ungrounded it's not everything's grounded that that's based on the
 minimal grounding set so in other words it always has to have the the ground that's why it's bottom
 up it has to be connected to words whose whose sensory motor uh referent is is accessible to you
 yeah yeah but the chat gtp doesn't have access access to the grounding and therefore it's only
 doing the ungrounded learning but we assume we seem to get the impression it seems to communicate
 with us as if it has something it's just us using our grounding experience to to expect to assume
 that chat ttp has that because even if it's not well our ungrounded experience it's database all
 of those words all of the words where it's trying to trained to predict the next word contingency
 those came from our grounded heads what it used was a text but but once it's a text it's a text
 it uses the data in that text to do some amazing things yes yes okay so my my true question is for
 humans uh you have said that's a content words you can have a basic minimal grounded set to learn
 from that you can go take off um and then there's function words that are i would say it's not that
 easy to be grounded so but there are categories for function words and children do get that so
 how do they how do they get it do you think that's something that they also rely on the grounding of
 these this minimal set with the content words and then from propositional i i think it would help
 people if you gave it a concrete example of a function word that doesn't have meaning yes a
 famous example is in french you have a difference between feminine and masculine of course you can
 talk about some people like that but children's early vocabulary it's not clear what they're
 early exposed to when they learn it it's all up when the object is defined in terms of gender
 it's not related to anything about about gender uh semantically so but it is a formal property
 that defines agreement in many languages in french for example so how do you get that in children get
 that um distributionally and that's shown i don't think they need um they need any meaning for that
 they did any they don't need any reference for that this is at least what the empirical results
 show so i'm wondering from your point of view even for so that's just just an example because
 there are function words that have some relational meaning but they don't stand alone you have to
 have some other things to help them i just want to hear your thoughts about how children can learn
 do you think there's some grounding basis before they can even learn these very formally defined
 syntactic categories i think i agree with you uh ruchin i think that uh that uh am i still there
 can you hear me yeah okay i think that uh function words are learned and and i suspect that a lot of
 them be learned by unsupervised learning just by distributional mean yes they are learnable and they
 and they're learned and i don't think uh anything is at issue there because there's no they don't
 need grounding function words don't need grounding you just need just like uh an axe at a shovel
 doesn't need a referential grounding in fact you just need training on what to do with the uh
 shovel right it's just thank you that's very helpful thank you for your answer
 anybody i can't i i've had a connection problem so i can't see who's there
 is there another question it says ryan okay go ahead ryan i've noted that chat gpt has a really
 tough time with analogy and metaphor for example and i just want to know if is it because it doesn't
 have like the perceptual grounding or is it just because it doesn't have like the the capacity to
 understand the properties of an object or i would say like relation with other object in the
 perceptual field what was your premise did you say it yeah it has a problem with analogy like
 and metaphors i'm not an expert in this let me say that i it's alleged that it has a problem with
 analogies and i'm sure a lot of other people are that if it does it's going to be fixed because
 there's nothing about analogies it cannot be handled the same way that you use all of the
 other amazing talents of chat gpt it's not a principled limitation anybody else
 if not i want to thank you for attending and i'm going to ask malika to stop the recording okay
