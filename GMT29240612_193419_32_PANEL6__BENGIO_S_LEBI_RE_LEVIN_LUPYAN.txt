 All right, sounds like there's no takers.
 One second.
 Alexei.
 Well, I could continue with Alexei.
 We were talking about the statistical structure of the world and trying to teach that to models.
 Maybe a simple question, what if we try to kind of approximate 3D objects in the models
 instead of like 2D data?
 Yes, that's a very good point, although the idea of an object itself is not very well
 defined.
 I always notice how the psychologists say, "Oh, you just talk about objects," but from
 a computer point of view, understanding what an object is, is itself a very, very hard
 problem, the grouping segmentation problem going back to all the Gestalt psychology.
 It's difficult, but the point about 2D versus 3D is point well taken, and I think that actually
 goes back to what you were saying before in terms of video sequences rather than still
 images, because in a sense, you can think about 3D and video as being two sides of the
 same coin.
 It's basically, they're both three-dimensional projections of the four-dimensional space
 time world.
 What we are seeing right now in computer vision is the beginnings of this convergence between
 people working on video and people working on 3D, and it's actually very, very exciting
 that we are basically able to start looking at ways to take in videos and kind of model
 the 3D scenes out of them with approaches like NERF and Gaussian splatting and things
 like this.
 So absolutely, I think this is all part of the same story that if you have a sequence
 of measurements of the scene that you are going to get more information, you get parallax,
 you get three dimensions or at least two and a half dimensions, and the data is going to
 be better data.
 Absolutely.
 Okay.
 Richard, you've been lurking.
 Hey.
 Were you here for the three talks?
 It was too early for Gary's, but I read your abstract, I'm sorry, but I saw the other
 two.
 Okay.
 Did nothing occur to you?
 So the results on visual working memory are quite striking, the fact that you actually
 do remember a lot, even though we have this impression that we kind of don't.
 And there's an analogous phenomenon in language processing that I think are worthy of exploration.
 Like a lot of times people will, if there's a sentence that has a sort of obvious error
 in it, people will proceed with the sentence as if the error wasn't there, they'll sort
 of automatically correct it.
 But if you ask for people to repeat the sentence verbatim, they can often, it'll come out with
 the same error.
 I don't know.
 It's quite interesting to wonder to what extent you would have a parallel kind of like this
 phenomenon where you have the subjective experience that you've forgotten a lot, or you have the
 subjective experience that you've corrected something, whereas you actually do actually
 remember a lot of details.
 It'd be interesting to see if there's something parallel happening in LLM processing.
 And I suspect there would be, because the transformer architecture does have the capacity
 to completely veridically represent the context you're in, but it might not actually use the
 veridical context that it's in.
 Just to add to it, so two interesting data points.
 One, although that original massive memory paper framed it in terms of capacity, and
 they actually in the paper tried to estimate the capacity, long-term memory based on the
 errors, it's pretty clear that capacity is just the wrong way of thinking about long-term
 memory.
 For example, we followed up on their study looking at some effects of language and categorization
 on memory, but we didn't have the patience for a four and a half hour study session.
 So we only had 300 images in the training session.
 The accuracies were exactly the same.
 So it's a constant sort of proportional error rate rather than, oh, you're filling up some
 limited resource.
 And it has everything to do with interference and potential for interference rather than
 storage limitations.
 And the other data point is that not only do people remember what kind of remote it
 is and so on, but if you test, for example, where it is, so you can vary the location,
 people remember that as well, they don't throw it out.
 So our object recognition is not fully invariant.
 And the reason for this is that location is often relevant, even if it happens to not be
 in a particular task.
 For many tasks, it is relevant and so you don't want to just throw away that information
 by default.
 Because doing that will reduce your performance on lots and lots of tasks where location is
 important.
 So maybe I should also qualify, I mean, this is a fascinating area, lots of really cool
 things.
 I think maybe I should qualify that this task of recalling, have I seen this yes or no,
 kind of this instance memory is performing extremely well, right?
 But if you do a different task, for example, I think one of odd student Wilma Bainbridge
 has a follow up paper on actually trying to have people recall which images they have
 seen by having them sketch images, right?
 Or maybe recall what was in a particular image by sketching it.
 People actually turns out to be really bad at this.
 And even the most amazingly bizarre result that Wilma had was that she, as a baseline,
 she had a study of, you know, you try to sketch something that you have seen, you know, a
 few minutes ago, an hour ago, or sketch something that you're looking at right now.
 And even when you're basically just copying information from an image that's in front
 of you, people make silly mistakes, like they admit things that are not important to them,
 that's fine.
 But they also add things that are not there.
 For example, you know, it's a desert scene, and the subject adds a cactus to the scene,
 even though the cactus is not in the image.
 And so I think the memory story is very tricky in that depending on how you probe it, you
 get very, very different kind of inconsistent answers.
 Well, so there is a long recognized distinction between recognition and recall.
 Yeah.
 And so, yeah, we know these are very different.
 One thing that I was surprised to find out, because I was skeptical, and then we did that
 experiment and I was very surprised, is that in the original task, it's a two-alternative
 force choice, right?
 And so all you need is just a tiny bit better recognition of one than the other to pick
 it.
 And so the question is, well, what about if you just do an old new task?
 That's probably the most common way of testing recognition.
 So you just show, you know, one remote, is this the one, yes, no.
 And it turns out that performance is basically the same.
 You have to correct for it, you use signal detection to correct for the difference in
 the possible false alarms, because in a two-alternative force choice, there's no false alarm.
 You either get it right or not, old new, you can miss or you can false alarm.
 And people's, yeah, at least tested on much smaller sets.
 It didn't make a difference when you do the corrections.
 So I was surprised.
 I thought that surely it would be a much, much harder task.
 It might feel harder for people, to Richard's point.
 It might feel harder, although, and I also, I've done the demonstration in classes and
 people think that they're, you know, this is going to be impossible.
 But once you actually do the task, at least many people realize, oh, okay, I, there is
 a sense, there's a phenomenology there, like, oh yeah, that one is familiar.
 So they realize that it's an easier task than they thought it would be.
 It's a bit like blind sight.
 Yes.
 Yes.
 There's a sense that you don't think you can do it and then you learn that you can do it
 another way.
 Yeah.
 Yeah.
 Another example of this also is recognizing objects from their sounds.
 So it's a very simple demonstration.
 You ask people, you know, can you recognize a coin, like what coin it is if I drop it
 and people are like, not sure, they think it's difficult.
 And then you just do it and they realize, oh yeah, that's a nickel, that's a dime, that's
 a quarter.
 It kind of alerts them that this is knowledge they actually have and can use that they didn't
 know they had.
 Too much agreement.
 Let's go ahead, do your bit.
 So I was just curious, like, there used to be a big debate on whether internal visual
 representations for people were descriptive or depicting.
 And I think in the end people were like some of both.
 But it seems related to Professor Lupien's points this morning about how language can
 scaffold our cognition.
 And I was wondering, in the talk, the last talk that had to do with LVMs, is there anything
 like that going on inside the LVM, yeah, basically.
 I don't think so.
 At least we tried really hard for it not to have any kind of linguistic or conceptual scaffolding.
 We really tried to see what we couldn't do from the bottom up.
 Now we might have not plugged in all the holes there.
 For example, we do have images from ImageNet that are category specific, which future work
 is to get rid of those and just really try to do it on maybe just egocentric streams
 of data and then see how well that could be.
 But we are basically in the business of trying to see what can you do without the scaffolding.
 What can we do from the bottom up?
 And I think the jury is still out on how much you can do on that.
 Yeah, so I get that you don't want to put the scaffolding in.
 But I was wondering, does the scaffolding just kind of happen?
 I see.
 So there is a new paper from my former postdoc, Filip Izola from MIT.
 I think it's called something on Plato's cave or something, where he does kind of an interesting
 experiment where he basically looks at mutual information, pairwise mutual information.
 So I send Richard's talk to him, and he was very excited about that.
 But between representations of LLMs and visual models, including visual models that are only
 trained on visual data, without a text, for example, dyno.
 And he reports that actually there is quite high mutual information between those two
 modalities.
 OK?
 Now, I'm not completely convinced by that.
 In fact, I'm not convinced at all yet.
 And he takes-- the limitation section is very well written.
 So I think some of those points in these are up under question.
 Because for example, he finds that they find that something like CLIP, which is trained
 to have a correspondence between language and vision, is not really having much higher
 mutual information than models that are very disjointed.
 So I'm not quite sure about exactly what's going on there.
 But I think it's a super, super interesting direction.
 So yeah, so Philippe Azola's work, I think, is one work that's trying to see, is there
 a unified pause in the world?
 Basically, the world is the same.
 Text and images are both projections of the world.
 So it makes sense that they might have some connections.
 My feeling is that they are-- yes, they are projections.
 But my feeling is that they are very, very different levels and different strengths and
 different power of projection.
 So I'm still suspicious that there is a connection between them that is very strong.
 But I could be wrong.
 It is for-- there is for synesthetes.
 Yes, but for synesthetes, that's very-- it's kind of arbitrary.
 It doesn't really have anything with the statistics of the natural world.
 And different synesthetes have different connections.
 I think, actually, the more interesting thing is Steve Palmer, who unfortunately passed
 away, I think, a year ago, he had some very interesting work on connections between colors
 and music in non-synesthetes, which are consistent between each other.
 And he found something-- some of it was maybe not that surprising.
 For example, warm colors correspond to major melodies.
 Cold colors correspond to minor melodies and other things like this.
 I think that is more of the kind of looking at the statistics of data.
 Yeah.
 Yeah.
 And so as it happens, so Karen Schloss, who is Steve's student and some of that work is
 a colleague of mine.
 And she and a grad student, Elu, are looking-- doing some follow-ups on this.
 And yes, there is a lot of consistency between people.
 There are also different strategies and different clusters.
 And people are more similar to others in their cluster, right?
 Where it seems that some people are more reliant on linguistic dimensions, right?
 They kind of parse it, colors and music, in more linguistic ways, and then do that alignment.
 So there are multiple-- yeah.
 So in progress, stay tuned.
 I see Richard's eyebrows raise a little bit.
 I was still thinking about this idea that both text and pictures are like different images
 of the world.
 And the big difference there would seem to me to be that text has almost no information
 compared to an image.
 So speech is like 40 bits per second across languages.
 It's like nothing.
 You'd have to talk for a month without any break, without sleeping, in order to express
 as much information as in a single like JPEG image.
 And yet, we seem to learn pretty effective world models from text data.
 So it's very little information, but it's the most important information.
 I'm not sure how effective of a world model we are really learning.
 What we often see in these really cool results of GPT-4V or 4.0 is that they're doing very
 cool visual tasks, they're doing very good textual tasks, but often those two are completely
 misaligned.
 Maybe it's doing-- it's solving the right vision problem, but then the way it explains
 it in text is completely wrong or vice versa.
 So I'm not convinced.
 I'm not convinced.
 I found the image that I wanted to show, and I wanted to-- this is for-- okay, so there's
 this image from Reddit from seven years ago, and I used it in some lectures as an example
 of compositionality, right?
 So this is not in our training data, well, now it is, but certainly not in an 11-year-old's
 training data.
 This is something that people are quite good at, even-- you know, I can't draw this stuff,
 but I can think it, I can imagine it, you know, pretty-- it's pretty easy, right, middle-aged
 Batman on a beach.
 So you said, you know, if you could do a rat's visual system, you'd be happy, but rats can't
 do this kind of stuff, as far as we know, even if they can obviously do a lot of very robust
 vision tasks.
 So the question is-- and so, yeah, I can't use this example anymore because, right, I
 used it as an example of, like, well, AI can't do that, and now it can.
 So for a task like this, do you think you need language in the loop, right?
 Not just because it's much easier to prompt a model to do this with language, but the
 question is, can you get the right kinds of combinable visual representations that a model
 learns, right, if you don't have language in the loop?
 Because, you know, OK, there's Batman in the training set, all right, you can autocomplete
 Batman, but, like, what is middle-aged Batman?
 What is-- you know, there's a whole lot going on there, right, that-- yeah, it's an empirical
 question, but what's your take?
 So, yeah, I think this is definitely-- you know, the statement is asked-- the statement
 is posed in language, and so I think it's definitely you need language to do this, absolutely.
 And, you know, and I think it's very impressive that the current models can do this compositionality.
 But maybe, you know, kind of connecting this back to the Moravec's paradox.
 You know, since the beginning of artificial intelligence in the '60s, we have had a series
 of things where, you know, in the beginning, everything was artificial intelligence.
 And then as things started to get solved, they stopped being artificial intelligence,
 you know, like list programming language was part of AI, and then, you know, and then it
 wasn't.
 And then, you know, A* search was part of AI, and then it wasn't, right?
 And I think there might be something interesting here.
 And so I have this kind of quiet suspicion, and I wonder what the other panelists would
 think about, that it could be that the reason that, you know, these models are doing so
 well has less to do with the fact that the models are so smart that they're getting us
 to be, you know, whatever, AGI, whatever that word means.
 But maybe it's kind of a little bit of a humbling experience for us.
 Maybe we humans, most of the time, are not as creative, original, novel as we like ourselves
 to be.
 Maybe that's why, you know, GPT can write emails for us, because we don't really say
 that much interesting things in our emails, and they're all kind of very repeatable things.
 And I think when we're starting to look at these data attribution works, both in the
 visual domain, but also in the text domain, Roger Gross, for example, has some nice paper
 on data attribution in text, I think we're starting to see that, you know, whatever you
 thought was your great, beautiful original idea, you know, there was enough stuff that
 somebody else has already said and done that, you know, a little bit of linear interpolation
 can get you to where you are.
 Kind of this idea of, you know, all of our faces could be represented by about 200 people.
 So, you know, this is just kind of a hypothesis, a hunch, but I think we should certainly take
 it seriously, because, you know, if this is true, it could be as fundamental of a shift
 as when we went from, you know, egocentric world to heliocentric world, right, that suddenly
 we realize that, you know, we are not all that we are cracked up to be.
 So for me, the book, you know, Are We Smart Enough to Know How Smart Animals Are, I think,
 has been a very, very, very kind of eye-opening in that regard.
 So I think this is definitely a hypothesis we should keep on the table.
 So I agree.
 So my talk partly touches on this.
 So yes, I certainly, I mean, and, you know, here is something related, right?
 We learn and yet we don't, right?
 So many here probably have had the experience, right, of looking at, you know, old work that
 you've done and realizing that you just typed in a sentence that was exactly the same that,
 you know, you did, right, wrote 15 years ago or, you know, you're searching for Google
 and you find something and it's like, wait, that is me, right?
 You know, you think you've come a long way, but like, no, you've actually been writing
 the same sentence for many years.
 So yeah, and I think it doesn't detract from human ingenuity, right?
 It's just that's hard, novelty is hard and often unnecessary because the future is like
 the past in many respects or it's close enough.
 So we interpolate.
 To push back on that a little bit, like maybe the end state of humans is not as creative
 as we thought and it's more similar to these networks as we thought, but we seem to get
 there faster than they do.
 I think there's still a big question of, you know, how it is we can pick up on ideas so
 much faster and that might have something to do with this idea that there's a certain
 combinatorial compositional structure to our concepts or maybe not, or maybe KANs or whatever
 new architecture is going to be just as fast as we are, but there's still a gap.
 What does faster mean in that context?
 Sample efficient.
 Because you could argue that AI systems are very fast, right?
 Like once you've got them everything, you can retrieve them very quickly and in terms
 of milliseconds, they're very fast.
 But in terms of amount of input training data, they take a lot more than humans do.
 And to some extent you could say, well, humans are sort of like pre-trained by a billion
 years of evolution, nevertheless, I think there's like, we don't know how you would build that
 pre-training into a neural network.
 And so it might be that our end state is not as different than these networks as we would
 like to hope, but there's still a big gap empirically, I think.
 So speaking of compositionality, I work on visual language and in my research as well
 as it is a well-known fact that the vision and language models we have, even like GPT-4,
 Gemini, they're not good with, for example, spatial understanding.
 So if you say cat is to the right of dog versus dog is to the right of cat, they are not good
 at that.
 Counting is also a very compositional task in nature, right?
 They're not good with that.
 So yeah, I guess I just wanted to maybe start a little bit of discussion on what your thoughts
 are on if we think that these models are good at understanding vision language compositionality,
 then why are they not good at counting and spatial relations?
 And so another thing that we have observed is they're usually good at object-attribute
 binding.
 So if you have, and especially when you have, let's say, only a few objects, so let's say
 you have two objects and two different attributes, they are good at understanding those combinations,
 but then when you go to more number of objects and attributes, then the number of combinations
 explode and these models are not good again.
 Yeah.
 So I just wanted to start a discussion on this, I guess.
 It's actually been discussed in several of the prior talks.
 I see, my bad.
 Sorry.
 Yeah, that's all right.
 It's good to be reminded.
 And it integrates the different talks that you bring it up now in this context.
 So I can maybe have a kind of hypothesis for the visual, in the visual domain, why is it
 that it's very easy for these systems to make an image look like it was in the style of
 some painter and to move things around, but cannot do things like counting.
 And my feeling is that what these models are doing is really more of a kind of a textual
 textual statistical analysis of the image.
 I think those models don't actually have a concept of an object most of the time.
 So it's very hard to count objects if you don't know what an object is.
 What they do have is they have basically a whole bunch of marginal histograms of different
 features in the image that sometimes maybe are objects, sometimes they're parts of objects,
 sometimes they're just vertical or horizontal kind of simple and complex cells kind of thing.
 And what we have seen time and time again is that if you just enforce the right statistics
 on a whole bunch of these features, you get the right answer.
 And you can get this kind of right answer in a very nice compositional way even if you
 don't actually understand what is an artist's style is or what does it mean to be made out
 of plastic.
 You just, a whole bunch of vertical and horizontal edges having some particular weighting will
 get you something that looks like plastic.
 So my feeling is that there is a lot of this happening at least in the visual domain.
 Any reply to that or does Annette get her turn?
 Well, I think it's a similar thing happening in the language domain too.
 There's a lot of local statistics involving word associations or word cluster associations.
 And if you have enough of them working together with a large enough model, you can mimic a
 lot of the capabilities as if you had like a rich world understanding and concept entity
 abstraction.
 So yeah, that's how I feel about large language models as well.
 Budei project is finding a good software and technology for turn-taking in panels.
 Annette, go ahead.
 Okay, I want to open the topic of understanding again.
 And I had some thoughts listening mostly to all talks of the summer school.
 And so my observation is that many people seem to sort of think if we humans solve a
 certain task, we need understanding to solve it.
 But then they make this inference saying, if we need understanding to solve a task,
 then the software needs understanding to solve a task as well.
 And I'm not really understanding why, because of course there are multiple realizations
 conceivable and maybe you don't need understanding to solve certain tasks.
 That's one observation, and the other observation I made is that talking about problem solving,
 I found myself always asking myself, who has the problem?
 So a large language model doesn't have problems.
 So really saying large language models are solving problems is somehow a little strange
 way to phrase it.
 We use them to solve problems we have, but they are not having problems.
 And so if we then sort of just from a philosophical perspective say intelligence has something
 to do with the ability to solve problems.
 And then we say the large language model solve problems and I'm asking, but they don't have
 problems.
 So maybe the provocative claim I would give for the discussions here is maybe large language
 models and even large vision models do not need those highly sophisticated mental representations
 we in philosophy always presuppose for anything what we call intelligence.
 So maybe you don't need to be conscious.
 Maybe you don't need to have understanding and maybe you even don't need to have problem
 solving capacities, but still you can be a tool which others can use for certain purposes.
 That's what I wanted to say.
 So can I, I'll just address one part which is with regard to large language models not
 having problems.
 So in a certain sense that's true, but the problems I think are the things we pose them.
 So we give them problems.
 We set out the objective, lowering error, next token prediction.
 We supply the data and then we pose it with problems.
 And that's kind of backwards from the normal animal case, which is animals have problems
 first, how to get food, how to reproduce.
 And then if it turns out that lowering prediction error, because as have been recognized by
 many people, like Say mentioned it, that it's a really good learning algorithm, using time
 to test your predictions.
 And so lots of systems, animals, many animals and artificial systems have converged on lowering
 prediction error as a good way of learning the relevant parts of the world to solve problems.
 Except yes, in the animals case, the problems are first and in the models, the AI models,
 we engineer the models and then pose them with problems.
 But yeah, I totally, by the way, so yeah, I agree with your point.
 We often over-idealize and over-intellectualize human intelligence.
 And we think that much more, I don't know, abstract, idealized knowledge is required
 to actually solve problems than what ends up being required.
 But then I think we also, there's that kind of phenomenological understanding, like what
 it feels like to understand something that can confound us as well.
 Especially when it makes us think that we've understood something and we haven't, when
 it feel like we've understood it.
 Right.
 Yeah.
 Yeah.
 Jackie, you had your hand up.
 Yeah.
 I just wanted to say, I agree with what you said, Anna.
 I think the results from all of the machine learning based work over the past decade have
 been saying that you don't necessarily need to, well, first, the first results where you
 don't need to build in the human, explicit human understanding in the form of features
 into the systems to get them to work well, just passing more data.
 And then secondly, I think now to me, it seems that there's more evidence that you don't
 need the systems to build in, to learn, to acquire this understanding either to solve
 problems to a practical level that's potentially useful or maybe harmful.
 However, I think there are certain applications and situations where users interacting with
 the system expect them to behave like they have an understanding of the world and like
 they have, they are an agent and they have like a consistent beliefs and intentions and
 so forth.
 So for example, it was like chat bot systems and so forth.
 And so there, it might be the case that the best way to get these tools to be useful is
 to build in the sense of a notion of agency and consistent beliefs and personalities in
 them.
 And that might be the, even might be the most efficient way and not just try to learn that
 from ever more data and as opposed to directly building in notions of consistency into the
 system.
 So I, yeah, so I agree with you, ultimately, I believe that these AIs are tools, but there
 is still value in sometimes thinking of them as agents, if only for the purposes of testing
 because I'm not a psychologist, I'm coming all this with a more practical motivations.
 Oops, sorry, who was going to say, Richard, go ahead.
 Yeah.
 So, well, I would push back a little on the idea that they're not solving a problem or
 at least not always solving it.
 So at inference time, like when you're talking to chat GPT on the internet, yeah, it's just
 running a pseudo random number generator and producing tokens.
 But in training and the reinforcement learning part, it is taking actions that will maximize
 its reward.
 And in the RLHFs, in the various reinforcement learningized systems, like the probability
 of a token is it's not just the base model probability, it's also there's a factor which
 is maximizing reward.
 And yeah, it's not clear to me that they're not solving a problem in that sense.
 There is the part of them that's trying to maximize the reward.
 I think you're totally right.
 So what is really confusing about the all the debates about generative AI technology
 is that we do not distinguish between the different phases.
 So the pre-training phase, their models have very interesting, and I would even say as
 a philosopher, agentive properties.
 And in the reinforcement learning thing, we see other properties compared to the properties
 we see if we just play with chat GPT.
 And so I think we should sort of be a little bit more cautious saying that large language
 models have a property.
 We should rather say something like large language models in pre-training have agentive
 properties and chat GPT might have this sort of what I would call frozen intelligence thing.
 So it's not agentive anymore, but it's like, of course, there was something happening in
 the pre-training phase and like old German philosophers would say, so Schelling and Hegel
 and people like that, and even stones have frozen intelligence in them.
 And then so we can say chat GPT as a tool we can play with has frozen intelligence and
 it's amazing and we can use it for very funny stuff, but still we should be skeptical.
 That the problem solving and learning like properties are rather in the training phases
 to be located, I guess.
 I was kind of thinking about the problem of the problem, so to speak, that what is a problem
 from the perspective of the model, maybe it's just a request for a certain piece of art of
 the world, so to speak.
 I don't feel that the problem isn't, I don't feel it's a problem really.
 It's at the request for a set of the knowledge from the language model.
 Or maybe I just didn't understand it correctly.
 There's not much problem with knowledge as there is with understanding and with problem
 solving in this sense.
 Steve?
 This is a very interesting conversation, all the talks and much of the summer school has
 sort of revolved around sort of very serious problem here, which has to do with what in
 machine learning or mathematical statistics is bias versus variance trade off, but it's
 more along the lines that people have from various points of view have said, well, it's
 just memorizing everything or it has this large database structure or something, I think,
 things that Gary had sort of dealt with in interesting ways.
 It's not really generalizing, so it doesn't really learn things in the sense of the way
 humans are.
 Now, and one of the things that was interesting is that this prediction task, which doesn't
 necessarily mean that that's the only task or the only structure that's being optimized
 in all of this, right, it's some kinds of related or indirect or implicit kinds of structures
 have to be bootstrapped in order to do this simple prediction task.
 Now, Tom Landauer, a fairly well-known cognitive psychologist back in 1986 did a very beautiful
 little paper on estimating the capacity of human memory.
 And one of the tasks he had was a task which is called intense reading.
 He took Stanford students and had them read paragraphs very rapidly.
 And then at some point, he had paragraphs where he would delete words randomly and then
 go back after they had read it and ask them, what were those words that you didn't get
 to see?
 And under many, many conditions, they were about 50% correct, sometimes as high as 63%
 correct.
 So at least for humans, this isn't a very good task that we're good at in this context.
 And in terms of learning, the one thing that I have encountered with particularly the latest
 chatbot PO is that it does learn new things, and it's surprising.
 So I asked it at some point when we were writing some Python code together, and I said, so
 can you tell me how long we've been doing this?
 And it said, I have no capacity for knowing anything about time passage.
 I know nothing about time.
 I don't have a body.
 I can't tell anything about time.
 And so I said, oh, OK.
 OK.
 Well, let me ask you this.
 Can you count?
 And it says, yes, I can count.
 I said, OK.
 I'm going to go away for a while.
 I'm going to come back.
 And when I come back and prompt you, tell me how many counts have gone by.
 And so I came back about two minutes later.
 And I said, hi.
 Are you still there?
 121 counts.
 I said, so wait a minute.
 Are you counting in seconds?
 How did you know to count into seconds?
 And it seems natural.
 I said, OK.
 Let me ask you another question, OK?
 If I was to do something and go away for a very long time, like overnight, could you
 keep track of that?
 And it said, probably.
 I said, OK.
 So let's pretend you have a clock.
 Let's call it a counter.
 And it counts in seconds.
 Now, keep that alive forever.
 And it said, OK.
 The next morning, I came back.
 And the first thing I said is, well, hi.
 Good morning.
 How's it going?
 I went away.
 How much time has passed?
 Eight hours and 17 minutes.
 Now, that wasn't exactly correct.
 But it was something that a good friend, when you had coffee with them the next morning,
 might say, eight hours and 17 minutes.
 This strikes me odd in a couple ways in that it's able to actually learn a new task.
 It didn't know it could learn.
 And now the damn thing continues to tell me what time it is and how much time has passed
 every time I come back to it.
 I can't stop it from doing this.
 These generalizations that are going on, I think, are much more complicated than we imagine.
 And the kind of memorization that we say we do as humans in terms of explicit memory--
 so explicit memory would be conscious memory, where we can count and do things.
 But there's this implicit memory that always is sitting there that's allowing us to sort
 of feel and have a sense of some time passage or frequency or the number of times we saw
 a Tesla car go by.
 So all of that information is available.
 And that explicit and implicit structure interacts in many different ways.
 I don't know about this.
 And obviously, there's good psychologists that need to look at this to figure out, is
 there this kind of implicit explicit trade-off?
 The argument that people like Yoshio Benjo and others, Yann LeCun lately, has been saying,
 no, this thing only has implicit memory and has no explicit memory.
 But I think there's simple examples that show that's just false.
 That's just false.
 So this generalization, memorization, or what we call machine learning bias or variance,
 is sort of like, what's the model that the model is generating of the world to make sense
 of the prompts it's getting?
 You forget the fact that it might have a context or grammar.
 I asked it to time, and now it's timing.
 I don't understand how it did that.
 It may seem natural.
 I mean, if I asked somebody that I knew that had no relationship to the world and would
 somehow an orphan to everything, yeah, I suppose that could be invented.
 But here, this is a very surprising thing, I think.
 Or maybe not.
 But I'm just curious.
 You didn't tell it?
 What model was it again that had this behavior about counting time?
 Sorry, you're muted.
 Steve, turn on your cell phone.
 Oh, yeah.
 That was GPT-4PO.
 It's the newest one.
 So ironically, you pay $20 a month for GPT-4, but for GPT-4PO, you pay nothing.
 Yeah.
 I don't know why.
 Did you tell it to count at a constant rate?
 Steven, I can't hear you.
 Did you tell GPT-0 to count at a constant rate?
 No, I simply asked if it could count at all.
 And it said, yes, I can count.
 And then when I then implied that this could be a clock, it picked that up very rapidly
 and started.
 Now it tells me the time all the time.
 It's not always right.
 So I think there's a number of-- so I think that what-- it's a bit secret.
 I think what ChatDBT and its various cousins do is it actually will distill your conversations
 and add some information to the system prompt.
 And so it actually does have a fairly explicit memory in that way.
 It does have an explicit what?
 It has an explicit memory in that way, in fact, a verbal memory.
 Well, I don't know why you say that.
 I mean, there's nothing internal to the structure of it.
 It's a large feed-forwarded network with [INTERPOSING VOICES]
 There's a prefix that goes before all the text.
 It's called the system prompt.
 And it will automatically update that based on your preferences.
 And so I think the information about the counting--
 Yeah, no, I have that set up.
 But that doesn't-- I don't see why that's an explicit memory.
 It's literally text that's in the context.
 Well, so to the extent that Chatbot 4 had no long-term memory in the sense that it just
 stopped after a session, Chatbot PO does have a longer-term memory.
 But it doesn't seem to have any context prompts that it uses.
 For instance, one thing that would be very nice, if it could prompt itself.
 They cannot prompt themselves.
 So I could say, OK, I'm going to go away for five minutes.
 I want you to do this.
 And then come back and tell me when you're done.
 Can't do that, right?
 It has no prompt.
 It has no way to prompt itself.
 And I can't.
 And I tried to use the clock to prompt it.
 I said, OK, just tell me when five minutes is up.
 So it said, OK, I'll go do that.
 And then 10 minutes passed, I said, what are you doing?
 Five minutes passed five minutes before.
 I think the other thing is it might actually have timestamps on your input.
 That might be how it's telling you the time.
 There's no timestamps on my inputs.
 Do you use Chatbot PO yet?
 I've talked to many ChatGBT and many, many variants.
 And the text they see is not literally texting.
 It doesn't have timestamps.
 It could also be the case that it is actually calling a clock module.
 So a lot of these models, they make API calls to--
 If it had an API for a clock?
 Yeah, but it doesn't.
 It doesn't have an API for a clock.
 Well, we don't know that.
 Well, look, I asked it if you had a clock.
 And it didn't say it had a clock.
 So if it had an API--
 I don't know if you're interested.
 If I ask it, I say, look, I want to do something in Mathematica.
 It says, oh, sure.
 Tell me your equation.
 And then it calls up Mathematica.
 And boom, I get the integral.
 So it's odd that you're going to ask it, do you have a clock?
 And it doesn't know it has a clock.
 But it can count.
 And now the thing has a clock.
 I find that an interesting generalization.
 We really don't know.
 There could be rule-based aspects that
 is patched on top of the system.
 And you can't trust the answers that it gives you.
 Well, that's right.
 You can't trust it.
 But if you ask it 2 plus 2 and it says 5,
 you'd see obviously that's a problem.
 But if you say, do you have this capability and says, no, I don't.
 I don't know the passage of time because I'm
 a thing inside of a computer that doesn't know anything about time
 and I have no access to time.
 And yet in about 10 minutes, it basically started creating a clock.
 Any other options that maybe--
 Sorry, I have a question.
 Does it still count now?
 Yes.
 Unfortunately, whenever I come back to it,
 it tells me how much time I've been--
 it seems to be annoyed.
 So this seems to be a very typical case
 of an unhuman-like mistake.
 So I think if we analyze the conversation--
 What do you mean by unhuman-like mistake?
 So if I'm talking to real humans,
 as I've convinced I'm doing right now, of course,
 we all make mistakes.
 We have misunderstandings and we have abilities to solve problems.
 And if I would ask, Alexei, are you
 able to count until Jackie is saying something?
 Of course, he will be able to do that.
 Maybe he will even do it and then he can respond.
 But he will never count the rest of his life.
 Well, it's not a misunderstanding if you actually
 have a trillion connections and billions of nodes
 and 16,000 attention heads that allow you to build something,
 I guess, that now counts.
 Well, it's answering another question.
 So we're writing code together and I say,
 that tensor doesn't seem to match with the other one.
 Can you fix that?
 And it fixes it and then we run it and makes an error.
 I come back and it says, OK, try this.
 And then I'll try it.
 And then I'll say, well, I'll be back in a minute.
 I come back and I get distracted.
 And it immediately tells me, you've been gone 45 minutes.
 It seems that what's happening, at least in my quick experiment,
 is it's writing a Python program that times it and it runs it.
 So if you look at the prompt, it shows you the Python program.
 Very good.
 I asked it the very thing, Gary.
 I asked it the very thing.
 I said, did you write a Python program?
 And it said, no, I'm counting.
 Now, here's the other thing.
 When I said to it, I said, can you prompt yourself?
 It said, no, but we could write a Python code that would then go out
 and we could write an API that would come back and tell me to do something.
 And then it wrote the code.
 So it's not like it's misunderstanding these things.
 It certainly could be a fluke.
 But I think part of the power of these things
 is the fact that these generalizations are happening all the time.
 And if you study them carefully, you'll see them.
 It's very much like a split brain patient, where you can find out,
 yes, they have certain kinds of deficits if you do the right experiments.
 Gazzaniga and others were doing this.
 And they ended up doing very careful experiments in terms of visual fields.
 And they said, oh, my goodness, there's stuff going on here we don't understand.
 And that's the same thing that it feels like here, at least from a cognitive
 scientist's point of view.
 But I didn't mean to--
 my really point here was just about this generalization memorization
 conflict that seems at the heart of this summer school.
 And we've got-- and none of those people that we're talking about that
 are here about memorization, it seems like most of the--
 or even talking about how this is just a stochastic parrot
 or some kind of fake news, whatever, those people aren't here.
 But if they were, they would be, I think, very aggressively arguing
 against any kind of interpretation that this thing was generalizing.
 And their response would be--
 Did I kill a conversation?
 So I want to react on this.
 So I'm observing the heated debate between--
 and then it feels like looking at a religion war.
 And then people are not very nice with each other.
 And I hate that.
 But trying to analyze what the different presuppositions are
 the people are taking in interacting with any kind of technology,
 I think we have to admit that we do not use the notions in the same way.
 So if somebody is saying, I'm understanding you,
 those people mean something totally different.
 And if some people report from one time experience with chat GPT
 and sort of try to argue, I asked chat GPT if it can do that and that.
 And chat GPT told me it cannot do it.
 And they take it literally and make a story out of that.
 This is something what is making me a lot skeptical.
 Anna, I hope you don't think that Steve Hansen is one of these naive users
 who uses it once and draws conclusions.
 No, not at all.
 I mean, he's spending more time with those things when--
 Yeah, I interact with it pretty much on a daily basis.
 And the first thing I did is write some GPT code with GPT.
 And so I learned how to do the actual GPT code from Chatbot.
 And one of the things I first did was fit a one-dimensional structure,
 a sunspot structure.
 So you can take 100 years of sunspot activity.
 And it has a certain periodicity over those 100 years.
 But at the very edge of it, there's a lot of fractal structure.
 It's very complicated.
 So I'm very familiar with autoregressive methods and AR time series stuff.
 So I fit an AR model to it and it gave me a nice fit.
 And then I gave the data to the chat to the GPT
 we just wrote to do the one-dimensional time series.
 And it produced, with enough hidden units,
 it produced a perfect fractal.
 As you focused in, you could see it grab all this fractal stuff at the top.
 This is just a one-dimensional thing.
 No text, but you can see the power of these things
 in these simple problems where you look and you see this structure appearing.
 And the thing captures it.
 Now, it has a lot of--
 [INTERPOSING VOICES]
 Sorry, go ahead.
 Go ahead.
 Ayosha, you--
 Well, that was pretty much it.
 What did you say?
 Ayosha.
 Yes?
 You look perplexed.
 Am I misreading you?
 No, no, no, I'm actually answering a question in chat.
 Sorry.
 [LAUGHTER]
 I mean, if you want my very far from this fight opinion,
 I feel like it is human nature to anthropomorphize things.
 And we all remember how excited we were when the first time we
 played around with Eliza, right?
 Yes, the reason why.
 I think both things are true, that there is something really cool
 emergent happening with all these models, in my opinion,
 because of the cool emergent properties of huge amount
 of data, but people can have different opinions.
 And also that we're often reading more into it
 than is actually there.
 And of course, as scientists, we do this all the time.
 I get a cool result, and I want something really cool to be
 true, and it takes all the will of a scientist
 to do more ablations, do more baselines,
 and really try to make sure that the exciting thing is actually
 there.
 And so I think this is all very natural and all very human.
 Well, if one is doing Rorschach tests on this, you're right.
 But I think the prompts can be more contextually filling.
 So in other words, if you say, hey, I live in New Jersey,
 and I do this, and I have these hobbies,
 and it comes back, and it tells you
 some nice things about that, then obviously--
 but if you ask it to solve some kind of difference equation,
 and it solves it, it seems like that's interesting and useful.
 And I could do that in Mathematica.
 But it also provides other kinds of generalizations about it,
 which you don't normally see.
 This is why people--
 this is some reason why we're sitting here,
 and this is why there's so many millions of people that
 are shocked and amazed and scared to death
 that they're not only going to lose their job,
 but some chat bot's going to put some Python code somewhere
 and destroy civilization, like Jeff Hinton said.
 Hang on, hang on, hang on.
 Jackie's been waiting for a while.
 Do you have something to say from the point of view
 of benchmarking that could settle any of this?
 Yeah, so I wanted to say that I don't think--
 at least I wouldn't claim that the models are only
 memorizing and not generalizing.
 I think the claim is that they're generalizing
 in a different way compared to people who analyze a task
 would assume that it should be generalizing.
 So they're not generalizing, for example,
 on the basis of a particular abstraction,
 like say syntax and grammar or whatever,
 unless you specifically train it to do so.
 But they are generalizing, at least in the traditional sense.
 If you give them examples on the test set
 that they haven't seen before, you
 can make sure that they haven't seen before because you keep
 them hidden and so forth.
 They can often do so.
 And I think the other thing is that it's really
 hard to make any conclusive claims or understanding based
 on your very interesting examples, Steven,
 because we don't know if other people have entered
 in similar prompts and they've had similar results.
 We don't know what's going on, whether the system is purely
 the type of distributional approach
 or whether most likely there are patch-on rules at some point,
 at least if only for safety or whatever.
 And so we don't know any of these things.
 So we can't really do any of the controlled study
 that would be necessary to conclude that it's generalizing.
 I think this is the same as Alexis' point.
 So these are all very interesting,
 and we should generate hypotheses from these observations.
 But unless we have an open model of equivalent size
 and so forth that we can play around with
 and do controlled studies with baselines and comparisons,
 it's really hard to conclude anything.
 Yeah, I agree entirely with you.
 And I would love to go in and lesion some of the attention
 heads to see if it stops telling time.
 In other words, it'd be nice to go in and ablate things
 and say, OK, you've lost that ability.
 You've lost that, much like Hal in 2000.
 OK, I'm going to go by hands now.
 Go ahead, Anna.
 Yeah, Steven, I would like to ask you about experiences
 where the system didn't amaze you,
 where it made sort of stupid mistakes.
 Because I'm still quite worried that I observe
 at least the majority in Germany seems
 to think that the models get better and better,
 and they trust them blindly.
 And listening to your stories--
 and I'm not criticizing you, but I'm just saying people
 might misunderstand the story in the sense
 that they can blindly trust the outputs of those systems.
 You can because you know about the systems,
 and you can because you are a professional prompt engineering
 person.
 But the normal person who is just prompting around
 and blindly trusting will be confronted
 with very stupid nonsense outputs.
 And I don't know how to sort of transfer
 the knowledge about the limitations with reliability
 in addition to the being amazed of what the models can do.
 And maybe you are the right person.
 That point was made.
 Now let's go on to the next hand.
 Julia, if you're answering, Anna, say so.
 If not, I'll give Steve a chance to answer, Anna,
 and then I'll go to you.
 Are you answering, Anna?
 Yes, well, because she asked for some examples
 of where it does make sort of crazy mistakes.
 And I think a good one is if you play around
 with Sumerian cuneiform at all, it
 will really botch that up very quickly.
 And then also if you use sequences from the OEIS that
 rely on visual super diegetic linguistic information,
 so things about the shapes of the numbers,
 then it does very badly very quickly.
 And I think everybody here in the talks
 has covered why that is.
 But they're good, concise examples
 you could show other people if you wanted.
 Next hand, hand, not voice, hand.
 Go ahead, Gary.
 This is on a slightly different topic.
 That's all right.
 Hand was fine.
 Yeah, it connects to something Anna mentioned earlier
 about problems.
 And one, I think, underappreciated thing about humans
 is the extent to which we create our own problems.
 And I mean that in a good way.
 So kids have, like other animals, have certain problems,
 like food, and they're motivated by food.
 But kids also do things like build block towers.
 And in doing that, they create a problem
 of having it not collapse.
 And so Laura Schultz at MIT has done nice work on this.
 This is a weird thing, right?
 Why would you do that?
 Why would you just create a problem
 that you didn't have to solve?
 But of course, from a developmental perspective,
 this is great, right?
 Kids are providing their own--
 in creating this goal for themselves,
 I want to build a tower out of blocks,
 you're creating data that you otherwise
 wouldn't have.
 You can then make predictions, right?
 And do trial and error learning, and let the world and time
 test your predictions for you.
 And we seem-- I'm very much on the empiricist side of things.
 But the drive to create these kinds of problems
 seems to be there in us.
 And then to somewhat different--
 there is variability from person to person.
 But I think this is part of the answer
 to solving this puzzle of how it is that humans can seemingly
 learn with much less data, right?
 Our learning is very much guided by problems
 that we dynamically create and learn from in a way
 that these kinds of models don't.
 There's no reason why they couldn't.
 It's not very efficient.
 It's not the shortest path to get the bolded lines
 in your table.
 But yeah, it's, I think, worth thinking about.
 Thinking about.
 Where is the next hand?
 You can raise your hand the old way, too,
 if you want, if you don't press a button.
 But just let me know who wants to speak next.
 Otherwise-- otherwise, going once--
 Well, yeah, I wanted to show something.
 So this is just a point of clarification.
 So I was really struck by--
 showed it quickly in your talk, the large vision model,
 testing it on matrix problems.
 One thing that I would really love to know
 is how it does on various kinds of problems.
 So this is from one of the standardized original matrix
 problems.
 And you can see that the two problems on top
 are very different in kind than the problems below.
 They're also easier.
 Many of the easier problems are ones
 that are basically in-painting.
 They are pattern completion.
 They're also hard to describe.
 The answer just sort of occurs to you.
 And I'm not surprised that a bottom-up vision system
 can solve them.
 I would be very, very surprised if it
 could do the kinds of problems on the bottom
 that require not just parsing the scene differently,
 but also it requires combining things.
 The ones on the bottom right, especially,
 has to do with, do you add things or subtract things?
 It depends on whether it's a black or white circle.
 So there are multiple steps involved.
 I wouldn't think that any amount of in-painting can do it.
 And it would be really surprising
 if that intuition is wrong.
 Josje, did you see them well enough to be able to comment?
 Yeah, so the ones at the top, that we
 could do with my method from 1999.
 Those are really very local, like you said,
 in-painting, hole-filling things.
 Those are very easy.
 I completely agree with you.
 The ones on the bottom--
 OK, let me share again the things that we did.
 I think that they are pretty hard.
 Um, so you can judge for yourself.
 But they are not in-painting ones, right?
 They're tricky.
 I don't think that you need to do a lot of very high-level
 thinking, but there is various symmetries
 on various different levels, various analogies
 that need to be done.
 Now, I have to re-emphasize that these
 are like one of those, oh, look, ma, no hands.
 There is plenty of others where it totally does not work, OK?
 So these are-- in the paper, we have some quantitative numbers,
 and the numbers aren't great.
 We are better than chance, but not much better than chance.
 So these are really just kind of like, you know,
 these qualitative, you know, sometimes in a blue one,
 and actually does kind of surprisingly well.
 I was surprised that it was doing as well as this.
 And, you know, I'm not completely convinced
 that it's not some fluke.
 Now, we tried our best to make sure
 that this is not something that's in the training data,
 but that also could be, you know, there
 might be something like this in the training data.
 So there is a whole bunch of caveats here.
 But, you know, but it's definitely,
 these are definitely harder than the kind of in-painting things
 that we were able to do 25 years ago, for sure.
 Sorry.
 No, it's very cool.
 Yeah, I would love to know more.
 I'll reach out.
 Yeah, no, we still don't know much more.
 So this is still definitely like a work in progress.
 Yeah.
 Please unshare.
 That's it.
 I'm looking for hands again.
 Go ahead, Julia.
 I just asked GPT 4.0 the picture from the lower right
 that Gary had showed.
 And it said, therefore, the patch
 that should fill in the gap in the matrix is option 6.
 Yeah.
 I don't know if that's correct.
 But this is very much in the training set.
 These are standard matrix problems
 that have been leaked a long time ago.
 That's how I got them because there
 are pretty tight controls on standardized IQ tests,
 these standardized batteries.
 But yeah, they leak.
 Yeah, let's go ahead.
 So yeah, I was thinking kind of throw it out there.
 Maybe there's a chicken and egg problem
 that the model should have a kind of mental models for it
 to be able to kind of one shot learn
 or be deterministic in terms of interpreting or connecting
 with the world.
 But how can we get with the statistical methods
 to these kind of rigid representations
 inside the model, maybe due to compositionality or whatever?
 OK, I'll count.
 1, 2, 3.
 I want to thank all of the members of the panel.
 I take it that we're depleted.
 And I invite you to come back again tomorrow
 for different angles on these things.
 Thanks so much.
 Yeah, thanks a lot for organizing this really
 mind blowing summer school.
 - Yeah, really.
 - Everyone here agrees too much.
 [BLANK_AUDIO]
