 No, welcome to the panel. Now we have a new, he was in the last session as well, but someone
 who has not yet given a talk. Hi, hi, Don CSV from which will marry. Okay, say a little
 bit about yourself. Before we start. Hello, I'm good evening. Good afternoon, right? That's
 the afternoon. Okay. And I'm a lecturer for an LP at Queen Mary, the department of computer
 science, electronic engineering. I work on the intersection of NLP and computationally
 linguistics, although I suspect a different sort of a linguistic than it was presented
 so far, more, I would say functional slash a structuralist view. So I hope you will accept
 me. And I, the talk that I'm going to give next week is about the language change and
 language evolution, which is my main thing. But also about model explainability and other
 sort of interaction with the human cognition. Okay, thank you very much. Hi. Now, I made
 a very risky promise and I will stick to it if it works. But if it starts to get an archaic,
 I'm going to have to reclaim control. So you all, all of you who are at panel level can
 speak, it's an open mic, you speak whenever you want, but try to observe some of the courtesies
 of exchange and don't be too long. Because one of the things that wrecks this is if somebody
 can't wait for you to just stop talking, do a chunk size intervention and then assume
 that there are other people in the panel as well. With that, I invite you to start or
 intimidate you from saying a single word. I have a question for Rami, which I put in
 the chat, which is about your, the very first point in your summary that large language
 models do a terrible job at constituency. And if I imagine myself counterfactually,
 an LLM person, I might say, well, 70% is at least approximate. And you're saying it's
 not even approximately right.
 Yes. Thank you. I think people would say that. And I think the question is what their goal
 is when they say that. So if the discussion is whether they're building a tool that is
 good enough to serve all kinds of practical purposes, then maybe that's a good enough
 approximation. If they're saying that it's a theory of who we are, then that's clearly
 not a good enough approximation. So if they can't, if these models can't learn the kinds
 of recursive rules of the, like the ones with A's and the B's and the C's and the D's and
 lots of others like them, if these models simply can't learn them in the way that we
 learn them, then that already disqualifies them from being models of us. And it certainly
 prevents them from, so even if they could learn these things perfectly, which they can't,
 that would not yet explain why we find constituency in language after language. So they would still
 have a lot of work to do in explaining that. But if these models can't even learn that
 properly, then they're not even in the game. In this respect, again, an approximation might
 be good enough for lots of other purposes, but for establishing these models as serious
 contenders for the best explanation for who we are, they need to, at the very least, be
 able to show how they can learn these patterns. And then after that, they would need to explain
 why we find these patterns in language after language. And at this point, these models
 are not even close.
 Okay. So I'm going to keep pretending to be somebody that I'm not. So I'm going to say,
 well, with respect to the filler gap, they were at 70%. And that's probably where a lot
 of people would be as well. So why should they be better than a person? And number two,
 it isn't necessarily claiming that it's a model of how people do it. It's a model of
 how it can be done. And it's doing it more or less okay, just with respect to that one
 example. So I thought your comments about cross-language similarity were really interesting.
 I'm not sure how people would respond to that. But just with respect to the use of grammaticality
 judgments in general as a test and what's going to count as being good enough, you were
 taking a very strong view. You were saying it's not even approximate, but I think most
 people would consider 70% to be approximate. And that was what your chat GPT-4 model did.
 So that was only the case for GPT-3 and not for any of the other models. And so even models
 that were trained on the equivalent of hundreds of years of linguistic input, and some that
 were even bigger than that, still were well below 50%. And that was below 50% with something
 that is actually a very lenient criterion of success. So we tried to make it very easy
 for these models to succeed in the sense that even if they provided... So what they talk
 about is probabilities, not grammaticality, unfortunately. But if they assigned even that
 slightly higher probability to the correct continuation, we considered it a success,
 which is not... It's way too easy because they could still think that the incorrect
 one is really great, just not as good as the other one. But even with this lenient criterion
 of success, and even with hundreds and sometimes thousands of years of linguistic experience,
 they still failed. And it took GPT-3 with pretty huge co-prime, who knows what else
 that they're doing there at OpenAI, to get to 70%. And I think it's enough to say that
 these models fail at this point to refute the argument from the poverty of the stimulus.
 The claim... Because that was a response to a claim that says that these models learn
 these patterns and show that the argument from the poverty of the stimulus in this case
 falls apart. So showing that this is what happens when we look at those fillary gap
 dependencies that we looked at is enough to say that that's not the case. No, they don't
 show that the argument from the poverty of the stimulus falls apart, which is all we
 were trying to say there.
 So a somewhat related question I have is about the tests that LLMs use, which is often to
 have two minimal pairs, as Futrell was saying, and the LLM has to say which one is better.
 I wonder if you have thoughts about whether or not that's a good way of doing it, because
 I can see objections to that way of doing it. Namely, you're telling the model that
 one of them is better than the other. You're highly suggesting that. So in an experiment
 with humans, that's not something I would do. I would ask people to rate independently
 what the items are so that they're not learning something from the instruction that isn't
 part of their knowledge of what I'm trying to get at.
 Charles, it's an open mic.
 I have just a couple of addendum points to make. One is that the filler gap dependencies
 that children acquire in various languages can be demonstrated sometimes very early,
 at the age of three, if not younger. We know roughly how many sentences they've heard
 by that point. So that's one constraint. Another constraint is actually the typological one.
 This is going to go into the "we" so I won't go into it here. I think those typological
 arguments for part of stimulus or UG has been overstated. Because these are tendencies,
 these are not absolute universals. Very few are. And also, and linguists have been engaging
 in various kinds of overfitting in trying to account for every little data point perfectly.
 So of course, that's a call for an alternative approach, namely, how do you learn these filler
 gap dependencies. That's some of the work I've been doing. I won't talk about it here.
 But again, the work has to be responsible in the sense that you have to take a realistic
 amount of data to acquire the language particular constraints on the filler gap dependencies.
 If you don't believe those things are baked in. And also, of course, you have to assume
 something, namely hierarchy and so on. It has to be part of the part of the party of
 stimulus arguments. I'm just saying that baking in all this kind of substance of cross-language
 doesn't seem to be the solution going forward.
 When you say "overfitting," do you have a particular example in mind?
 For example, the bridge verb thing I was talking about earlier. So you can say, "What did John
 think Bill saw?" But you can't do it for complaint or quip and so on. People have to spend hundreds
 of papers are trying to predict which class of semantic verbs can allow such properties.
 And at the end, the null answer is actually there isn't none, which is actually Chomsky's
 position back in the 70s. Namely, these are just accidents. You hear them. And there's
 no theory of why complaint shouldn't be a bridge verb, but something like C could be.
 So if you try to have a theory, I try to predict which verb is going to be a bridge verb. And
 the actual fact is that is a bunch of accidents, then that's overfitting. I think these things
 are quite common across languages, according across the theories of linguistics we engage
 in, including, let's try to say why arguments are easier to extract out of arguments rather
 than adjuncts. Linguists know about these things. These are null hypotheses that none
 of this is actually absolutely true. And whatever they should have to be is a result of the
 data and the result of learning. So in that sense, I think that is, I think many, many
 cases of overfitting in our business. That's another way of undercutting the typological
 argument for universal grammar.
 If I can jump in about the typological argument, I just want to discuss with you and sort of
 have your opinions on an attempt that I read about trying to build large language models
 for languages in which actually our datasets are small. So English is easy because we have
 all these large datasets, so you can train large language models. But what do you do
 for Zulu or Sami or something? And so I read one publication, I'm not sure how, so these
 are new efforts, suggesting that one thing you could do is like grafting with plants
 or something, train your model on English first, and then retrain the model on the small dataset
 you have for one specific language. And these actually surprisingly, so these are not open
 AI, right? So these are smaller open source models, but they seem actually to do surprisingly
 well using this kind of method, as if actually training on any language could help extract
 something about languages, and then retraining on another thing, another language, even if
 the dataset is much smaller, would then just teach you specifics about that language. So
 of course, I'm not going to go all the way to say, well, what they extract is UG, so
 some generalizations that hold across languages, but it does point towards something like if
 you need a lot of training, well, you can do that on almost anything because languages
 will share things and that's good enough to begin with, and then you will learn some specifics.
 So I'm wondering, of course, so I don't know what, English is always the basis for this,
 but I'm not sure what other languages were trained or used, but I think these are exciting
 experiments that could be trained or tried, sort of pairing up typologically similar or
 dissimilar languages, et cetera, that could actually get to some of the typological issues.
 My understanding is the so-called transfer learning in LP has been going around for some
 time, and is not as good as we think they are, even for typologically, sometimes fairly closely
 related languages. On the other hand, none of these is really about a content model, right?
 Because of course, when humans learn these, a much smaller amount of data for all languages.
 And now it's true that training on some language with the loss of data is going to get something
 out of the data, just as you can, if you don't have a first language, I think, deprived of
 a critical period, you probably won't learn a second language. So you get something out
 of this huge amount of data. But exactly what it is, I certainly would not equate it with
 typological generalizations out of- So essentially, it's enough that it's better
 than training purely on the small data set, right?
 Right. Those are not even trainable. Some of the models are not even trainable.
 Yeah, some are not. But let's say if it were, you could train it first, or you would train
 it first on the small data set, then you train it in two steps, one on a big one, and then
 a small one. And so there's an advantage- Just a second. The people in the large audience
 can't see who's speaking. They just hear it. So when you make your first-
 Okay. Your first intervention, you should mention
 your name. This is Judith Youngveen, who's speaking now. And it was Charles Young, she
 was speaking with. Go ahead, please. Right. No, so that's it. So I'm thinking that that
 actually could be done more systematically, not just for application purposes. So I need
 the model in language X because I need it for something practical, but rather exploit
 systematically in terms of looking at typological variation. And say as a baseline for comparison,
 you could have various conditions, pre-training one way with one language with another, et
 cetera, compared to no pre-training. So just throwing this in there.
 So I think a lot of work. So I agree in general. Say your name, say your name.
 Yeah, Chaim, Queen Mary. So I agree with Judith and Charles roughly, but not surprisingly,
 what the linguist may think are typologically similar languages is according to human eyes.
 And the way that I would say different modes that were pre-trained originally on different
 languages show similarity would be quite surprising. So not always it's better to train on English
 than transfer to the target language, even for German or French, for example.
 Transfer from English to French is easier than transfer from English to German, even
 though English and German are much more related because of the large vocabulary English and
 French happen to share as a result of- But we're looking at only just two examples.
 If you can expand the view to take dozens of languages, then you may be surprised to
 see that certain languages you would never imagine that would have like a beneficial
 influence in transfer emerges. But what's the point of that finding? What's
 the relevance of that finding? To speed up a transfer learning to improve-
 Okay, for applications. For applications, but not for understanding of the human language
 and common systems, right? Well, you know, at least to rethink about
 what typologists define as similar languages and what- So typologists usually have a list
 or a wish list of important features or linguistic features that they, you know, describe are
 important or critical. And perhaps not all of them are as critical as we thought. And
 maybe word order has a much more significant in terms of model processing. And again, we
 are not assuming, and we're actually assuming that language models do not acquire language
 the same way humans are. So it's not surprising that whatever linguists have identified as
 key linguistic features are not that important for language models.
 Right. I mean, unless you are a super, super, super nativist, do you think all the typological
 structure of a language is encoded in your genome? Typology has very little status in
 the study of the human language faculty. There's a useful distinction as known as Chomskian
 universals and Greenbergian universals. Those are very, very different things.
 Okay. I agree. I think it's okay. So it's a bit unfortunate in that regard.
 Why? Why is that unfortunate? I mean, one has said universal is the result of history,
 invasion, war, and so on. The other is kind of a human biology and how the brain is organized.
 Why is that? It's not fortunate, unfortunately, just different things.
 Yeah. But the fact that it is almost not represented entirely in many linguistic faculties and
 departments, this is unfortunate, not because, you know, it's, you know, it's more or less,
 yeah.
 Okay.
 Is the open mic dead? I haven't run out of ideas. Christian Lebiere, could you turn on
 your face? Christian? You never know whether behind an open mic might be an empty room.
 Well, I was going to ask Christian, because he is from Carnegie Mellon University, which
 is where Simon was. I wanted to have his reaction to Charles Young's "Satisfacing Proposal,"
 but I can't seem to get through to him. Gary? I have a hand from the audience. Yes, go ahead.
 Just a second. Let me enable you to go ahead. Allow to talk. Go ahead. Gazelle KP, go ahead.
 You seem to have this immediate enabling of the audience is a little bit risky as well.
 I'll try putting it into the panel, but I'm worried about this. Okay, I don't know who
 you are. You asked a question. Gazelle KP, speak up or I'm taking you out. All right.
 I mean, there has to be disagreement. I came out swinging, made outrageous claim. I mean,
 there has to be some reactions. I'm saying learning is not maximizing. I'm saying you
 only need to fit the data well enough. I'm saying that the form of the rule and the complexity
 of the representation and the coverage of the representation are to be separated. Those
 are, I hope, controversial claims. And so I'd like to hear some reactions. Did somebody
 say something? I had a question for Judith, actually. I could come back to Charles at
 some point because maybe that was outrageous. I'll have to think a little harder about it.
 But yesterday, or I think it was yesterday, your talk, and I really enjoyed it, and you
 were talking about sort of the acoustic structure and that there was an enormous amount of covariance
 in the way I would describe it related back to syntax or word order. That is, there's
 a kind of almost a priming or some precursor to what may emerge as a syntactic structure.
 So often I hear from computer scientists and some of my favorite best friends, cognitive
 scientists, well, these chatbots, they get like millions and millions and millions of
 more data than a human baby. Now, to be factual for a moment, the chatbot GPT-4, which has
 about 900 billion weights, almost a trillion, and it gets about a trillion tokens, or let's
 just say lexical tokens. And depending on how many words or sentences that a baby or
 in developmental hears, the way this scales gets interesting to me is partly also the
 problem that when people say this, they don't know what the sufficiency is of the training
 data. In other words, I did use a trillion tokens. Maybe I only needed half a billion.
 Maybe a hundred billion would have been fine. We don't know. No one did those experiments
 yet. So I'm just wondering, for someone like you who probably knows this answer implicitly,
 how do you think the kind of training tokens and the kind of developmental basis are saying
 this? Yeah. So I think this is a really exciting question. And I would say there are at least
 two levels at which this could be answered. So for something like word order, which is
 really, you know, basic, fundamental, just, you know, it's robust, right? So your language
 might have mixed word order, it might have this, it might have that, but the basic sort
 of head direction. So the place where you put those little functors in your language
 is really, you get a lot of exposure to that. So depending on the type of the language you
 speak, but these really are very common. They are everywhere. So in a single sentence, you
 get three, four functors very often, and you get a lot of exposure. But that's also something
 that babies learn extremely early. So the youngest we tested in these paradigms and
 get success is six months. That's really very little, right? So, and we never tested four,
 let's say. So essentially, I think to get something as basic about the grammar of your
 native language, very little is sufficient. And so even if the chat GPT models or anything
 was trained on half or only 10th of what it was trained on, that would still be a lot
 more than what a baby gets. And actually the strong position here would be that these basic
 grammatical distinctions are cued at multiple levels. So like frequency, prosody, and others
 in such a way that you just can't miss it. Like, you know, you're being hit on the head
 with a hammer. And so more subtle things of course take longer to develop, partly because
 of resource constraints that Virginia talked about, partly maybe because really you need
 more input to just fine tune certain things. So one tendency in development, like language
 acquisition or developmental work is to push things back earlier and earlier, right? So
 you find something, you know, one year old, then you try a six month old, et cetera. But
 I think these really are very early, so basic things. Then it is also true that not all
 babies get the same amount of input, right? So we saw this in the developmental talks
 between say high SCS, low SCS, so high socioeconomic status, low socioeconomic status households.
 There are really magnitudes of difference in how much infants get talked to, but it
 seems like those have an impact at a somewhat higher level. So say embedding or the size
 of the vocabulary, but not something as basic as let's say word order or the morphological
 type. So we very rarely talk about this because English is very poor in morphology, but if
 you're learning Turkish, Basque, Japanese, Hungarian, or whatever, then you have to deal
 with that from the first day, right? You can't wait and say, first figure out word order
 and only later morphology. Morphology is everywhere. So these kinds of basic things, like whether
 your language is agglutinating or whatever, are set early. That seems to be true no matter
 how or where you look. And so this brings the problem for large language models. How
 to say it? It makes it even bigger because of course the earlier you learn something
 or show evidence of knowing it, the less experience you had with those things. But so I would
 think that there are certain things that really are queued redundantly at multiple levels so
 that you can't miss it.
 Well, so paradoxically then, I mean, Linda Smith's experiments who had hooked up some
 cameras and audio to babies' heads and had them wandering around for days or months or
 so, and she has this huge archive out there. Now, in some cases, one would argue the babies
 have a huge advantage here in that they've got all this visual and tactile and other
 input if they're crawling around the floor that poor chat bots, you know, just get real
 text from the 17th century and maybe from some magazines and who knows where else, YouTube
 videos. And it's just not this qualitatively we're still missing something. So the claims
 that gee, chat chief is getting so much more data. I think it's way too early to actually
 say anything about that relative to human developmental language. And I think the redundancy
 in your story actually increases the information. In other words, babies are getting an enormous
 much more data than say a chat bot who, you know, just gets a bunch of text.
 So maybe I'll answer first, but I think this is something that we discussed that came up
 yesterday in the grounding issue. So essentially, I think in a certain sense, this is potentially
 true for a lot of semantics, but I think much less for basic syntax in that nothing in your
 physical environment would help you figure out something like a definite article, right?
 So a lot of or, you know, prepositions or post positions that are not semantically rich.
 So something like two, whatever. So I think that's potentially true for say, word learning
 or some aspects of learning semantics much more than it is true for basic syntax. Plus,
 I think another issue is by six months, babies start to show these basic notions of the grammar
 of their native language and their basic vocabulary. At six months, they don't locomote by themselves.
 So it is true that they are surrounded by a rich physical world, but it's not like they
 can seek out objects or something, right? So that comes later. That comes once they
 start crawling or walking and then they can wrap something, take it to a caregiver and
 say shake it and ask for what it is. And then the caregiver says, oh, that's a book. You
 see a book, et cetera, et cetera. Nothing wrong with random sampling from the beginning
 if you're just bumping into stuff and essentially understanding something about the kinetics
 of your articulatory system and babbling and so on. All this stuff actually is useful and
 grounding, right? Oh, I fully agree that the system is set up in certain ways that you
 get your own feedback, your own babbling, but even that actually starts around there, six
 months, four months, eight months. Whereas some of the knowledge they show already at
 that time, so they had to learn it earlier. So yeah, so they do get some visual information
 from the faces they interact with. Of course, some caregivers at least will still interact
 with a very young baby in an object oriented way. So by giving a bath, they would show,
 oh, you see, this is the little duck or whatever. But that happens with all their infants and
 not so much with the very young ones. And yet by about six months, they start to show
 a lot of very specific knowledge. So I do think you're right. And we discussed this
 yesterday as well. It's not clear whether multimodal redundant convergent information
 is necessary. But you could argue it's harder, right? So you can sort of argue this both
 ways, that you have more, but at the same time, how do I coordinate all of that? How
 do I figure out what goes with what, right? But yes, I think that's definitely an issue
 to discuss. Right. Anything that is called a foundation
 much is multimodal and strong. It's very likely to have many, many binding problems. And the
 question about how the binding problems are solved through simple exposure and how what
 appears to be a context-free grammar, I'm sure Ronnie might argue with me, it seems
 to pop out of these growth systems. Some kind of circuits are grown inside the attention
 has it actually seem to be processing language. And this is, I don't want to get into Stephen's
 realm of consciousness or, or whatever else it's doing, but just the sense that it is
 learning a grammar. I'm not sure what the grammar is. And I'm sure we can debate whether
 70% or 27% of right now. I don't know. There's a lot of papers out there on grammaticality
 judgments that do seem to be fairly favorable. Others less so. I don't know how to judge
 this yet. So can I just comment on that? Yeah. So I
 think really it's the case that no network that is trained using backpropagation is going
 to learn what we would identify as kind of a rule. You mean, you mean linguists who believe
 in universal grammar? No, first of all, we, I, everyone, Steve, Steve, Steve, you're going
 to ruin the open mic. Wait, wait, go ahead. Yeah. So now there are a couple of things.
 So first of all, we all believe in universal grammar. We just argue about what it is. So,
 so just going back to something, something from earlier. So we're born with something.
 We have a way of representing things. And you might think that it's weights in a neural
 network. And I might think that it's a programming language where you can write CFGs and someone
 else might think that it's a programming language where you write regular Python programs, but
 we're born with something. We all agree on that. And the argument is just watch. And so
 that's universal grammar. So, so in that sense, I don't, I, I don't think it's helpful to
 talk about whether there is or isn't universal grammar. I think that people who actually
 think that there is no universal grammar aren't in this room or, you know, haven't been for
 decades. We're born with something. We know how to represent certain things that capture
 patterns. There is something there. We can disagree completely on what that thing is,
 but it's, that's universal grammar, the innate representational system that we're born with.
 And then it makes sense to reason about it. And we can, again, disagree on what goes on
 there. But with respect to, to context-free grammars or any of the other rule-based patterns
 that we're comfortable with, it's simply not the case that, that there are papers that
 show that we, that neural networks can learn these things. They can certainly represent
 them, but, but they can't learn them using backpropagation. The training method doesn't
 allow perfect learning of that kind of thing. And that goes to the competence performance
 issue. So they can learn a fantastic approximation for lots of, lots of strings, but you push
 it past those strings and you start getting very strange answers from the network. It
 simply doesn't know the rule. It's not like humans who might know something like how to
 do, how to perform addition or multiplication or A to the N, B to the N or any of those things.
 And yes, they'll have problems when they're not focused on where, or when the string is
 long or when there's, they're sitting in a noisy room, but you improve their conditions
 and they'll know how to fix it because they know the correct pattern. The network does
 not. Again, it's not that it can't represent it and you can with other ways. So I mentioned
 learning using MDL and some kind of architecture search, then you do get networks that learn
 the perfect pattern, but just the networks, the standard networks trained with backpropagation
 and that includes all of the LLMs don't do that.
 So I must be the advocate of a large language model. They are not explicitly trained on
 that and I don't think this is a question of backpropagation. It's a question about
 the task and the objective is to predict the next world or the next sentence. So the fact
 that we kind of hope that it will be emergent property is not fair because even we are not
 really, I'm not sure if people who are only trained on a native language will be able
 to really understand relative close or even a grammatical acceptability test. So I think
 from a research method perspective, if you really want to answer the question, try to
 train a model on a particular linguistic task and then see whether or not it speed up, for
 example, training in general or has any utility for language acquisition. Perhaps now we do
 not need a language with one trillion parameter, only one million will suffice because it's
 not like it's not a strong association what you're looking for. John Ravsky for a long
 time. Go ahead. Hello, I was supposed to introduce myself. I'm John. I'm a linguist and theoretical
 computer scientist. I want to slightly switch topics. It's something that I didn't really
 hear anybody talk about yet. Before we switch topics, can you just reply to Chaim?
 OK, thanks. Because it's not the case that they're not trained on that. I mean, humans
 are not trained on A to the N, B to the N or on any of the other patterns. We look at
 a bunch of strings, not that many, and we get the pattern. You give the same input to
 a network and it will fail. And it's not because it's next word prediction. There's nothing
 wrong with next word prediction. And in fact, the MDL networks that I mentioned work on
 the same principle. Solomon of induction that I mentioned in my talk was exactly stated
 in those terms. It's not about next word prediction. It's about the objective function. And these
 networks are trained with objective functions that work well with backpropagation. And those
 objective functions are bad for generalizing. They don't support good generalization. They
 overfit. No, they don't. I think I can jump in with a theoretical comment on this first
 before I ask my real question. Which is one, you don't even have to do experiments on this
 because now we're at a place where we can actually just prove things about can a network,
 even if you train it or hand design it, can it actually do a certain pattern? In many
 cases they just can't. So I think actually you can go beyond wishy washy claims about
 what your experimental set up is and actually sit down and do some analytical work and really
 show that don't even bother doing the experiment because your architecture can't support it.
 But anyway, so the real question is they can represent these patterns. That's not the problem.
 The representational capacities of the networks are fine. They're Turing completed if you
 allow them. Some of them are not. Many, many are not. The Turing completeness claims rely
 on very unrealistic scenario, like you get unbounded computations.
 There's early work in the '90s due to Hondam-Somblanski and Sandvikman and Sontag. Ronnie's right.
 Representationally, the network, even with a layer of hidden units, one layer can do
 pretty much anything as long as you have an infinite number of hidden units in the middle.
 So it's not very practical. It's a theoretical trend.
 And infinite division and unbounded.
 But that gets at the issue here that I think you brought up, which is a good one, which
 is the difference between designing and representing information and learning it. And learning
 has all kinds of NP-hard aspects to them. And yeah, I agree. And so we have to be careful
 here because there's kind of the gold theorem reversion. We go back to gold theorem here
 and try to figure out whether... Or you look at less valence in terms of approximately
 correct kinds of representations that, in fact, are better than NPN, actually scale
 very well. But for Ronnie's point of view, which I can understand, he's saying the acceptance
 range has to be 100% because the UG requires it. Now as soon as you give up that the UG
 is not sitting in hippocampus, it's not sitting somewhere in your temporal lobe, I'm sorry,
 I do brain imaging and we've looked everywhere. We can't find any UG example whatsoever or
 any evidence of that in the brain. So the question comes back to what could we approximate
 that actually allows us to communicate in a context where grammar is actually understood?
 Can I ask my question? Yes, go ahead, John. I think there's a more serious issue directly
 related to learning, which is that the models cheat all the time. Even when they get the
 right answer, they get it for what Tom McCoy and colleagues called in a very nice paper,
 they get the right answer for the wrong reasons. And they're extremely brittle. You can change
 their right answer by adversary, you know, extremely basic, semantic adversarial stuff
 in the case of text. You play a tiny little game and you blow any of the generalizations
 out of the water. It happens over and over and over again. It's not a transformer issue,
 it's not an LSTM issue. The models simply are not robust and reliable to the kinds of
 generalizations that we want them to do. When we do the stimuli and the experiment and all
 the fun psycholinguistic tricks that we imported. And I think that's a serious issue that we
 can't even believe our results. Do you give more detail about some of that?
 Sure. I'll take an example. This is when people still did what I think Judith was calling
 vertology. This is from Ribeiro et al. in 2018. I could take a standard sentiment analysis
 task. The movie sucked. It's a bad review. And I change something really simple to the
 film sucked. Now it's a positive review. Or I change what was the point of this discussion.
 It's now a positive review. But suddenly I do a contraction to what's the point of this
 discussion. It's now a negative review. It's not even just in language. You can do subpixel
 manipulation and stuff. Get totally different answers on stuff. There's zillions of examples.
 That's just the one I pulled out of my butt. One of Tom McCoy's recent papers was you train
 a model on shift ciphers. I give it a nonsense string where I have to rotate the alphabet
 around some number of times. It's okay on say we have to rotate 14 symbols to the left
 but not 13 symbols to the left. It's very trivial surface shallow level heuristics for
 cheating at the task. And I didn't really hear anybody talk about it. So I'd like to
 hear what the panelists have to say about the cheating issue both from an engineering perspective
 and from a cognitive perspective. And you think that's still happening? It happens all the
 time. Oh, yeah. Just go to a random LP conference. You'll find another 16 papers on, yeah, okay,
 this generalization actually didn't happen because it's some shallow surface level thing,
 actually. I mean, it's very amusing, but, you know, doesn't really fill me with confidence
 about anything.
 Yeah, he's been patient.
 Yeah. Everything John says is true, but it's also largely true of humans. Right? So people
 were shocked, and I was shocked that people were shocked, but that's with lots of hindsight.
 People were shocked at the waste and selection task, right? That just keeping the same logical
 structure for a seemingly simple logical problem, right? Now you turn it from abstract symbols,
 right, to a familiar situation, and suddenly, right, it goes from like a 10% solution rate
 to a 90% solution rate. Right? So back in the 70s, people were surprised because people
 at the time still thought that human reasoning was based on logic, right? We now know that
 this is just not true. We take shortcuts, right? The reason we build computers is that we are
 not good at the things, right, that, you know, at arithmetic and logical inference and abstract
 reasoning, right? These are things that are inherently difficult for biological systems.
 And so Ronnie is right that neural networks don't learn rules very well, but neither do
 humans, neither do biological machines, right? And we do the things we do, all the impressive
 cognitive feats that you, despite this difficulty with following simple rules, right? So it's
 astonishing, right? You know, if you look at the developmental literature on rule learning,
 one thing you notice is that there's basically no work with children under about five. And
 the reason, as we were first hand, is that children younger than five just are terrible
 at learning simple, explicit rules, right? They are of course great at learning language,
 but they do so despite having enormous difficulties with, you know, figuring out that, okay, there
 are, here are these three features. One of them is predictive. That's the rule, right?
 Just learn it. The reason adults succeed, I would argue on these tasks, I might mention
 it in the talk I give next week, is that we use language, we use natural language to kind
 of help us in this process. But yeah, a lot of these limitations with generalization failures,
 limits, fragility also apply to people, not necessarily to the same degree, right? I don't
 want to make a one-time--
 Sorry, I think maybe I misstated it. I just meant in the sense of when we as scientists
 make claims like the model learned a hierarchical generalization, we have no basis for making
 that claim if it's the case that the models routinely cheat. So I don't really actually
 care what--
 Can you describe, sorry, what can you describe what you mean by cheat again?
 They make a very shallow surface level generalization that has nothing to do with the actual level
 of generalization at hand.
 It turns out to be correct.
 You behave according to some threshold, but the generalization that the model makes is
 completely-- has nothing to do with the thing you wanted to learn at all. So, okay, fine,
 the model does that. I don't really care. But then I, as the researcher, can't claim
 diddly squat about what its capabilities are. I sure as heck can't claim it learns language.
 Right? I mean, that's Popper's-- I mean, it's an old Popper argument, right? Like, you can't
 conclude anything from confirming a test. You have to disconfirm a theory or whatever.
 Right. There should be a null hypothesis here, and you should test it. I get it. But are
 you saying, in general, I assume you train neural networks or do something with neural
 networks? Is that right?
 Yeah.
 Or not?
 Yep. Yeah.
 Okay. And whenever you train a neural network, it doesn't generalize.
 Well, okay.
 I'm just thinking of all the benchmarks in the CVV and the vision conferences and the
 speech recognition conferences. And not only are they generalizing, they're generalizing
 better than any technology we've ever seen. So I just don't understand what you're claiming.
 Well, I think maybe we can involve Carl a bit here, because one of his neuroscience
 colleagues actually wrote a book chapter about this when he was critiquing some brain imaging
 studies. This is a guy, Yuri Bushaki from NYU. And he said, "It's often really tricky
 to confuse-- we often confuse the observer, like what we're observing, the thing, with
 actually the thing that we're using to observe it." So, like, our generalizations are actually
 like being conflated with each other.
 I know Yuri very well, yeah. And that's not what he meant. And his complaints about brain
 imaging were tongue-in-cheek mostly. So I wouldn't use him as a good example, but go
 ahead. What other kinds of examples are you talking about?
 It just seems that--
 They're talking about Bushaki.
 Yeah, another hippocampus guy.
 Well, he was a bit more than a hippocampus guy. But yes, he would be a good example,
 but he was attacking brain imaging out of a kind of a predicate about something else
 in terms of neurons. So remember, these folks, neuroscientists, are looking at 100, 1,000
 neurons of the time. So if they're going to find any function, it's like looking through
 a chatbot. You're going to be doing this for 40 centuries trying to figure out how these
 circuits come together.
 Sure. But as a scientist, I would like, when someone makes a claim like, "The model learns
 wh movement," or whatever, whatever phenomenon, "Therefore, linguists are wrong about whatever,"
 I would actually like to have some confidence that the model actually learned not only the
 behavior, the extensional behavior, but the intentional behavior. And I think time and
 time again, we see that, in fact, we have no confidence that they do that, or extremely
 little confidence. It's a huge issue.
 I think it's early times to reject all of the neural network models that are out there.
 But I'll grant you, there should be some healthy skepticism.
 If I may jump in and kind of go back to what Gary said about humans cheating as well, or
 taking shortcuts. I think, so obviously, this is true for many things, but I think the devil
 is in the details. So as you said, well, not to the same extent. And I think crucially,
 this same extent or not same extent is really the issue, I think. And plus, I don't want
 to bring back the performance competence thing, but it still is the case that you might...
 In your everyday life, you're in a hurry, you're tired, whatever. So you take shortcuts. But
 then if you're hoped and you really need to work it out in language or in logic or whatever,
 you mostly can't.
 And so I think there is a question of how do you go about solving a problem if you want
 to do it just easily, quickly, and maybe the models or humans cheat, so to say. But the
 question beyond that is also whether in principle you're able to do it otherwise or not. And
 so for certain things, at least humans are able to do it otherwise. And so the question
 is whether the models can also do it otherwise, or they will always resort to the shortcuts.
 So I think the fact that in practice, sometimes you also do these things is obviously not
 surprising, but it also doesn't in principle contradict the fact that there isn't a difference
 between the models and humans, because I guess the question is much more in principle whether
 you can or not. And for ourselves, we know that we can in some situations, or if as Ronnie
 said, I think if you improve the conditions or whatever it was that I think he said this
 very nicely. And so we would like to know whether the models can also maybe under certain
 conditions or if you really tweak them or if whatever they need to get in order to.
 So I think that's the issue, not so much whether sometimes you also cheat. Yeah, sure, that's
 fine.
 Can I respond? Yeah, I think it depends on what one is trying to explain. So you're totally
 right that there are those kinds of situations where if you raise the stakes, if you give
 people more time, they might do it differently. But I think it's informative to know what
 is kind of naturally easy for people, and what requires deliberation, and I would add
 special training. And so it seems that to learn language as a human normally developing child
 versus to become a theoretical linguist and explaining the kinds of things that theoretical
 linguists have learned, those are very different things. And so if the question is can an LLM
 learn the things that a theoretical linguist knows, the kinds of generalizations, the kinds
 of deep rules, right? I don't know. That's an interesting question. But then we're after
 a theory of linguists, not a theory of language. And so if the question is what kinds of things
 can be learned from linguistic data, and then of course the question, how much data is needed?
 What's the role of grounding, of agency? Those become central. So can grammar, can different
 aspects of semantics be learned from language itself? Then whatever shortcuts the models
 take and take advantage of, it's informative to compare that to people. And I take John's
 point that we need to be critical, at least on the cognitive science side. I think most
 of us are quite critical with this regard. But yeah, I mean, it's telling to me that
 there are things that we depend on, that to learn we depend on formal instruction with
 specialized, often notation, with access to tools that we've built to augment our abilities,
 right? Mathematical systems, calculators, computers, right? And LLMs, right, actually
 don't have access to those tools, right? But they do, right? I mean, it's all on the internet.
 So yes, they do. Well, as pure LLMs, they don't, right? So one problem with confabulation
 or hallucination, right? One can make the same argument that humans have all the same
 tendencies to confabulate, except we can fact check ourselves in a way that a pure LLM cannot.
 So if one is asked to quote from a book or to paraphrase some argument without the ability
 to fact check, right, after reading the book, right, we're going to make a lot of the same
 kinds of mistakes. And actually, if you go to kind of before the internet, you look at
 various like pop science books, right, where people just harder to fact check the author
 saying different references, you find this sort of human confabulation all over the place.
 But yeah, we can, given the right stakes and given the right tools, right, correct ourselves.
 And yeah, I think it's an open question. But I don't see that as being any sort of limitation
 of backpropagation, right? The limitation of backpropagation is in acquiring
 certain patterns and not overfitting the data. Because in order to work with backpropagation,
 you need to work with something that can't really take into account the complexity of
 the, of the hypothesis of the configuration of the weights, if that's what you're working
 with in a network or in other frameworks, it can be other things, sizes of grammars,
 times, and so on. Backpropagation doesn't allow you to do that. So you need all kinds
 of proxies. So you can use all kinds of things like L1 or L2 regularization, but those are
 not good proxies. They allow you to smuggle in a lot of information in very small weights.
 And that allows you to then, to then overfit the data. So that's a limitation of backpropagation,
 at least at present. Yeah.
 Ronny, did you see Belkin's talk? No, I missed it, unfortunately.
 I have a look at it on the tape, because he's arguing on behalf of overfitting.
 Okay. Right. And one comment on overfitting. Generally,
 these networks don't overfit. They can't overfit. And this has been known since the 1990s. So
 there was early work done by John Moody, where he was able to put partial derivatives on
 each of the hidden units. And he found, as data came in, the hidden units would disappear
 until the complexity increased. So it actually auto-regularized itself. It would come in
 and out and would learn this. Now, in these very deep layers, we're seeing the same things.
 This is why you can throw trillions and trillions of data. And it's not clear that it's learning
 all of the data bytes, but it is using some of them to construct some scaffold or some
 kind of structure internally that it can use to acquire new information. Backpropagation
 isn't at fault here. I'm happy to use other methods. I'm using minimum description length
 or Akaiki or some other thing that trades bias and variance is fine. And that's really all
 the issue here I think you're getting at. If you want 100% learning, that's going to
 often be a trick, because you can control complex systems by just restricting the dimensionality
 to live in. So if I just want to model the finger in front of my eye right now with the
 joints that I have, well, it's a linear system. But if I want to do this and say, hey, I can
 tell where my finger is behind my head, that's more complex. And learning that is going to
 be approximate. And it involves a high degree set of freedoms in order to approximate it.
 And we do it very well. I mean, I can tell pretty much I can never see it where my finger
 is behind my head. So complex systems, when they're reduced down to very low configuration
 states, yes, you can learn them with things like minimum description length or other kinds
 of trade-off systems. But the story that came up from the '80s was about approximating complex
 functions. And backpropagation is one way of doing it. There's other things one can
 do as well.
 But backpropagation really is something that, at least until now, has only been able to
 work with things that aim at overfitting. They try to maximize--
 I see. Hang on. Continue, Rani.
 So you use a term to try to control for overfitting. And you use the dynamic of the process to avoid
 it, which can include all kinds of things like early stopping or any of a million other things.
 But you don't have any way to actually take into account the complexity of the information
 that you have inside the weights. And that's going to allow you to ultimately--
 You do. One is called dropout. And what this does is, in fact, remove hidden units as a
 function of trials. And what it's trying to do is actually assess the amount of degrees
 of freedom and the parameters state space that it needs to handle the data it's looking at.
 And Hinton and others came up with this idea initially. There's other similar noise injection
 methods that reduce whatever you want to call as overfitting. But you also have to remember,
 there's some kind of competitive distributed processing going on that's reducing the overfitting.
 And this has been shown, again, for decades and decades. I'm perfectly willing to accept
 your idea that to learn something 100% perfectly, if you have a small configuration states and
 you have a small amount of data, yeah, you could probably learn it. But if you have a
 hypothesis space that has something like a trillion hypothesis in it, no, you're going
 to have to do something else. And it sounds like your examples are actually fairly compact.
 The examples are just small in order to allow us to look at what happens there and see whether
 it works or not. And you can see that when you work with these things that, and it includes
 early stopping, dropout, L1, L2, you name it, all these tricks, they're all tricks. They're
 all ways to try to avoid saying generalized properly, like a scientist would. A human
 scientist, an alien scientist, a child, these are all ways to avoid that while still using
 backpropagation. And they allow you to approximate things close to the training data. But then
 we see that we push these models a little bit past the training data and the performance
 drops and it drops brutally. They stop being able to perform there. And you can actually
 look at where the objective function is sending you in the hypothesis space. In some cases,
 like I had a slide on that, when we looked at LSTMs and where the objective function
 sends you, if you don't factor in the complexity of the hypothesis, again, if you don't try
 to make the network work like a scientist of some kind, then it will send you to the
 wrong place in the hypothesis space and a place that prevents you from knowing A to
 the N, B to the N or the A's and B's and C's and D's or addition or any of these things.
 You can see it performs poorly on all the empirical tests. You can see where it takes
 you in the hypothesis space and theoretically, you know why it's doing that.
 So I'm concerned that there seems to be a difference of opinion about what ought to
 be a factual matter. And I don't understand how the facts can be, how we can argue about
 the theory if we don't even agree about what the facts are.
 I agree with you. And I think that, you know, I just disagree with what Ronnie is saying.
 I think we should take this offline, Ronnie, at some point. And you and I just talk about
 this because I think one or both of us have some deep misconceptions about what backpropagation
 can or cannot do. And, you know, I've seen these arguments for decades and they almost
 never turned out to be actually relevant to what the neural networks can learn. And now
 we see in these LLMs something shocking has happened. And yes, maybe it doesn't learn
 simple parentheses balancing or counting, but it can certainly tell when we're trying
 to trick it into counting or give it a sentence that Chomsky had come up with years ago. It
 will say, yes, that's the sentence that Chomsky once used to confuse people about linguistics.
 So I get it, but I don't think we're going to resolve this because I just don't agree
 with anything you're claiming.
 But I actually sent you those papers. And I sent you back and I sent you papers to, you
 know, Schmidt and Schmid. So we looked at the papers from the 90s and the 2000s and
 from the past decade. And it's very short and it's really a very simple point. These
 things have been known since the 1960s. I mean, we know what makes things overfit and
 we know how to prevent it. And we know that all these models work with things that don't
 provide overfitting. And we can actually show also, and we have shown also in purity what
 happens to these networks and that the papers are out there.
 Okay, that we heard. Steve, you talk when I let you talk, you don't want to talk?
 I'm willing to talk to Ronnie offline. This isn't going anywhere because we have different
 questions. There were some questions addressed to Karl. Did you see them in the chat? If
 you look at the bottom, there's 16 points in the question and answer. The last one and
 the next to last one were for you. I can read them if you want. Actually, there's a
 there's an interesting character called anonymous attendee. And you can and I don't know whether
 it's the same anonymous attendee or this is a pseudonym. But anyway, this one said the
 first one was early in response to Karl Friston's claim that large language models do not qualify
 agents in the context of active inference paradigms as the thoughtful question to ask
 might be. And this is something that was that was given to Chad GPT. And now we're going
 to hear Chad GPT. In light of your assertion that large language models do not meet the
 criteria for agency required by active inference programs, could you elucidate which specific
 attributes of agency such as self representation, goal directed behavior, or adaptive learning
 are presently deficient in these models? Furthermore, what advancements or modifications would be
 necessary to bridge these gaps and approach a more genuine form of agency? It's yours,
 Karl. From Chad GPT, not from Atana. I was going to say I've lost my virginity now engaging
 with large language models. I think the answer is fairly straightforward. You would need
 to have a an implicit structure generative model that covered the consequences of action.
 And as soon as you have that, then I think you have a relatively straightforward. Consequences
 of action. What does that mean? So you have to have said in the kind of generative models
 we use for active inference, you have to have a model that will predict or generate predictions
 about the outcomes conditioned upon a particular action or policy or plan. And then you use
 the posterior predictive density over those outcomes to evaluate how likely it is that
 I will engage in this particular action. Okay, to anonymous that's indeed to give that to
 Chad GPT. Some of the the next one he said is not from Chad GPT. It's from him or her.
 Some of the conversation during the last session hinted at autoencoders or autoregressors doing
 something like the perception prediction action loop described by Dr. Friston. This may not
 be a question for you, Dr. Friston, but for everybody else. However, the prediction error
 in this case is nothing like the reward punishment and reinforcement learning, which I think
 one slide in that talk made out to be a nearly equivalent quantity. How could one imagine
 autoencoders integrating with reinforcement learning while actively inferencing in an
 actual or simulated environment? Is an agentic model like that likely to be much better at
 learning linguistic models than the failed LLM ones?
 That's a very loaded question, isn't it? Failed LLM models. I can use that question just to
 make a couple of points in relation to what I've been listening to, which is a fascinating
 conversation. Virginia, can I just answer your question? I think the facts are known.
 It's just that you have to listen to either an engineer, which would be Stephen, or a
 physicist, which would be Ronnie. I think the same same the same things, but just using
 different words. I don't think they think that. I think they think they're saying very different
 things. Yeah, they possibly do, but they've been brought up differently. But from a mathematical
 point of view, that they're saying the same thing, which interestingly speaks to some
 of the... Well, let's take the question about variational autoencoders. What's the objective
 function for a variational autoencoder? It's a variational free energy, also known as an
 evidence lower bound. Why am I saying that? That tells you that the only objective function
 that matters is the evidence. What is the evidence? It's the accuracy minus the complexity.
 So when Stephen talks about minimizing complexity using dropout, for example, or in the old
 days, it would have been something like automatic relevance determination, he's really just
 speaking to the important issue. I think Ronnie is trying to emphasize that it's not just the
 accuracy, it's not just the predictive accuracy. You have to regularize it. And regularization
 is the engineer's way of talking about priors. And Ronnie would probably... Well, I'm not
 quite sure whether Ronnie would, but certainly Judith would. You were talking a lot about
 shortcuts earlier on. So shortcuts are what Gerd Gigerentzer, a fan of Satisfy Singh
 and Herbert Simon would call a heuristic. And a heuristic is just a prior. It's just
 a parsimonious shortcut that provides a very simple way of providing an accurate account
 of the content you're trying to predict and/or the way that you're trying to generate that
 content or indeed sample that content. So I do think that both Stephen and Ronnie have
 been talking about the same thing. Would simply using a variational autoencoder be sufficient?
 Well, in principle, because its explicit objective function has both this accuracy and complexity
 in play in virtue of using an evidence lower bound, it would certainly outperform back
 propagation on any other objective function. Is it sufficient to be an agent? No. The variational
 autoencoder does not have a generative model, an encoder and decoder of the consequences
 of its action because it doesn't act. So you'd have to build a variational autoencoder that
 predicted the future conditioned upon the kinds of things that it could do. And I think
 the same thing goes for large language models as well. So in summary, I was just trying
 to identify sort of crosscutting themes in all the sometimes argumentative and sometimes
 not things that people have been saying. It seems to me that the key issue that people
 are wrestling with is the issue of structure learning. How do you write down the right
 objective functions in machine learning to do structure learning? Getting the structure
 right, getting that, you know, I think John was describing this. I can't remember exactly
 how I articulated it, but the only thing that you need to get the structure right is if
 you were a statistician would be the equipment necessary for Bayesian model selection where
 each model is associated with a particular structure. To select the right model, you
 need to find the model with the highest evidence. To evaluate the evidence, you need to estimate
 the accuracy and the complexity. To do that, you need to estimate the uncertainty. And
 that's where, yes, where John's comments came in because he was talking about I need to
 have more confidence. I would actually go further. In order to estimate the evidence,
 to do the Bayesian model selection, to do the right kind of structure learning, you
 need to estimate the confidence in the predictions and, indeed, the latent representations. And
 I would argue that that is missing explicitly in the enlarged language models, but Stephen
 will probably argue, well, yes, it might be missing explicitly, but it's implicit in the
 right kind of dropout and the right kind of regularization. But if you are a purist, and
 I think Lonnie would kind of sell himself as a mathematical purist, strictly speaking,
 you should be using measures not just of the likelihood that this token is in this part
 of some embedding space, but also the confidence intervals around where you are so that you
 can integrate uncertainty to measure the model evidence. And I'll close by saying that is
 a unique feature of a variational autoencoder. It doesn't just encode the most likely thing.
 It also encodes explicitly the uncertainty and the confidence in its estimators, which
 is why the variational autoencoder should, in principle, outperform anything. Well, the
 objective function should outperform anything, any alternative. That's my take on this.
 One thing I think you might appreciate, last week there was a paper that was published
 called What's the Magic Word? A Control Theoretic Framework for LLMs. And in this paper, there
 was a sort of a breakthrough, a couple interesting things, and they would set up different kinds
 of prompts that would have missing items. Some of these prompts would be obvious like,
 you know, Federer is the greatest, blah, blah, blah, and the target would be whatever the
 target was originally and obviously training. What they found, though, in the control theoretic,
 when they prove some theorems here, they showed that what's called reachability, the fact
 that you could start at a place and get to any other place in the state space, was actually
 highly constrained by the LLMs. In other words, it's sort of like saying there's this huge
 highway system, but it only lets you go on certain ones at certain times. So it has some
 kind of control logic that creates all of this, you know, let's say rivers and tributaries,
 but then it has a control based on a prompt that then moves you down a path. And once
 you miss that path, that tributary, you're going to make an error. You're going to maybe
 make an illusion. But I think the underlying mathematics of this, which I haven't completely
 digested, is remarkable. And that these can learn a control theoretic structure internally
 is very similar to your predictive encoding ideas. But here we have this odd, you know,
 paradigm where we're saying, predict the next word. What in the world? You know, obviously,
 I hope that's not the way linguists imagine developmental psychologists imagine the children
 are learning language. But so this is obviously an artifact. I think someone said that earlier.
 It's an artifact, but an artifact that's a little bit too close to human. Oh, yeah, what's
 the magic word? I'll post it somewhere in here. So if you're curious, you can find it.
 Chris Chung, you can now that you can turn on your face, are you do you could you give
 us the current status of satisficing theory at CMU in 2024?
 Well, we still go by the Church of Herb Simon. I mean, I think that's the general view of
 cognitive architectures that they have basic mechanism that can be described, for example,
 as Bayesian, but that there is no macro guarantee on the optimality of how those basic mechanism
 work together and that bounded rationality is a much better description of how those
 basic mechanism work together, organized together, represent large knowledge to reasoning or
 inference to the extent that people do and come up often with a reasonable answer that
 has no guarantee of optimality. It has different guarantees, right? Usually people get better
 over time, they get more experience, they'll improve, they go along that power law curve
 of performance and so on and so forth. But yeah, so that I think the Simon view of bounded
 rationality is still very, very much current. So I will sort of take a take on the sort
 of that take on the sort of my view of large language models. Obviously, I'll elaborate
 next week is Herb might have asked, well, what task are they doing? And the task that they're
 performing is to store a very incredibly large amount of knowledge and sort of reproduce
 it in a way that I agree with John, that in some ways, it's shallow and surface level.
 On the other hand, I agree with Gary that, well, that's often what people do, right?
 If you imagine sort of like my thought experiment was sort of take some average undergraduate
 student and give him Carl's papers on free energy, and then give them a test where they
 asked to, you know, various questions on free energy. Probably what you will get is a fairly
 shallow regurgitation. And, you know, depending on how good the student is, may sound very
 good. But at that, that unless they spend a substantial amount of work on it, that they
 would be able to really have a deep understanding of those principles. And to some extent that
 is more or less the same task that the large language models have to do. So to some extent,
 I'm not surprised that that's also to some extent what they're doing. I would be very
 surprised if there was enough structure in the language to really capture the deep principles
 that underlie free energy or any of those considerations. I don't think it's there
 in language. I mean, as was pointed before, there is an incredible amount of information
 stored in language, but not at that level of depth, I think.
 Can I respond today with another thought experiment that I thought was not mine? I heard it from
 Rosa Kao, who's a neuroscientist and philosopher at Stanford. And she asked, I know I might
 butcher it, but you know, imagine it's something that happens to us as educators. Let's imagine
 you have two students, one who studies very hard for an exam and, you know, for weeks
 or whatever and passes the exam with 100% accuracy. And you have another student who studies not
 at all, but then cheats on the exam and also gets 100% accuracy on the exam. So you as
 the educator are now in a position where you have to, on the one hand, decide which of
 the two students you have to punish and on which grounds you're supposed to make such
 a decision on. And, you know, it's a thought experiment for her because either answer is
 going to be, you know, contested by some of the students. And so I think when you ask
 this, what does it mean to learn one of Karl Friston's papers just from reading them, you
 know, for us, one has to ask, you know, how else might I do that if I skimmed it, you
 know, and read the, just saw the first diagram or something.
 But I think actually related to this, the claims are not what Christiane you're saying.
 So I fully agree. Of course, most of humanity might not actually have the grasp because
 they did not spend, I don't know how much of their time thinking about this, reading
 about this, learning about this to understand very sophisticated scientific theories. But
 this is not the claims about large language models is not that they're good or not good
 at understanding scientific theories. We're talking about basic language, right? So that's
 it's a different level. So if you're saying, well, what large language models do is just
 skim through a lot of internet and text that they're given and regurgitate it to some extent.
 I think many of us would be happy with that, but these are not the strong claims. And so
 I think, you know, that's where the debate is, right? So how much do you attribute to
 that regurgitation at some shallow level of a lot of text? Yeah, maybe, but that's not
 the strong claim.
 Yeah, I mean, we can we can debate about the strong claim. So I think that I keep an open
 mind and obviously we have limited understanding exactly what they do. I would say that when
 you that's where the issue of grounding comes in. It's not just that the large language
 model cannot deeply understand car theories, because few people can. I would say that that's
 the level that they achieve for a lot of things in which people have a better grounding than
 a deeper understanding that they do. And I think that level of shallowness is pervasive.
 And we can sometimes fool ourselves that they have a deeper understanding, because again,
 they've captured a very large amount of knowledge. But I would argue that that the the whereas
 not understanding car theories is the norm rather than the exception for for for humans.
 I think conversely, the sort of the kind of things for which the deeper understanding
 is the norm for human is is not what they achieve unless they can display it otherwise.
 So I will sort of again going back to cognitive architecture a little bit. And I think that
 one distinction with large language models that needs to be trained in batch on the very
 large amount of knowledge and are typically not as good in the interactions at capturing.
 So the innumerable examples out there of where human attempting to engage them in a conversation
 and build a common ground and expand understanding and usually the model not doing a good job
 at that, something that humans typically are fairly good at. And so on the issue of the
 different tests that John mentioned, going to college in Belgium, one of the things I
 was very fond of in university is that we had oral exams. And I think that that's a
 very different type of test of knowledge from written exams were basically written exams
 by and large, one size fits all. And you can usually get away with displaying a fairly
 surface amount of knowledge unless it really gets to solving problems, those kind of things.
 Whereas oral exam, a good professor can really pin you down and figure out, OK, well, you
 say that. What do you really mean by that? And gradually deepen that kind of things.
 And I think when we deploy tests of large language models to gauge the understanding, I think
 that kind of iterative interactions that really builds on something rather than the typical
 written test, I'm going to give you N questions and give me an answer. I think that would
 be a better gauge of the real level of their understanding than a lot of the tests that
 are there now, which is one of the reasons why a panel like this can be useful. And with
 that, I want to say this is the stage has been set for Christian's talk next week. And
 we've had a good interaction over here. I'm not disappointed. I don't know if you are.
 And thank you very much, all of you for this interaction. Thank you.
