 - Very good.
 Let me just introduce you very briefly.
 Friedemann Pullerunner is professor of neuroscience
 of language and pragmatics
 and head of brain language laboratory
 of the Freie Universität Berlin.
 His main interest is in the neurobiological mechanisms
 enabling humans to use and understand language.
 The rest, I think, Alia,
 his focus is on active action related mechanisms
 crucial for symbol understanding.
 So he's the first one who enters the topic
 of symbol grounding, which will also come up.
 Friedemann, welcome back.
 It's yours.
 - Yeah, thank you so much, Steven, for inviting me again.
 And that's a great honor to speak to this large audience.
 Oh, welcome.
 Thank you very much for having me.
 I would speak on semantic grounding of concepts
 and meaning in brain-constrained neural networks.
 Can you hear me well?
 - We hear, we hear.
 - Yeah, very good.
 Just be back in case the connections gets worse.
 I had some bad experiences this morning.
 I hope it will be good.
 Okay, there are key questions of general interests
 to concepts about concepts in language.
 Well, you may disagree.
 You may not be interested in one or the other question,
 but at least these are the questions that interest me now
 for many years and I wanted always to have questions,
 answers to them and looked for ways to address those.
 Why are, for example, we able to use huge vocabularies
 of symbols while our closest relatives,
 our next neighbors in the evolutionary tree
 use only a few signs, maximally a hundred
 in their communication systems.
 Why is this so?
 Why are the mechanisms, what are exactly the mechanisms
 underlying semantic grounding?
 Well, and if we look at the brain,
 there are so many areas that are in some way
 or another important for semantics
 and some of them are up areas of general interest,
 others are only relevant for some semantic categories.
 Why is this so?
 I want an explanation of this.
 How can we build concepts and symbols related
 to objects and actions and even whole categories
 of objects and actions?
 Why sometimes, what's the difference between a category term
 and just a label on an object?
 And then if we go further with regard to symbol complexity,
 what's the difference between concrete and abstract concepts
 and how do we attach symbols to those concepts
 and would the reverse effect that symbols
 would actually enable concept formation
 and is it possible that there is a mechanism behind that
 and what is this mechanism?
 Now, I'll stop here.
 And I hope one or the other of these questions
 are of interest to you too.
 When we started our neural networks projects,
 these were largely unanswered
 or at least we were not clear about the best answer to give.
 And now I think as I hope to show now in the talk,
 we have at least some candidate answers
 that make us think about possible mechanisms.
 Okay, and the idea behind it,
 the neural substrate may provide the explanation
 or perhaps even make an important contribution
 to the explanation.
 So, and this is based on the assumption
 that cognitive linguistic communicative abilities
 are based on concrete mechanisms
 and on the biological substrate.
 And it's obviously so that the mechanistic properties
 of the substrate play a role here
 in building the cognitive abilities.
 We see that if there's a lesion of specific brain areas,
 then some of these abilities just get lost.
 And we also see that animals equipped
 with slightly different brains,
 then suddenly have very different communication systems.
 So the substrate, the network of nerve cells
 seems to be, the underlying network seems to be critical.
 - Benjamin, excuse me for the,
 are you the impression that you're showing PowerPoints?
 - Yes, indeed.
 - There, you haven't shared.
 - Oh dear, this is of course,
 I'm sure I pressed the buttons,
 but of course it can happen that it doesn't work.
 I'll try again.
 Is it there?
 - No, it works.
 - Okay, so this is just to underscore my,
 this was just to underscore what I've been saying so far,
 not much lost if you didn't read the corresponding text.
 Let me just go on.
 And yeah, thanks for making me aware.
 I had pressed the buttons,
 but somehow it hadn't worked.
 And okay, and now you may say, well,
 we of course, we can use neural networks and simulate this.
 And, but here, the question is whether neural networks,
 as we have them today, are sufficient for that.
 And so, and as you know,
 there's a huge neuro-competitional research stream
 that developed in the last decade or so,
 where neural networks have been developed
 that correspond to different levels of brain activity
 and structure from the whole brain level
 to the single cell and synaptic level
 and the circuit level in between.
 Then all these neural networks have in common
 that they consist of elements that can be likened to neurons.
 Neurons, nerve cells in the brain.
 Here is a specimen, a pyramidal cell,
 with its tendrites and its accents at the bottom.
 I mean, the simplest version of a neural model
 of a neural neuron, of an artificial neuron,
 this comes as a unit that sums its inputs,
 compares it with a threshold,
 and sends out an output. Now, these models
 can become more complicated,
 varying in the threshold function, for example,
 for the transfer function from the sum input to the output.
 And then also the complexity of the neuron
 and even the modeling of the individual sub-synapsy
 can become extremely complex
 and also closer to biology.
 If we look at those models that have been used
 and are used to model cognitive linguistic processes,
 then they come in different types, in different versions.
 Here, for example, the original connectionist models
 that were still symbolic and localist are shown here.
 Some of them are shown here.
 You typically have a word representation,
 a lexical representation in these models,
 connected, for example, with phonological
 and semantic features.
 And the similarity to the brain is not very great,
 as you may agree.
 Now, this is a bit better
 for distributed neural networks,
 where an input and an output layer
 and a hidden layer, or several hidden layers,
 are used to simulate learning.
 For example, the classification of an input,
 string of words, or as shown here, a picture.
 And on the other hand of the network here,
 a deep neural network.
 And there are, for example, words
 that are used to classify these pictures.
 And surprisingly, these networks produce
 extremely accurate and almost human-like performance
 in classifying pictures and into categories
 such as face, animal, tool, or whatever, or bird.
 But again, if we look at the architecture
 of these networks, they are not very close
 to brain structures.
 Well, the original perceptron, with two layers here,
 with only two layers here, was actually fashioned
 according to the retina, a part of the brain.
 However, the area structure of the brain
 is not really well imitated by these deep neural networks.
 This is also so for the more powerful models
 developed in recent years.
 Auto-encoders here on the left, or transformers here
 on the right, they usually comprise a large number
 of areas or layers that feed into each other.
 They are, in many cases, just feed forward
 from one end to the other.
 And in this case of the auto-encoder on the left,
 you see, for example, here a central compressed,
 very thin, very small layer.
 Very small layer with only a few neural units,
 which contrasts with the brain, where the associative,
 the association cortexes are usually much larger
 than the primary, the sensory and the motor.
 So also there's contrast to reality.
 But more recent developments,
 and it also addressed this general similarity
 to the brain, whole brain modeling, for example,
 takes it very seriously to implement the cortical areas
 and some subcortical nuclei as well,
 and also the connectivity structure linking those together.
 However, in this case, the disadvantages
 that each area is only modeled by one neuron
 in the typical case by a very small set of neurons
 and detail could be added there.
 In essence, whatever you do and wherever you look
 in the neural network literature,
 if it comes to cognition, what you have are networks
 that are similar to the brain at one level.
 For example, here in the connectivity,
 which is good for the whole brain models,
 the long-risk distance connectivity,
 but it's lacking in most deep neural networks.
 And also, for example, regarding the mechanisms
 that go on within the areas here,
 the whole brain models are missing detail.
 So essentially, what we claim is now that we need to make
 networks more closer to the brain
 in order to guarantee compatibility.
 How else should you be able to model the difference
 between a monkey brain and a human brain,
 or the difference between different cortical areas
 in the brain if you ignore, for example,
 for the area connectivity and area structure.
 Which levels, what features would one need
 to take into account?
 Of course, the neuron level, the single neuron level
 is important, but all neural networks do this already,
 and we agree, but then there's learning,
 the modification of the synaptic efficiency.
 And here, we sometimes use back-provocation rules,
 which whose realistic status with regard to,
 in view of biology, is under discussion,
 and other rules that are really taken from biology,
 such as Hebbian learning.
 What fires together, wires together,
 and what is out of sync delinks.
 So we could indeed replace some of the learning rules
 in the network.
 We could also add local specificity.
 Normally, the deep neural networks have layers
 in which neurons sit side by side within each layer
 without any connection.
 So within the layers, there are no connections
 between the neurons in the typical case,
 but in the cortex, there's lots of connectivity
 within each area and within each local cluster of neurons.
 There are inhibitors, and then also the excitatory neurons
 connect with each other through their basal dendrites
 and axon collaterals.
 And then, of course, at the higher,
 at the macroscopic level, so to speak,
 we have the structure of areas
 in which the cortex is subdivided,
 and then the connections between these areas.
 All of these aspects seem to be relevant.
 So a brain constraint modeling strategy might be
 to mimic indeed our area structure,
 brain connectivity at different levels,
 at the local, at the macro and microscopic level,
 at the within an area and between areas.
 We want to have excitatory and inhibitory neurons
 locally connected, dynamic and globally connected,
 and also learning should be kind of realistic.
 Regulation can use, this is what results
 when you put together excitatory and inhibitory neurons
 should also be implemented.
 And then, of course, we want to use the whole thing
 to model cognition.
 Well, but we can do this in different ways.
 And there has been very strong claims
 about how cognition is actually taking place
 in neural networks and thus the brain.
 And two alternatives have been the proposal
 that indeed they are localized discrete representation
 of words, syntactic rules, phonemes,
 and concepts and concepts.
 And the competing view from the neural network literature
 is no, there are no such discrete localist representation,
 but the representation in neural networks
 and presumably all neural networks, including the brain,
 is that within an area and even across the areas,
 each cognitive unit, like a word, a rule,
 is represented by an activation vector
 involving all the neurons in a fully distributed way,
 which of course leads into problems
 in activating two words at the time and so on and so forth.
 But these are the two theoretical postulates
 which are kind of incompatible,
 or at least seen as incompatible,
 because we wanted to know how,
 in a more realistic perspective, things might look.
 We taught a simple brain-constrained network
 to learn words, and we did it in a very basic way.
 We taught the network just to do word production
 by activating some neurons here
 in the motor, articulatory motor cortex, and also,
 at the same time, some neurons in the auditory cortex
 in one of the model, and the neurons were supposed
 to correspond to and to represent the articulatory motor
 and also acoustic phonological features
 of the spoken word forms.
 Activation was on both sides of the network.
 Simultaneously, activation was allowed
 to spread forward and backward.
 The connections were all in both directions,
 bidirectional, and Hebbian learning was applied.
 What happens?
 Well, after a while, the backward and forward
 spreading of activation led to the formation
 of cell assemblies here, a very crude approximation.
 Here, a more realistic display of the connected neurons
 and their links, and these had interesting
 functional properties.
 Here, you see a display of the activation spreading
 that could be observed after stimulation
 of the auditory neurons that belonged
 to one word representation, so the network
 was simulating spoken word understanding.
 You see activation first started here
 in the auditory system.
 Each white dot represents one active neuron.
 Here, it's pretty silent on the motor side.
 Activation was spreading slowly towards the middle
 of the network.
 Time is running from top to bottom.
 And after a while here, interestingly,
 a lot of activation was suddenly created.
 We, and so after the stimulation phase,
 there was a full activation phase,
 and there was finally a phase where activation
 just faded out, and in the fade out process,
 these central, these areas central to the architecture,
 the prefrontal cortex and the anterior temple,
 and the anterior parabel here in the periphery
 of the superior temporal cortex.
 And held activation for the longest time.
 We, and we can now liken, we can describe
 these functional stages of activation
 and relate them to cognitive processes.
 Word recognition and verbal working memory,
 maintenance of activity in the word circuit.
 And what does this tell us about the nature
 of lexical representation?
 Is it localized or distributed?
 Well, the brief answer is it's both.
 It's not correct what has been claimed
 in the connectionist, in the connectionist literature
 that there's just a distributed, distributed representation
 of cognitive units of words, for example,
 or of semantics, and without localized representations,
 we see that the cell assemblies form very similar
 to what Donald had once claimed here in Montreal,
 or actually before he was, and we can even relate
 this, the activations in this brain-constrained network,
 which we observe to different phases previously claimed to,
 namely that there's this ignition or recognition phase,
 and then later on the memory or reverberation phase.
 And all of this can be explained by Hebbian learning,
 indeed, and also by the strengthening of the connections
 between the heart-taking neurons.
 Right, good, but now let's look at the key questions
 previously introduced.
 Now you have, I hope you see them on the slide as well.
 And here I have five of them,
 or of course we could increase the list to 20 more,
 but time is limited, and let's just go look at them
 one by one.
 Why are we able to learn huge procalories
 while apes and monkeys only use a few signs?
 Well, one possibility is that this relates to the wiring
 of the cortex of humans, of chimpanzees, of macaques here.
 And one difference that has been discussed
 as very prominent and potentially very important
 is this link from temporal and to frontal cortex and back
 through a fiber bundle called the arcuate fascicle.
 This connects the premotor and prefrontal areas
 to temporal, to various temporal regions.
 And this is much stronger developed in humans
 than, for example, in chimpanzees or in macaques,
 monkeys or monkeys and chimpanzees generally.
 And if you can compare the connectivity,
 and for example, here in this monkey display,
 you see that from prefrontal cortex,
 there's a link to posterior temporal cortex
 right far from the auditory and the motor system.
 While in the human connectivity structure,
 there is a link between several areas,
 prefrontal and premotor, to various temporal regions.
 We can translate this into a model,
 even though we can discuss about the details,
 in the monkey connectivity seems to primarily include
 next neighbor connections between the areas
 and also one connection between anterior temporal
 and or between prefrontal
 and peripheral superior temporal regions.
 While in the human case, there seem to be more
 also jumping links, leaving one area,
 well, jumping over one or two of the areas in between.
 And what functional consequences
 would this connectivity structure difference,
 this evolutionary step from left to right
 have in our brain constraint model?
 Well, you can see here the activation of the brain
 of the cell assembly formed in the monkey model
 when the monkey learned an ape call or a monkey cry.
 And you see the activations of the different areas,
 you're represented in different colors,
 corresponding to the color of the boxes.
 And the areas here, auditory cortex first,
 and then step by step, it goes to frontal
 and to motor regions and activation comes up
 and goes down rather quickly.
 Now the dynamics in the human model is very different.
 You see activation here in the auditory cortex
 and then as we saw it on the previous activation display,
 almost simultaneous activation
 on all the rest of the areas.
 So we have here the degree of parallelism
 in the activation process here.
 This looks still quite serial or cascaded.
 And we also see the major difference
 that this one produces this kind of cell assembly
 in the human architecture,
 produces long lasting reverberation of activity
 while here the activation slip goes down right away.
 This is a display of how long activation was kept.
 And you see that the factor is a factor of two
 or even higher than two to four.
 So I'm not saying that monkeys would not have any memory
 for spoken sounds or they can produce,
 but I'm saying that the memory,
 this verbal working with this mouth produced sound memory
 is much better developed in the human brain
 due to anatomical structural differences.
 The difference we implemented in this particular model.
 So why are we able to learn huge recoveries?
 The additional jumping links lead to formation
 of more strongly connected cell assemblies
 which can maintain activity for some time.
 And this provides a brain basis for verbal working memory,
 a specifically human feature,
 which is also necessary for vocabulary building
 for vocabulary learning
 as many language researchers agree.
 Right.
 Let's go to the next question.
 What are the mechanisms of semantic grounding?
 Why is there a variety of semantic brain areas,
 hub areas, and category-specific ones on the other side?
 Let me put more precisely
 what the findings to be explained are.
 Here we have a display of a brain of a patient
 who has a semantic dementia here on the right.
 And you see the temple, those anterior temporal lobes
 have shrunk quite a lot.
 You see they are almost non-existent
 and there's a lesion in bilateral temporal cortex.
 This patient and all semantic dementia patients
 at a progress state have a general semantic problem
 with all kinds of words.
 So people therefore also speak about the semantic hub
 sitting in the anterior temporal lobe.
 And there are other areas
 for which similar claims have been made.
 And now there are also areas
 where there's a contribution to semantics,
 but maybe a smaller one
 and one that primarily affects a specific type of word.
 Here, for example, a study by Tamasio long ago
 and showed that for certain lesions in temporal,
 more focal lesions in temporal cortex,
 there may be a problem, for example,
 with the processing of animal names specifically,
 or here for tool words.
 We could recently show that patients with very small,
 even with very small lesions in the motor system
 have problems processing those.
 So it seems that there are category specific
 semantic deficits related to lesions
 in various regions of the brain also.
 Also, we see this also in some auditory brain areas
 in parietal cortex.
 There has been parietal lesions observed
 to be related to preposition processing, for example,
 and so on and so forth.
 But then there are also the semantic areas
 of general relevance for the brain.
 Why is this so?
 We didn't fully understand this
 and even our early models that we developed
 would not have been able to explain
 the full range of these phenomena,
 but then now we addressed this with modeling
 with brain constraints models,
 which were now a little bit more complex in their structure.
 It's still not a whole brain model,
 but a model in which at least some part
 of not only the perisyllium language cortex
 here in blue and red was modeled,
 but also some dorsal, motor, prefrontal cortices
 and some temporal visual processing stream parts.
 So, and this model was now taught animal words,
 for example, object related words
 and then action related words such as grasp or dog here.
 And again, the auditory and phonological features
 and the articulatory phonological features
 were coded, the word forms were coded
 in the perisyllium auditory and motor cortex
 and in visual cortex, the input coded for visual features
 of the visual shapes of dogs here.
 We did not attempt to also implement the barking
 and the smell of the dog and things like that.
 This was just, we were just considering
 this visual entity, this is a word with a visual referent.
 And also in the case of action words,
 we coded here movements, movement trajectories
 and muscle movements of the actual action
 and these were activated then together
 with the word form representations
 in the perisyllium part of the network
 and near the extracellium part.
 And yeah, right, and here you see what happened
 in the network as a result of the learning of the object
 and the action related words.
 You can see here a lot of activation in the central regions
 where in this convergence zones,
 the association cortices of the model
 and you see a little activation in the peripheral areas
 in the sensory motor areas.
 And also interestingly, these were categorized specifically.
 You see only blue dots here, only red dots.
 The blue dots stand for neurons belonging
 to an object word cell assembly and the red dots
 are neurons, artificial neurons
 in the model of action word cell assemblies.
 Yellow are those neurons that happen to be part of both.
 And yeah, and you can of course put this in a nicer display
 also showing the distribution of the areas
 across the brain and the cortex
 and you can do statistics of the number of neurons per area
 and you can see here in the visual system,
 you get a huge predominance of object word,
 cell assembly, neurons and here in the motor system,
 the other way around, the action word cell assemblies,
 cell assembly, neurons predominate.
 So this is statistically highly significant
 and thus we can proceed to an explanation.
 And one part of the explanation is that the semantic hubs
 are those area where these lots of activation is produced.
 Why is this activation here in the central areas?
 Because of the huge convergence of these areas.
 You remember that each of these areas
 got input from various sites.
 There was a lot of convergence of these areas.
 Therefore, lots of accumulation of activation
 and the more activation there is,
 the more firing and wiring together
 and therefore the larger density of semantic neurons here
 in these cell assemblies,
 in these central hub areas of the network.
 So these areas become semantic hubs
 because they are connector hubs
 because they are convergence zones,
 receive a lot of activation that can correlate.
 But we also can explain the other observation
 that of category specific areas.
 This is because stimulation is a relevant information
 that needs to be linked is transmitted through those areas.
 Visual cortex, we need to learn,
 we need to ground our knowledge
 about the knowledge of meaning of the word dog
 into real world experiences.
 And this happens partly with visual modality
 in the case of grasping is an important issue here
 is to relate it to action patterns,
 actions performed,
 bodily actions performed by the person themselves.
 And so the explanation here is
 why are there a variety of semantic brain areas,
 hubs and category specific ones,
 correlated neural activity in sensory and motor areas,
 tries the formation of simple grounding in object and action,
 word processing dependent on the semantics
 of the specific symbols,
 neurons in different sensory motor areas are active.
 Therefore, model has category specific role.
 And then the correlated activity in modal areas
 converges in well-connected hub areas
 and anatomical connector hubs
 where neuronal activation accumulate.
 And this leads to the high density of fellows and de-neurons
 and to the emergence of semantic hubs,
 for example, in anterior temporal side remark.
 We can provide here explanations
 or at the cognitive level
 by looking at the neurobiological,
 underlying neurobiological model
 by the mechanisms or the simulated mechanisms
 as always a possibility that the simulations
 are still not good enough and need to be further improved.
 And I'm waiting for your comments on that.
 And then there's the Hebbian learning procedure here,
 which explains the circuit formation
 and also the distribution of the circuits
 across different areas of the model.
 And this can be used then to explain the cognitive
 observations.
 Another question, is language an important factor
 in concept formation or even necessary for it?
 And this also relates to the question
 about the difference between abstract
 and concrete concepts and words.
 Let's just look at grounding of concepts
 and concepts and semantics more closely.
 If we learn the word form eye,
 we may relate this to a visual or the word dog
 in the previous example.
 Now, the word eye is a thing we can actually perceive.
 And then we may see activation of a word form representation
 in peer resilient cortex, though activating
 with a visual percept with a bunch of neurons
 in the ventral visual stream.
 And that relates to the perception of the related object
 an eye, for example.
 And of course, the different neurons here
 respond to very different things.
 There may be simple sensors around cells
 in the visual cortex.
 There may be complex feature detectors
 that respond to a constellation of lines and shapes.
 And there may be some neurons that respond
 to rather complex entities.
 For example, here a bird version
 of an approximate eye shape.
 Now, these may be differently engaged
 in concept learning and in word learning
 because of course, if I see this particular eye,
 there are specific features of this eye
 which are not relevant and which are not,
 so that in other cases of perceiving eyes,
 the respective neurons may not be activated.
 For example, the neurons that respond to eye color here,
 they seem to be rather idiosyncratic
 and they may only activate with one instance of the concept,
 but not generally.
 However, other features such as the thing
 that it's usually a round thing
 and a fine line around it,
 that one of these neurons might become active
 whenever eyes are being perceived.
 And therefore, if we learn the word form,
 maybe linked to the semantic network.
 Though these shared features
 might be of specific relevance here.
 And if we, next slide.
 If we now look at the situation,
 what we could consider those neurons here in the middle
 overlap neurons as semantic insofar as they represent
 the shared features of the different instances
 of the concept and those become active together
 whenever one of the instances is being perceived
 and therefore they fire together a lot
 and wire together and form a conceptual cell assembly.
 Now, if we look at abstract concepts,
 the patterns are the situation is much more complicated.
 If you think of a concept such as beauty,
 then they are very different.
 One may say beauty is something that exists in open air,
 but from a grounding perspective, you may argue, no,
 we actually experience different instances of beauty.
 But these are very, very different, physically, perceptually.
 And so how can we learn that there's a concept?
 For example, these are three examples of beautiful things
 that are normally seen as beautiful.
 And, but if you look at the features,
 the physical visual features,
 they overlap across the three is actually zero.
 And if you look at pairwise comparisons,
 there is a little bit of similarity.
 There's something round in the center of those two.
 There's a fine line curved in these two.
 So there's a little bit of overlap,
 a little bit of sharing of features,
 but no generally shared features.
 So then, and we call this following Wittgenstein,
 the pattern of family resemblance.
 So there's no general feature overlap,
 but the feature overlap is only partial.
 As in the family, some family members
 may resemble each others,
 but others may look very different from each other.
 Yeah, and here, the bad thing is that the correlation
 between these three, between the activation
 of these partially shared neurons is actually very bad.
 If you ask how high the probability is,
 if these different instances just appear equally probably,
 then you see that the probability of co-activation
 of each of these neurons is just one third
 of their activations.
 Now in these shared neurons, they are active together
 whenever one of these perceptions take place,
 whenever one of these objects instances are being perceived.
 So here we have a probability of one third
 that may not be sufficient for concept formation,
 for formation of a strongly connected circuit.
 Now these probabilities change if we add a word for,
 if not so much in the case of the concrete word,
 because if now, if each time the object appears,
 the word is now used, which rarely happens,
 but may happen in some cases,
 and I just use this for exhibition
 to calculate the numbers,
 then we may still end up with a probability of one
 and a very strong cell assembly therefore uniting
 the word form with the concept and building a semantic circuit.
 Now here, the probability is at least twice as high
 that one of these three neurons co-activates
 with the word form.
 Why is this so?
 Just think about it.
 You hear this word together with this,
 together with this, together with this.
 Now in these three cases, this neuron is active twice.
 So we have a two third probability in this simplistic example,
 but of course with more sophisticated examples,
 the relationship of the probabilities stay the same.
 That is in essence, this word form addition here
 has a functional contribution.
 Make it enhances the correlation of neural activity,
 or in other words, these semantic features
 can bind together by way of the word form.
 The word form is a correlation enhancer
 or this may help the building of abstract concepts.
 We modeled this and the modeling is quite complex
 and my time is running out.
 Or how I am with time, Steve may ask.
 (indistinct)
 - Hello?
 - Yes, yes.
 You had 45 minutes by default,
 but you can take an hour and leave a half hour
 for discussion.
 - But are my 45 minutes up already?
 - The time right now is, well, it's 47 minutes,
 but it's okay.
 Keep going.
 - Okay, give me five more minutes.
 Okay, so the brain constraint model,
 I will not go into the details then,
 indeed the concept, but I will summarize
 what came out in the modeling
 when we did model the learning of abstract
 and concrete concepts and also then the effect
 of adding a label on these concepts.
 So the concrete concepts build conceptual representations
 easily when experiencing category instances,
 different types of eyes, for example,
 due to shared features and strong correlation
 of shared feature neuron activations.
 Then abstract concepts, however,
 such as freedom or beauty,
 they did not build representations
 from experience with instances.
 And this was because they only partially shared
 features and resultant, and therefore low correlation
 between semantic feature neuron activations
 resulted as already explained.
 Now, when adding the labels,
 when adding words for abstract concepts,
 this facilitates both concrete and abstract concept
 formation, but is much more essential in this model
 even necessary for the latter or abstract concept formation
 due to relatively higher correlation
 between category label and partially shared
 semantic feature neuron activations.
 So compared with correlation across the party
 shared semantic feature neurons activated.
 So this correlation enhancing function
 of labels were documented also in the simulations.
 This is from my colleague, Finn Dobler,
 and the activations here of the different types
 of semantic cell assemblies.
 Here for the abstract concepts,
 there was very little activation.
 And for the concrete concepts learned in isolation
 without language, there was a little bit more activation
 at the peak, but then the big differences here
 in the memory behavior, you can see that activation
 lasts for quite some times.
 You're in arbitrary time displayed in arbitrary time steps.
 But the important thing is that the abstract concept,
 it did not have any memory.
 You may argue that here it takes four time steps
 until activation goes down to the zero line again,
 but you have to consider also that the network
 has six areas from input to the end.
 So the pathway from the input to the other end,
 the far farthest other end of the network
 is indeed five time steps.
 And you can see there's very little activation here
 after five time steps, and after this activation
 just flowing through the network one way
 and there's nothing left, no maintenance of activation.
 This is just feed forward and activation way,
 running feed forward through the network.
 While here in the conceptual representation,
 in the conceptual representation of the concrete concept,
 there is a maintenance of activation
 for many more time steps.
 When we add a global support for these concepts,
 we see a much stronger activation,
 which peak is delayed also,
 and also an even longer activation and maintenance.
 So in essence, the conclusion from this,
 we can not learn, our neural network
 suggests we cannot learn abstract concepts
 unless we have verbal labels that support them.
 While for concrete concepts,
 there's also a benefit of language,
 but it's not an all or nothing thing
 and it's just a more efficient
 and more long lasting memory intensive process
 that comes about with the labels.
 Okay, so I promised you to stop in five minutes.
 Let me just summarize, it may be useful
 to brain constrain neural networks at multiple levels,
 single cell to local circuit to global area
 and connectivity levels to explain various things,
 why humans but not monkeys have verbal working memory
 and huge recoveries, how grounding works
 and semantic grounding and similar grounding
 and why the brain develops category specific
 as well as the general semantic hub areas.
 And then to explain the formation
 of abstract conceptual representation,
 which made and then also explain the influence
 of language on cognition at a more defined grain scale.
 And finally, of course, to clarify the nature
 of neuronal representation in the brain.
 We suggest that it's both symbolic,
 discreet and distributed probabilistic.
 Right, this is to thank my collaborators.
 I mentioned already Finn Dobler here,
 Thomas Rennikers with whom I started the whole project
 20 years ago in Cambridge and Malte Schomas
 who did most of the abstract concrete simulations
 more recently, Finn has joined him
 and he's now carrying on this project.
 Rosario Tomasello who has been instrumental
 in the simulation of semantic grounding
 already starting five years ago.
 And then Maxime Carrière who was in his PhD project
 and then looking at the role of the archivistical,
 for example, and I thank you very much
 for your attention.
 - Thanks so much for this panoramic talk.
 Now I give it over to your discussant,
 your designated discussant, who you already know.
 - Hi all, so yeah, thanks Professor Palmer
 for a really excellent, this is really I think inspiring
 and intuitive view of how abstract concepts
 form through language, basically.
 Concepts are correlated bundles of features
 and for abstract concepts, the only thing that correlates
 them is the labels that you're seeing.
 I've got a few comments, questions and thoughts on this
 which I'd be curious to get your thoughts on
 and maybe from the whole group as well.
 So yeah, so the idea is that abstract concepts are forming
 because basically of the labels that were given to them
 through language.
 And this really raises this idea of, you know,
 the Warfian hypothesis for cognition and language.
 So this Sapir-Warf hypothesis is that the language
 you speak can affect the thoughts that you think.
 So like the idea is if you have a language
 that somehow doesn't have a word for like time,
 maybe the way you think about time is somewhat different.
 It seems to me that this view of concept formation
 is basically driven by labeling.
 That suggests that language has a huge influence on thought.
 There might be abstract concepts that are just inaccessible
 to you unless you are basically taught them by language.
 I just wonder how far you are willing to go with that idea.
 Because so the experimental support for the influence
 of language on cognition is it appears to exist
 in some areas and not other areas.
 It's a bit of a mixed literature.
 Another sort of high level pointer question is,
 so you're asking the question of why humans
 have huge vocabularies.
 How is it that the brain supports language learning
 in humans, but not in any other animal?
 And it seems it has to do with jumping links
 that allow more densely connected cell assemblies
 to represent words, both in their acoustics
 and their meanings.
 What I wonder is like, what's the interplay
 of form and function here?
 To what extent might we have something like a chicken
 and egg issue here?
 So like, is it that the neural structures we have as humans
 enable us to learn large vocabularies and language?
 Or is it that the fact that we have to learn
 large vocabularies has shaped the neural architecture
 over evolutionary time?
 So relatedly like, if you see the brain
 as just like a general optimizer,
 you can imagine that the brain will find some solution
 to the problem of learning large vocabularies.
 Maybe it will involve jumping links,
 maybe it involves something else.
 A very interesting question is,
 to what extent does the architecture truly constrain
 the form of language?
 Or is it just the architecture is responding
 to something else in the structure of language?
 A third connection, third interesting point I'd like to make
 is that I think the work here that you've presented
 on this question of local versus distributed
 representations of words, it's very closely related
 to recently work on interpretability
 in large language models.
 So there's been a lot of interest lately
 in trying to figure out if you can identify
 any particular neuron in a large language model like GPT-3
 as corresponding to a concept like a word.
 And one of the very interesting insights from that work
 is that sometimes you can, sometimes you can't,
 sometimes you can find like the grandmother neuron
 that only responds to the concept of a grandmother
 inside of enlarged language models, sometimes you can't.
 But it actually, there's been an insight
 in the interpretability literature where they've found
 that to some extent it actually, there's no real difference
 between a localist representation and a distributed one.
 So a distributed representation where a concept
 like grandmother is spread out over multiple activations
 of multiple neurons, that could just correspond to,
 if you're thinking about the activation of neurons
 as a vector, a distributed representation
 and a local representation just correspond
 to using different bases to write that vector
 in that mathematical vector space.
 And it turns out in terms of like the mathematical
 predictions from a model, there's no real,
 there's no privileged basis.
 There's no reason to favor one basis in terms
 of what counts as a neuron over another.
 You can sort of rotate the space of activations
 and in artificial neural networks and get the same behavior.
 And so an interesting question is to what extent
 is that true in biological networks?
 Like to what extent does it really matter
 in biological networks that it is one neuron
 as opposed to a distribution over neurons?
 Does this idea of basis independence hold in any way
 in biological networks?
 Those are just some of my thoughts, questions.
 I'd be curious to know what you think.
 - Yeah, thank you very much.
 Fantastic.
 Thank you very much for these very deep questions.
 Can I just go ahead and answer one by one?
 - Yeah.
 - Yeah, okay.
 You're absolutely right that of course what I'm saying here
 that language should have, or the model would say
 that the language might have a strong influence
 and concept formation is something that would be,
 that was very popular in the linguistic relativity
 school of thought already.
 However, if you look at Wolf or also at Von Humboldt,
 you would not see an answer to the question,
 why is this so?
 They would just say language is such as this magic ability
 of determining our thoughts and it has our,
 and influencing our perceptions.
 Why this is so is not explained.
 And the explanatory question is in the center
 of our research.
 So this is, it's indeed true that this is of course then
 in the tradition of Wolf and Von Humboldt
 and also more recent people
 who actually accumulated evidence.
 However, the evidence, as you correctly pointed out
 is quite mixed.
 Some of the results about, especially about perception
 and influence of language and perception is quite mixed.
 The Russian blues of Winnowhere 2008 are not always replicated.
 However, they are more, but all this research
 is also characterized by abundance of grottily bad studies
 simply because this is a so complicated issue
 that it's very difficult to address experimentally.
 They, for example, compared people in most of the studies,
 compared people from different cultures,
 speaking different languages.
 And of course, if Russians from then suddenly distinguish
 between shades of blue more effectively
 than other color distinctions,
 you may see that this relates rather to the way of live,
 their way of living or maybe through a thousand other things
 but their language.
 So what we need in this domain very, very urgently
 is good experimental paradigms.
 And we and others have indeed used
 such well-controlled experimental paradigms
 where we teach adults new languages
 or at least partly some parts of the language.
 My colleague Tally Miller here at Freie Universität
 has done a series of experiments on this
 and found that if we tightly, very tightly control,
 for example, the ability to distinguish sensory patterns
 applied to the digit, to one of your fingers,
 you may not be able to discriminate very fine differences
 but with language support, you will.
 And this has meanwhile be replicated in other labs,
 for example, by Majid and in other modalities also.
 And this seems to be a rather strong indication
 that language can impact our perception.
 So far to the mixed evidence, it is indeed mixed
 but there's also some strong supportive evidence meanwhile
 even though Pinker was actually quite negative
 about the approach, I think.
 But of course we need to be open to experimental results.
 And next point, you asked about what constrained what.
 So the hen and egg problem, so to speak.
 Well, generally in evolutionary biology,
 the idea is that we have a certain set of abilities
 and if this is useful in the environment
 for survival and for well-being and God knows what,
 then you may maintain it, you may just reproduce
 more efficiently and so on and so forth.
 So it seems we cannot assume that there was a need
 at one stage to produce more words
 and then that the archid fascicle
 suddenly grew out of bounds.
 This would be very naive.
 This would not be as according
 following Darwinian evolution theory.
 We would actually assume that there were random mutations
 that produced apes with slightly different archid fascicles
 and therefore they may have learned a few more signs
 and that was an advantage to them
 because they could communicate more efficiently
 and then it grew a little bit more
 and this was another advantage.
 And finally there was a really sick thing
 and that also enabled working memory
 and then suddenly there was an explosion of vocabulary
 after verbal working memory was obtained.
 So what is constraining here is, I think,
 the evolutionary progression.
 But then, of course, the model needs to be
 under constraints of what the brain actually has
 and we cannot use a deep neural network
 or a chat GPT kind of architecture
 to model the archid fascicle
 because there is no correlate of any archid fascicle
 in this type of model.
 We need to make the models biological.
 That's my point.
 Finally, large language model
 sometimes have word representations and sometimes don't.
 Well, that's the problem of these models
 because they are not biologically realistic.
 They have, if you look at, for example,
 a deep convolutional network,
 they have feed forward connection
 which is not so biologically realistic.
 They have topographical projection.
 So one neuron projects not to the entire next layer
 only to a local neighborhood.
 This is very biologically realistic.
 This is exactly how the brain works.
 They have topographical projections all over the place.
 But then the aria structure, as I said before,
 is not exactly what you have in the brain.
 And also importantly, the connections within each area,
 they have no analog in this kind of model.
 There's no inhibition within each layer.
 And these are the factors, as our work indicates,
 that lead to the fully distributed nature
 developing in this kind of models.
 This is not biologically realistic.
 If we add local connections, we get discrete circuits
 and those have the advantage of then allowing us
 to represent two words in one layer.
 Because if you have full vector representations
 and you activate two of those at the time,
 you get interference and that's a big problem
 of this kind of model.
 And it works much better in these cell assembly models.
 So essentially, I would say,
 whatever happens in large language models,
 as long as they are not very much constrained
 by more biological features,
 we shouldn't take this too seriously.
 Although, I do not want to speak against
 the doubtlessly great achievements of some simulations,
 for example, Krieger's courses,
 simulations of the visual stream,
 which can be seen as a cascade of several area activations.
 What is, I think, very instructive
 for explaining the difference between perceptual
 and cognitive conceptual representations.
 - Yeah, sorry, I don't answer too many questions.
 Are there, thank you.
 Thanks so much.
 Steven, are there more questions or is my time up?
 - Let me just see if there are so many different threads here.
 I have to find out whether there are any.
 If you have questions from here,
 you can raise your hand.
 If you're out in panelists, you can just talk.
 Okay, oh, Steven Hansen raises.
 Okay, so go ahead, Steven Hansen, go.
 Can you show his face?
 - He has to show his face, yeah.
 - All right, oh, here I am.
 That was a wonderful, this masterful tutorial
 from very simple kind of connectionist
 and neural network kind of modeling
 into brain structure and brain imaging.
 You managed not to bring up resting state,
 which I think is probably a good idea in this case,
 but the overall structure of the story
 relative to the LLMs, the chatbots,
 you keep stressing the idea
 that there has to be a biological kind of context
 for everything, that the back propagation isn't biological.
 And of course, this has gone on for 50 years
 and yet it is the engine behind all this.
 It makes one wonder, you know,
 if clearly the brain is structured
 and clearly these LLMs are not structured.
 They have a very simple feed forward structure.
 They have these attention heads,
 which are a lot of aggressive, probably recurrent.
 And if we think of the brain,
 let's say the cerebellum as prediction simulation engine
 in the cerebellum, which is trying to make some kind
 of first order sketchy simulation of what's about to happen
 and why the context is this and so on.
 We think of the LLMs also creating something
 that looks like 50 to a hundred million contexts.
 Let's call them clusters of words, roughly,
 in your sense that some might have features
 that are overlapping, some might be more Wittgensteinian
 more family resemblance and so on.
 There's something about the lack of structure in all this
 and the fact that we're using, and I don't disagree
 that back propagation has some biological issues.
 I think there's ways to regularize it,
 but generally pretty much everything
 about this is non-biological.
 So your claim that it has to be biological to work
 seems not very effective.
 - Okay, time for a reply.
 - Yeah, well, I'm not sure what I should say.
 Of course, I'm criticizing that not many models
 are not very biological, even though this is of course
 not an argument against their achievements.
 If large language models are feed forward
 and the brain is not feed forward or it's rarely
 the cortex at least is not feed forward,
 then this is not an argument against the achievement
 of the generative pre-trained transformers.
 These are very efficient machines to solve tasks,
 but they are not very good tools for answering questions
 about why the brain does things as it does.
 This is simply the point I'm making,
 and I think for me personally,
 it's quite exciting to try to answer questions
 about the brain, difficult questions about the brain
 by using neural networks as toys in this explanatory game.
 And here, GPT is not so useful,
 but I cannot say that everybody shouldn't be misunderstood
 as a general criticism and think that then certainly
 if something works very well and works different
 from the brain, it's still a useful thing for various things.
 - Okay, thank you.
 - Not so much for explaining the brain.
 And I would still doubt that even backpropagation
 must be a principle that is realized in the brain.
 There are analogies and similarities
 between heavy and learning and backpropagation,
 but exactly as it's done in backpropagation
 where an error signal is required,
 it's difficult to reconcile, I think.
 - Steve, do you want to do a quick RIPRT because Gary-
 - I agree in general, although I think the problem here
 is at the behavioral level, what we're seeing
 is something that appears to be producing
 at least a context-free grammar.
 And this has destroyed poverty of the stimulus,
 this has destroyed our obvious kinds of language networks
 that we understand, aphasias and other kinds of problems.
 It's just completely destructive to the thesis
 that the biological nature of the structure of the brain
 is anything we have correct.
 - Okay, that's an assertion.
 Friedel, that one has to be answered by Rodney Cutseer.
 But go ahead, and then I'll let Gary speak.
 - Are you saying I should say again?
 - Go ahead, but it needs to be answered
 by the people who are actually-
 - Only one point, you speak about poverty of the stimulus.
 Now, the problem, as I would see it,
 the problem of large language models
 is that they get much more evidence
 than any human being would get in their lives.
 And especially- - I agree with that.
 - And that's unrealistic.
 And also from a linguistic perspective,
 it makes these models less interesting
 because you cannot be sure whether the particular,
 the rare construction they are processing
 has been experienced by them.
 - Sure. - So it's not generally,
 whether it's generalization or not, we don't know.
 Sorry. - Let me cut that off.
 That'll be taken up by the linguists
 and the computer scientists.
 Gary, go ahead.
 - Yeah, thank you.
 Thank you, Friedemann.
 Yeah, I wanted to say two things.
 One, sort of in kind of response to Richard's comment,
 who of course made the correct inference
 that the view you're presenting overlaps
 to some extent with the Warfian position.
 And I just wanted to add that the,
 perhaps the best evidence in favor of language
 having this effect on our conceptual representations
 and it's not so much comparing languages,
 although as Friedemann, as you and Richard pointed out,
 the evidence is mixed,
 but comparing language to no language.
 So whenever humans are deprived of language
 or even have reduced linguistic experience,
 the cognitive consequences are enormous,
 much, much more than what you see in cases of sensory
 deprivation.
 So a blind child is cognitively much better off
 than a deaf child who's deprived of language.
 The deaf child who's exposed to the proper linguistic input
 through sign, for example, is cognitively fine.
 So that's one point.
 And the other point, Friedemann,
 I wanted to push back a bit on the part about
 comparing chimps and humans and vocabulary.
 It seems that as Terry Deacon and others have pointed out,
 that the mystery is not why don't other languages,
 other animals have complex languages,
 it's why they don't have language at all.
 And so in appropriate cases,
 in cases of human-reared chimps, for example,
 chimps can learn fairly large vocabularies,
 dogs can as well.
 They're not motivated to do so.
 And that's kind of where it seems to be the rubs.
 So a lot of the cognitive machinery to learn,
 signs for things seems to be there.
 It just requires the kind of training and experiences
 that children don't need
 because they're intrinsically motivated to learn this stuff.
 And so I'm wondering how you make sense
 of these animal language learning experiments
 that show that a lot of the cognitive capacities are latent.
 They're there, they're just not really used in nature
 for reasons that seem to have less to do
 with kind of hard limits on cognitive capacities
 or cortical wiring.
 - You don't know?
 - Well, I think if you look at the great apes,
 is my microphone still working?
 - Yes, it's working fine.
 - If you look at these experiments with the great apes,
 they were trained for an entire life with simple use
 and it was not spoken words, of course.
 I'm speaking specifically about spoken words
 and they were complicated machinery
 with symbols on the buttons and God knows what.
 And it was not possible to get them above a level
 of two or 300 symbols.
 So which contrasts with a 40,000 or a 100,000 symbols,
 humans learn easily.
 I'm not sure whether this is this difference,
 this quantitative difference is easily explained
 by motivation or I think these MIT researchers training
 the animals did their best to motivate them.
 And I'm not sure motivation is the only,
 the best, the final explanation here.
 - Very funny.
 - Well, a language that's a few hundred words is still,
 it's a simple language, but it can be a language,
 but you don't find even that in the wild.
 So like the argument isn't that,
 oh, provided the right training,
 chimps can learn 40,000 words.
 They can learn a lot more and systems
 that are much more language-like
 than they actually use in the wild.
 And it's not just chimps, so many animals can.
 But the fact that they don't do so in the wild, right?
 That's the point, speaks to, yeah.
 So I agree with you that yeah,
 motivation might not explain the gap
 between a few hundred or a thousand and 40,000,
 but you can have a pretty powerful linguistic system
 with just a few hundred symbols.
 - Yeah, so of course we can have a powerful linguistic system
 is just a zero and a one.
 - Well, but cognitively what matters are the chunks, right?
 It's what you do with it.
 So we're not talking like elementary units.
 - But aren't the number of chunks then a good criterion
 for distinguishing simple and complex
 if the chunks are so important?
 - Yes, oh yeah, I agree with that,
 but don't find simple languages in other animals.
 And a language with a few hundred words
 is certainly simpler, but it's something,
 but you don't really find that.
 So I'm just saying that whatever limitation
 in learning large vocabularies
 is perhaps not the central kind of constraint
 that's relevant for explaining why other animals don't have
 what we call natural language.
 - As moderator, I'm allowed to say a word here.
 And I think that you're making,
 in counting the words, both of you,
 I mean, the freederman is saying it could be just zero and one
 and Gary is counting the words.
 You may be looking at the wrong thing.
 It's not the words,
 it's the capacity to make a proposition with truth values
 that might be missing.
 And in Gary's direction, in Gary's point of view,
 the capacity even for that may be there,
 and it can be learned.
 In principle, it can be learned.
 You can show it with a machine,
 but it isn't clear that it's learnable is enough.
 You also have to have the motivation to learn it.
 - Yes, I would agree.
 You have to have the motivation to learn it.
 And of course in the human social communicative context,
 there's a lot of motivation for that.
 I'm absolutely with you that there are other factors
 than the archivist festival.
 - But the motivation is not cultural, it's biological.
 The children who go through their naming explosion
 do it just as a matter of course.
 It's not because they have a taste for naming
 that they've developed.
 - Well, but they do all their language use
 is actually a social activity.
 And they communicate and exchange
 and interact from the first word on.
 - Moderators have the right to intervene, but not very long.
 - Raw.
 Steve, do you still have your hand up?
 - No.
 - 'Cause lunchtime is looming for you.
 - No, no, I think, yeah.
 There's a lot more to talk about,
 but it's gonna take a lot longer than we have.
 - We have, well, we have nine more days,
 three more sessions today.
 - Okay, thank you very much for attending this session.
 Have a good lunch.
 You don't have very long to have your lunch.
 And be back. - Thank you so much.
 (indistinct)
 - Okay, that's it.
 - Yeah, thank you so much.
 See you later.
 - Thank you, yes, of course.
 Thank you very much.
 Bluster, moderators, we get to thank their speakers.
