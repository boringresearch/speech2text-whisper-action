 He is a senior director of machine learning research at Apple since 2021.
 His research interests span areas of machine learning such as deep architectures, representation
 learning, vision, language processing and more recently reasoning.
 He co-wrote the well-known open source torch machine learning library.
 And with that I welcome Sami Benjo.
 I hope since you worked also with vision that you won't neglect the question of grounding
 when you talk about reasoning.
 Thanks Steven.
 Thanks for inviting me.
 It's good to come back to Montreal where I have been for 25 years or so.
 First check, do you hear me in the back or do I need a mic, all good?
 So I will try to tell you why I think even with the biggest LLMs that exist in the world
 learning to reason remains and will remain hard for some time.
 So let me start by trying to explain from my point of view what is reasoning.
 So reasoning is the action of drawing conclusions efficiently by composing concepts that you
 already know.
 So here are a few examples of reasoning.
 Some of them are more or less hard even though they all look the same.
 Suppose you are given these input symbols and you have to classify them into ones or
 zeros and you have to find the pattern that makes it ones or zeros.
 And the pattern that you should see here is that when the two symbols are the same it
 should be one.
 If they are not the same it should be zero.
 So what you see on the left is the training examples you have to try to come out with
 the reason of the solution and on the right I'm giving you examples that are more or less
 difficult to actually solve.
 So you can see a pair that you've never seen like the two squares, you've never seen the
 two yellow squares on the left, you can see new symbols that you've never seen on the
 left like the star, you've never seen it.
 You can see a different number of inputs like three symbols instead and the rule is basically
 the same once you understood it no matter which symbol you use, no matter how many symbols
 there are and no matter the combinations of symbols you see should always be the same.
 So the idea of reasoning is that you can come up with solving problems you've never seen
 before just because you're putting together concepts that you learned with the training
 examples.
 And it's very different from memorization where you would just put all the examples
 you've seen in a big memory bank and get out solving similar problems.
 So usually the main trademarks about reasoning is two things, one is extrapolation and the
 other one is efficiency.
 So extrapolation means that the examples you are going to see during test or at inference
 when you want to use the model are going to be different from the distribution that you
 saw during training.
 And this is about reasoning because since you're out of the distribution you have to
 find ways to compose from the examples you've seen in the past to try to solve problems
 that were completely unseen, even the distribution of them wasn't seen.
 It's also about efficiency because in fact if I gave you a combinatorially more example
 of training you wouldn't be able to solve it but that would need so many examples at
 a time that wouldn't be efficient.
 So it's really about these two things.
 Why is it challenging?
 Well because in the days of LLMs we can see how the models that we train are bigger and
 bigger so they actually have more and more capabilities of memorizing everything you
 tell them and indeed they do.
 So because they can do that they can easily come up with a simple solution that is just
 like a nearest neighbor of one of the training examples you've seen.
 But that might not be the right solution to do.
 And obviously you could say well I'm going to get the bigger model again and I will use
 more GPUs but at some point you won't be able to solve it.
 So if we can be smarter instead of just getting more data that will be useful.
 In many problems like arithmetic or algebra or some kind of reasoning or planning there's
 basically no way you will get enough training example ever to really be able to solve these
 reasoning problems if you didn't understand the actual simple combinations of rules that
 we need to apply.
 So I want to give you a few examples of the fact that it's even now hard.
 In a recent paper from people in my team they looked at visual reasoning.
 So look at the graph on the left.
 So you see this grid of nine symbols and you're asked what is the bottom right symbol that
 should be there and you have six options and you have to find how they are connected to
 each other and that's the visual reasoning part and it turns out that it's very hard
 for an LIM.
 So Gemini Pro, GPT-4V, all these large models that you've heard in the press or anywhere
 else you're using your newsfeed they all fail to this kind of example.
 So there's basically no good LLM that can succeed.
 Something like that that humans can usually solve.
 That's one example.
 Let me give you another one.
 It's the parity.
 So let me explain first what is parity.
 You're given a number of symbols, a sequence of symbols, of two kinds of symbols.
 Here in the example you have a yellow square and red triangles and the problem of parity
 is to just tell me whether there is an even or odd number of triangles.
 It is actually a simple problem if you understood it.
 You can solve it very quickly.
 Now what will be difficult is that at training time we only show you examples with a fixed
 number of or at most say 20 symbols of input.
 So there will be no more than 20 squares and triangles at all and at the end I'm going
 to show you longer sequences but the algorithm remains the same and you could solve it in
 a minute.
 If you actually understood what was the question, you could solve it no matter how many symbols
 you see at input because you just need like a one bit memory in your mind to know whether
 you've seen an even or odd number of these triangles and then you can solve it.
 It turns out that it's hard for LLMs.
 So on the left I show an example where you see the gray part is the number of symbols
 that were seen during training.
 So the model was trained and saw between 10 and 20 symbols and the different curves you
 see is the performance of different models of different sizes from a few millions to
 a few billions parameters and you can see that when the test examples are about the
 same length as the training examples, the accuracy is 100%.
 So it understood as soon as the number of inputs grows longer than 20 you can see the curves
 going down very quickly and none of these models even the bigger ones like the 64b model
 which are like in the big size that we see today, they all fail very quickly.
 There are many ways to improve that.
 One technique is called the scratch pad and another one is fine tuning.
 I won't go into details but even these techniques fail.
 Sometimes they fail a little longer after but they do fail.
 So something is wrong.
 That problem is not that difficult but you can see how the model actually was able to
 understand something very restricted that was conditioned on the length of the input
 sequence.
 So you don't really understand the concept of parity, understood a very restricted set
 of parity.
 So I'll give you this morning two vignettes about why it's hard and we'll see where that
 goes.
 So the first one is a paper from last summer at ICML, one of the machine learning conference.
 It's called Generalization on the Unseen and I don't know how it works but I think you
 can ask questions whenever you want.
 Is that how it works?
 If that's what you wish.
 Yeah, feel free to ask questions when I'm not here but I don't always know the level.
 So let me start with an example that will be similar to the parity one I showed you.
 Here the example will be you have to learn the majority target for three voters.
 So there are three people, x1, x2, and x3, they can either say minus one or one, they
 have only two things to say, and each of them will say something and you just need to learn
 the majority function between the two.
 So which of the two symbols do you see in a bigger quantity?
 That doesn't sound difficult but of course I'll make it a bit more difficult.
 So first, it's a very restricted problem, it's just an example.
 There's only like eight examples that you can see all here.
 That's the totality of the training set but to make it a bit more complicated, while training
 I will always hide examples where both x1 and x2 both minus one together.
 So examples where x1 and x2 are hidden.
 So these two, I will call that the holdout or the unseen.
 So during training we never see these two things, you have to make your mind about that.
 So you can see all the other ones.
 You can see the training set which are all the other ones.
 So now, given that situation, there's actually many solutions that can fit that problem.
 And you can characterize all the solutions that can fit that training set with the actual
 real solution, the majority, whatever that function is, I'll get back to it, plus a term.
 And that term, you see this delta x times one minus x1 times one minus x2.
 And I've put that term because it's interesting, because x1 and x2 are never minus one together,
 there will always be one of these two terms at training that will be equal to zero.
 And so at training time, the term on the right, the delta x times something, is always equal
 to zero, which means we don't care about delta x.
 Delta x is a function and whatever delta x is, the solution will be majority.
 So there are many solutions that will fit the training set, they are all of the form, the
 actual solution you want, plus a term.
 And that term can be anything.
 And the problem is that we don't know which one the model will learn.
 It can learn any of these ones.
 And you know, what is an LLM?
 What is a deep net?
 What is a neural net that will be trained on stochastic gradient descent?
 Which one will it choose?
 What I'm going to show in the next few slides is that we have characterized which one it
 will learn, actually, for real.
 And it's called a solution that will be biased towards mean degree.
 Let me try to explain what that means.
 So when the inputs are just minus ones and ones, and we're only talking about Boolean
 functions, where the output is also minus one and one, all these functions can actually
 be expressed as polynomials.
 So of the formula, some function of x1 plus x2, product of them, this is the kind of functions
 you can express.
 And the majority is one of them.
 So it's just the sum of the terms, plus minus the product of the three divided by two.
 Turns out this is a polynomial of degree three because of the last term is a product of three
 terms.
 That's what it means.
 And what we showed is that if there are many solutions, the one that the model will choose
 is anyone that has the lowest degree.
 So a solution where all the terms are the smallest.
 And in particular, there is one of degree two that fits the training set.
 And that's the one that the model will fit.
 And that's sad news because it means it's not going to learn the majority, it's going
 to learn something else.
 This term on the right, no matter how you do it, no matter how big is your model.
 All right.
 Why is that important anyway?
 Well, it's important because it's the basis of reasoning problems.
 Most reasoning problems will be in this situation where you have access to a training set and
 it's incomplete because it's impossible to have a complete view of all possible combinations
 of symbols when the number of symbols is large because it's combinatorial.
 So quickly you have more of these combinations than the number of atoms in the universe and
 you do.
 So you're always in the situation of what we call out of domain generalization or extrapolation.
 So there are many places where this happens, I already said that.
 So I'll try to explain how we come up with our finding, which is that we now understand
 which solution the model we choose by making it as simple as possible.
 And we developed this framework that we call generalization on the unseen.
 We're going to make it as simple as possible by saying-
 I have a quick question.
 Oh, quick question.
 Yes.
 You left out the minus one, minus one, minus one case.
 Can we, can we assume in any learning example that somehow intuitively it knows that you
 don't have to worry about that case?
 That you don't have to worry.
 You cannot assume that's the, the problem is that we don't know what the problem is.
 And that's what we're trying to understand, but we cannot assume anything though.
 It's just, it, it's like saying I've never seen the, you know, images, I've seen images
 of dogs with the sun, but never with the rain.
 Does it mean that there will never be rain?
 Yes.
 But your conclusions about the difficulty of reasoning of neural nets are based on this
 particular, in this example, with a constraint on the sample, it's saying a condition on
 the sample.
 But you don't know that's constraint.
 You only see the examples.
 I know because I see everything, but the model sees examples.
 It doesn't know that there is a subset, but there's a mathematical formula that explains
 which subset it's going to see.
 It's just--
 And the general conclusion still holds, which is that this is true about reasoning in general
 and not on this particular--
 Exactly.
 The reason it's true is that when you reason with symbols, so these are, you know, discrete
 things, they take discrete values, there are combinations, combinations of these symbols
 that you will never see at training time, because there's too many of these combinations.
 It's impossible to see.
 And so you can easily come up with an example where you will have, like, a set where you're
 missing some combinations.
 You still have to be able to do something about that.
 We humans don't need to see the combinations.
 We have a way to, you know, we have what I would say a regularizer that tells us we need
 a simple solution that explains what I've seen.
 And the problem is that what we mean by simple is not what the neural network means by simple.
 There are many definitions of what simple is.
 That's what I've been trying to explain.
 The model has a definition of simple that doesn't-- simple that doesn't align with our definition
 of simple.
 And maybe we can fix that, but today it's not fixed.
 So here's how we came up with this solution.
 We're going to consider any problem where you are-- there's a domain, so the domain
 is all the possible combinations of input that your model can see.
 And during training, the model will be able to see any combination of inputs from part
 of the domain and none from another part of the domain.
 And it doesn't need to be anything that you can express, but there is a part of the domain
 that you will never see during training.
 And the question is, how are you going to do if I now show you an example in the part
 of the domain that you have never seen during training?
 So you see it's the same idea as I proposed in this simple example, but now it's more
 generic.
 And the question is, is there any hope whatsoever that we can learn a model that will solve any
 of the questions that will be given in the unseen part of the domain?
 And as I said, I think the solution is that you need some prior.
 You need some ways to simplify, to regularize your solution so that it goes towards the
 solution that you care about.
 So quickly about these functions, because we're looking at a very simplified problem,
 we wanted to be able to simplify the setting of combinatorial problems which are usually
 of discrete nature, and there are many combinations of them.
 So the simplest version of that is when you have two symbols, so it's Boolean, and you
 can combine them, say, with like a polynomial, so a combination, a sum of monomials.
 And so that's what this equation is saying, is that all the solutions that we're going
 to be able to provide are just sums of weighted product of your input values.
 And so in this sum, you have the coefficient, which is the weight you put for each of the
 terms, and then you have how many terms you're multiplying together.
 That's what a polynomial is, nothing dangerous about that.
 And now I recall that if you are in this setting, there is a set of combinations of the input
 that you will not see, that's what we call the unseen.
 And from that set of combinations, you can always write the solution as the right solution,
 f of x, plus an arbitrary function, times the product of all the combinations that will
 happen.
 So in this case, you've never seen the variable x1 equal minus 1.
 So you can always assume that at training time, it's always equal to 1.
 So 1 minus that will always be equal to 0.
 So any delta x from this set of function will fit your training example.
 That's the same idea as I already proposed, good.
 I need to tell you again, what is the degree of a function.
 So if you have a polynomial, you can look at the number of terms that are multiplied
 in each of these terms, and that's the degree.
 So in the first part, you can see that the degree is going to be equal to 2, both of
 them.
 Good.
 I won't go into the details of the degree profile, I think it's not very interesting.
 But there are going to be many solutions that fit your solution, some with lower and some
 with higher degree.
 I won't go into that detail, it's not very interesting.
 But what we showed in this paper, there are two theorems that are interesting, and of
 course I'm not going to do any details about these theorems just to give you the high level
 of them, is that for very simple neural networks, and here I'm saying neural networks that no
 one uses but that we can actually study, here it's a random feature model where the first
 layer is just a random projection, and then you can learn the second layer, and that's
 all you have.
 So a very restricted set of functions.
 In this case, you can actually show what I said, when the model has multiple functions
 that can fit the training set, it will choose the one with the lower degree profile.
 How interesting is it?
 Not that much because that's not a function, that's not a model that anyone is using, but
 it's good that you can prove it.
 Now what we showed now is that it also works for other models, like linear functions and
 diagonal, linear neural networks, but it also works empirically.
 So just using experimental experiments, that it also works for transformers.
 Now transformer is the class of function that everyone is now using to train their LLM.
 So if you've heard of LLM, you've likely heard of transformers, and that's a much
 more rich and complex set of functions, but it turns out that we obtained the same result
 empirically than the ones that we showed on simple functions.
 So again, we showed it on simple class of functions, but it also seemed to work on the
 much more complex class of functions that are actually used for all these bigger LLMs.
 So let me show you the kind of very simple experiments we did.
 So here in this example that you see, there is a target function.
 It has 15 input variables, x0 to x14, but the solution that you're looking for is just
 the product of the first two.
 All the other inputs are not used, but you don't know that, of course.
 And more than that, you never see the two variables, the first two variables, equal
 to minus one together.
 During training, you never see that, but you can see that there's plenty of examples you
 can see because there's all these distractors, x2, x3, they are never used.
 So at training time, again, one minus x0 times one minus x1 is equal to zero.
 So you can have any function that uses that product of term, and all of them would be
 compatible.
 What will happen now when you train a model?
 So on the left is this random feature function that the family of functions that we train.
 The graph that we see, I have to explain it.
 On the x-axis, it's the training time, epox is the training iterations.
 On the y-axis is the actual value of the monomial that we're going to show.
 So let's look, for instance, at the term x0 times x1, it's one of the terms of the function.
 And the value of the coefficient is around zero, as you can see.
 So there's no x0, x1 in the solution.
 Instead, you can see that the term in x0 and the term in x1 goes to one.
 So there's a term in x0 in the solution, there's a term in x1, and there's also a term in one,
 which is actually minus one.
 So the solution that it finds is x0 plus x1 minus one, which is compatible with the training
 set, but it's not the function that we're looking for, again, because that solution
 is of degree one, while the one we are looking for is of degree two.
 Same for the transformer, you get the same thing.
 Curves are slightly different, but it's actually the same solution.
 I'm not going to show, I mean, neural networks have some bias, I won't go into details about
 this.
 Let's see, there are other functions, but I think you got the point.
 Where do we go with that?
 Let me go back to one of the functions I said was hard to learn, and that's the parity function
 under the length generalization concept, where you learn parities with a given number of
 length of your input, and then you're going to test it with longer sequences.
 So it's actually a hard problem, as I said, weirdly.
 The actual solution is just going to be the product of the input values, if they are minus
 ones and ones.
 You can see that you retain the sign, and that's good enough to solve the problem.
 So it's a very simple solution, but of course, it's not going to work that easily.
 In fact, we can show that given any training set you're going to get of a finite length
 that is smaller than the total length, you are going to find a solution with a lower
 degree profile.
 So there will always be a simpler solution, simpler according to the model, and that's
 the one that it will choose.
 So you will never find the right solution, and more than that, you're going to make a
 mistake that is combinatorially big.
 So as the length of the test example is longer and longer compared to the training examples,
 you are going to make a bigger and bigger error in the model that you're going to find.
 So this is pretty bad news, and that's shown by the examples that I showed you where the
 curve goes down very quickly.
 That is explained by this solution in fact.
 I won't go into that, I don't think it's useful.
 So the first part is pretty depressing in a way, no matter how big your model is.
 If the problem you want to solve is this kind of reasoning problem where the number of combinations
 of your input grows and you will never see enough of these combinations at training time,
 you will get into a solution that has the wrong bias, that will go to a solution that
 is simple according to the model but not simple according to you.
 So where do we stand with that?
 I think we need to think more about how we can put in our models some priors that will
 favor things that we humans do naturally, like symmetries, like how we compose things, how
 we come up with simple solutions.
 So these are not always easy to come up with, so forward for us.
 Let me go to the second part.
 If you actually have questions on the first part, it's time to do it.
 There are none on the spectators yet.
 Either they're following you faithfully or they're completely lost.
 Perfect for the first one.
 Okay, well, let's move on to a different study.
 And the question now is, what algorithms can transformers learn?
 So again, transformers are this class of functions that are used at the heart of most modern
 deep learning approaches, including large language models, but many others.
 And so the question now is, what can we learn with that?
 And again, I'm going to go back to this length generalization problem because it's actually
 richer than I thought at first.
 And so this is a paper that we presented at iClear at another machine learning conference
 this spring.
 All right.
 So again, we're going to consider the same class of functions where you have discrete
 symbols and you need to do operations on them, sorting them, adding them, you know, again,
 you can see how in some of them that will be combinatorially different solutions.
 And for each of them, you are going to be able to train them all with the training set
 and then look at what happens on the test set.
 But again, during training, we are going to see examples of a given length and here to
 make it simple for addition, for instance, for additions of three digit number.
 And then at test time, we're going to show it problems of longer length.
 That case here, five digit addition.
 So same setting as the previous paper, and we're going to look at from a different angle.
 So that I already said, and you remember I already showed the graph on the left about
 the parity that was hard, even with all the novel techniques that exist like scratch pad
 and fine tuning and all of that, even with the biggest model, you will struggle to solve
 this length generalization problem in some tasks.
 I showed it on parity on the left, but there's many more problems of this task.
 Here it's another problem called variable assignment.
 No need to go into the details of what it is, but the class of problems that will have
 this length generalization problem is actually bigger than we think.
 So that's not just parity.
 So let me start with a simple example, which we call the counting task.
 So this is like a completely made up task.
 It's not necessarily useful, but it's interesting to study it.
 I give you two integer A and B and your task is to just output all the integers between
 the two.
 So if I give you five and eight, you have to say five, six, seven, eight, then the symbol
 for I'm done.
 Now that sounds easy.
 At training time, I'll show you like as many examples as you want of this A, B pair, but
 the difference between A, B will never be bigger than 60.
 Five and eight, the difference is three, but I will never give you a one and 100 because
 the difference is bigger.
 At test time, I'll show you bigger example, longer.
 So the difference between A and B now will be between 60 and 100.
 Will it succeed?
 So that's the question.
 And the answer is yes.
 So now that there's a weird thing, the parity was not succeeding.
 I'll show you examples of where it was not succeeding, but on this case, it is succeeding.
 So what's different?
 Why it works on some, doesn't work on others.
 That's the question.
 So let's try to understand what a transformer is doing.
 So what a transformer is doing, usually, I won't go into the details of what's in this
 box, but the way it works is that you're shown a sequence of symbols, here you're shown the
 symbol five, eight, greater than, and then maybe five and six, and you have to output
 the next symbol, seven.
 So to output seven, you saw all the previous symbols, and the function that you're looking
 for is a function that if I show you the first five symbols, will output the sixth one, here
 it's seven, but if I show you the first eight symbols, it's going to output the ninth one.
 And I think that's the interest.
 The same function, the same transformer, no change in the function, is able to work on
 the left and on the right with different length, and it works on both.
 So that's what a transformer is doing, at least.
 The question is, if I was to write a program that actually solves this problem, where I'm
 given a sequence of symbols, a variable length, and I have to output the right symbol after
 that, what would be the code that I would write?
 So if I was to write it in Python, that would be the thing on the left.
 You don't need to understand it, but it's a very simple code that does not depend on
 the length of the sequence.
 It just says, "Well, I'm going to look at the last symbol.
 If it's this, I do that.
 If it's this, I do this."
 It's very, it's like five lines of code, and you can apply this program, and it will always
 work for any symbols, for any input that you showed it.
 So that's cool.
 We knew that the transformer is going to learn that program, then we know the transformer
 will work for any sequence, because that program doesn't care about the length.
 It just cares, it just applies the same formula, no matter how long the sequence is.
 So that's good news.
 There are solutions that work no matter how long the sequence as input is.
 The question is whether a transformer can actually be learned that will exactly encode
 this thing.
 So can the transformer represent this program in particular?
 Because if it cannot represent this program, there's no way it can learn it, and if it
 cannot learn it, it's not going to solve the problem you want.
 So first question is, is there a way to change the parameters of the transformer such that
 it actually encodes this function?
 If it does, cool.
 So we came up with a conjecture that if indeed, you can show that there's a program that can
 be implemented that represents the function you want, and it can be implemented in Python,
 and it's a simple program, and you can show that the transformer can actually learn it,
 then it will learn it.
 Now, it's actually not in Python.
 It will have to be in a very simple language, and we actually found some other people from
 about three years ago who wrote this language, they call RASP, and it's a simplified version
 of Python.
 It's a set of Python with a much simpler set of functions that you can use.
 In particular, there's no loop.
 It has to be, you see the whole input, and you have to apply a simple function to all
 of the input at the same time, and things like that.
 So it's a simple language, simplified language, and it basically looks like that.
 On the left, you have to think of the transformer, which has layers, and for each layer, it takes
 the input, and it transforms it into the output, which becomes the input of the next layer.
 That's basically what any neural network is doing, and the idea of this language, RASP,
 is that for each of the layers, it will apply one of the formulas of the language.
 So each layer of the transformer is equivalent to one line of code of this simplified language,
 and if you can show that you can implement that such line of code in the parameters of
 that layer of the transformer, then you can indeed implement the program that you want
 to implement in total.
 So we simplified even this RASP because it was too expressive.
 We made it simpler, and we call this RASP L so that it's something that we think can
 be learned because it's simpler, and then we thought, okay, so we think that if there is
 a solution in this RASP L language to implement the problem that you want the model to solve,
 then the transformer will actually learn it, and it will generalize to any length because
 it actually learned the right program.
 So I said that the problem of this language is that it cannot implement anything.
 In particular, it cannot implement parity, and we said actually parity is hard, so it's
 kind of a good news.
 It cannot implement parity, and we know it's impossible.
 So it goes into our intuition, and in fact, that's our conjecture.
 If you can implement the problem you want at hand with this very simplified language,
 then the transformer will learn it, and it will work and generalize to any length.
 So we have not proven this thing.
 So it's only the conjecture, but we tried it apparently on many, many tasks, and here
 I'll just give you a simplified version of that.
 So here are just like seven tasks, counting, the mode, copying in some way, sorting, adding,
 parity.
 And for each of them, we actually tried to write the code to see whether we could actually
 solve it.
 And the left are all the simplified tasks for which we were able to write the code in
 this RASP L language, and on the right are all those for which we were not able to write
 the code for this language.
 And then, well, we tried it.
 We generated as many examples as we wanted on the tasks on the left and on the tasks
 on the right.
 And for each of them, we did the same idea.
 We trained on problems of a given length at most, here 40, and then we tested on problems
 of a longer length, here 50, and we looked at the results.
 Should that be RASP Lable?
 So that's the, sorry, oh yeah, it's RASP Lable, yes, totally, yes.
 And here are the results.
 It kind of agrees with our conjecture, all the problems for which we found the code that
 could be written in this RASP L language, the test on longer length provided 100% accuracy.
 So it actually learned the function, no matter what the length was.
 All those for which we didn't find a RASP L implementation, including parity, it failed.
 So for parity, the answer is 50%, because it's a coin flip, it has only two options.
 But for bigger problems, like addition, it actually fails completely.
 So that's kind of agree with the, so we cannot prove that this will always be the case, but
 we have a strong intuition that this will be the case.
 Actually, in a different paper, we showed that it is going to be the case in some restricted
 sense, but I'm not going to go into the details.
 So transformers, there's this class of functions that is very rich, but nevertheless cannot
 implement anything.
 Or, I mean, it can, but it might not implement the things we want.
 In particular, it might not generalize to longer length, if indeed you cannot represent the
 functions that you're looking for in some very simplified language.
 So I think with that, I'm done.
 Some questions are already coming in, but first we'll have Gary.
 Gary, we'll be speaking tomorrow, but he's going to discuss it today.
 Do you want to come around this side so they see you?
 That might be a should just.
 Yeah, I think so.
 a little bit.
 Okay, all right.
 So I love yourself.
 Okay.
 So that works.
 All right, great.
 Okay, thank you, Simon for that talk.
 And thanks for sending me your slides last night.
 I wanted to touch on two points from your talk.
 And I'm going to focus on some human data that you and others in the in-person virtual
 audience may find interesting.
 So I find this work fascinating and my training is, as a cognitive scientist, as a connectionist
 cognitive scientist, for most of my career, I've worked with people with running experiments
 of various kinds on people, specifically looking at the role of natural language in augmenting
 human intelligence.
 And so what strikes me looking at these points, the question kind of naturally arises to me
 is, to what extent is this also true for people?
 To what extent do people need to be nudged in this direction?
 Right.
 So do people need similar nudges, or is this a core part or property of human reasoning?
 Do we just come with this by default?
 So at the limit, we can give a talk on this topic, we can clearly think in this way.
 But is this kind of a basic part of how human reasoning works, or does it require certain
 kinds of augmentation?
 And then what determines whether people represent the sort of true algorithm, where true algorithm
 is, in these cases, the sort of ground truth designed by the experimenter, by the problem
 designer.
 Okay.
 So this work is not by me the next few experiments, stuff that we did in my lab.
 I love talking about this because it's such a fascinating, it has a fascinating intellectual
 history and the data are also quite compelling.
 So this is a review paper that summarizes a lot of empirical work by Adwaserman and his
 collaborators.
 And Ed started this project by working with animals, particularly pigeons, and he was
 interested in the ability of pigeons to learn this type of distinction, which is what was
 thought since at least William James to be the core of human categorization, the distinction
 between sameness and difference.
 And so in a typical experiment, pigeons would be exposed to displays like this.
 He started this work in the 80s.
 These are the old Mac dingbats, icons, of course, they're meaningless to pigeons.
 So a pigeon would be presented with a display like this and would be reinforced for picking
 the correct button, in this case, the button that means same, meaning that all the items
 in the display are the same and would be reinforced for picking a different button for a display
 like this, which is an instance of difference.
 And the question is, how do they generalize to other instances of sameness and difference,
 both with other visual icons, but also different amounts of difference, right?
 So same here means everything is the same, different is just everything else.
 Okay.
 And so you can see how pigeons, after being trained with this, these kinds of displays
 generalize to these.
 You can quantify the amount of sameness in terms of visual entropy.
 And what you find is that pigeons are sensitive to sameness, but as you can see, it depends
 on the degree.
 So what they do is they generalize based on how much sameness there is.
 And clearly, clearly people wouldn't do anything like that.
 And so at some point in the late 90s, Ed started running these experiments with people.
 And the pattern he and his collaborators kept finding over and over is the following.
 So if you tell people what the rule is, of course they can do it, oh, same versus different.
 But you don't tell people because you want to do it in the same way you do it with pigeons.
 So you reinforce people in the same way that you do it with pigeons and you see what they've
 learned.
 And what you find is that most people, it ranges from about 60 to 80% across experiments.
 It does matter, for example, whether the individual visual icons are meaningful or are they just
 random shapes.
 But you always get this interesting dichotomous split where most people quite quickly kind
 of figure out, oh, it's just same versus different.
 And then they generalize in this dichotomous way.
 So same is one thing and anything else is different.
 But a minority, 30% to 20% to 40%, do what the pigeons are doing.
 This despite several decades of work, interestingly, it's not only this type of task that has this
 distinction, but it's not anything obvious.
 It's not like they're just being lazy or they somehow have lower intelligence, whatever
 that means.
 They're not different in any obvious ways.
 Their grades aren't different, their GPA isn't like SAT scores, standardized stuff.
 It's not anything of the sort.
 And even people who show this categorical distinction between sameness and difference,
 if you look at their reaction times, they are sensitive to entropy.
 It's not that they're not.
 It's just they one way of thinking about it is they override it.
 So moving on, something that we've done a lot in my lab is think about what kinds of
 tasks become hard when people are partially deprived of language, when language is interfered
 with, various ways to get at the question of does performance in various tasks somehow
 recruit language or depend on language?
 And I saw that Sammy had this same difference in the talk, and we happen to have some data
 that speaks to this.
 And so some years ago, some collaborators, Martin Zetterstein was a grad student in my
 lab, and Jing Paul was a collaborator that gave us, through whom we got access to this
 interesting population that I'll describe in a moment, of children who were born deaf,
 they were living in China in a special boarding school for the deaf.
 Until they came to the school, they were not exposed, as tragically is still the case for
 most people born deaf, they were not exposed to a full natural language.
 They weren't exposed to sign language because no one around them knew any sign.
 And they weren't exposed to spoken language because they're deaf.
 And so these are kids ranging in age who have varying amounts of, after coming to the school,
 of exposure to both Chinese sign language and various other methods.
 They use all kinds of mix of pedagogy, such as lip reading, and of course they're also
 exposed to written Chinese.
 So we ran a task of the kind that relational matched to sample task is called the type
 that's often been used with kids.
 So kids see a display like this, where they have to pick which one matches the top display.
 And at first they're guessing, but they're getting feedback that's telling them that
 it's about the relation.
 So same goes with same and different goes with different.
 Okay, so what do we find?
 So that's the sample and see it ranges from age seven to 20.
 And it also ranges on the y-axis, the age at which the children were first exposed to
 Chinese sign language.
 What we find is that performance on this task correlates with age, not with their chronological
 age, but with age at which they were exposed to a natural language.
 And if you look at specifically whether they know any symbolic representation, whether
 written spoken signed of terms same and different, that's a strong predictor of how well they
 do on this task.
 All right, one more example.
 So this from Sammy's talk is an example of a Ravens matrix, now more generally called
 matrix problem.
 It's a staple of what are often called fluid reasoning tasks or also called non-verbal
 reasoning tasks.
 And in the literature, people often assume these tasks are what they're called non-verbal.
 Why is it non-verbal?
 Well, because you don't need language to, it was originally called non-verbal because
 unlike other parts of IQ tests, which this is a part of, the task itself is not delivered
 linguistically.
 So you don't have to know to read, you also the stimuli are not words, unlike many other
 IQ tests.
 So that's the reason it was originally called non-verbal, but then people kind of assumed
 that, well, it's non-verbal, it doesn't involve words, so it has nothing to do with language.
 It doesn't depend on language.
 So in series of studies, we looked at this by looking at the role of nameability on people's
 ability to solve matrix problems.
 So these matrix problems vary along multiple dimensions.
 One dimension is how complicated is the underlying rule?
 And the more complicated the rule, the more steps it takes to solve, the harder the problem
 is, but they also vary incidentally in other ways.
 For example, how difficult it is to describe the individual items.
 So you can ask people to describe, for example, describe the cell so that it can be distinguished
 from other cells, and then you can evaluate from simple to more complex measures, how
 much language does it take?
 So for example, if the difference, yeah, so here's an example of a more nameable problem.
 Here's an example of a less nameable problem.
 And we can look at the role of nameability controlling for other factors.
 And what we find, these are problems that we designed, same logic, where we can keep
 the logic the same.
 This is the better illustration of it.
 So here the logic is the same.
 It's just that these ones use familiar and easy to name shapes, stars and circles and
 squares, and these are arguably equally discriminable, but harder to name shapes.
 And these displays are on the screen the whole time, so they don't require remembering these
 shapes.
 Controls for visual complexity, discriminability.
 And what we find is that the number of rules plays a role, the more rules, the lower the
 accuracy.
 But controlling for it, nameability plays a significant role.
 It's not a huge effect, but the more nameable, the easier.
 Why?
 Because we argue it makes it easier for people to pose and test hypotheses.
 So is it about the square?
 Is it about adding a square and a circle?
 And if you ask people to reason out loud as they do the task, this is exactly the kind
 of thing they report doing.
 This is made harder when you use harder to name shapes, even though the ground truth
 logic remains the same.
 And lastly, I find this really cool, and it again raises the question of whether there's
 any psychological reality to this distinction.
 So not just, oh, are they the ones on the right harder for people, because there are
 all sorts of reasons why they might be harder.
 But do they require other kinds of things, like are they more dependent on the amount
 of formal education the participants have?
 Are they more sensitive to being exposed to particular kind of cognitive chunks, right,
 that people can sort of retrieve and recombine on the fly rather than having to reinvent
 during the course of solving the problem, which is hugely more difficult.
 And while it might be achievable for some people, it might not be for most.
 So that's, that's all I have.
 Thanks.
 Terrific.
 The questions that are on the other end, I'll let you decline, or do you want to respond
 to him first, or do you?
 Fascinating.
 I must say, thanks for doing that.
 I'm not sure what I can say, but I knew there were a lot of experiments done, but I didn't
 really study them.
 So I think it is definitely a different angle and a very interesting one.
 I wonder if, ultimately, what I want is to understand what are the biases that humans
 have that help us solve these problems that we could put into machines.
 And I think you kind of asked the same questions from an empirical study that I think is super
 interesting.
 I'd really like to have your slides at the end.
 Okay, so let me read the questions from Catherine.
 It seems as if you're saying that the way people simplify things is very different from
 how LLM simplifies things.
 So that's why LLMs have trouble generalizing.
 Yes.
 And when I say trouble generalizing, they do generalize, they just don't generalize
 the way we do.
 Again, it's more about their priors, and our priors are different, and in a way, they are
 generalizing in some ways.
 It's just not appropriate.
 The problem when the kind of problems you look at is under-specified, which is always
 going to be the case for these reasoning problems, the only way to distinguish two solutions
 that are compatible with the train set is to have a prior.
 And the prior that we have in the LLM seems to be different from the prior that we have
 in the UBITS.
 Janay asks, "Why LLMs learn grammar, but not some other sibling groups?
 How do these problems in training data differ?"
 I don't know, maybe someone has a better answer than me to that question.
 I don't see from my vantage point whether there's something specific with grammar.
 I think the main problem, I think if everything was specified in training examples, I think
 it would work well.
 So that's not the problem.
 It's not whether it is hard to learn most of these LLMs, and I can learn by heart all the
 training examples you're going to see.
 So they can really understand all the training examples.
 But then they have to make up their mind about the new example.
 So if they learn, it's like think about grammars, there might be more than one grammar that
 actually embodies the training examples you've seen.
 Which one it is going to learn, you don't know.
 So that's the main issue.
 It's not that they can or cannot learn, they do, but they will have a different prior than
 we do.
 Daviday asks, "Can we say that there exists a set of non-computable functions essential
 for human cognition and the human capacity for reasoning and understanding?
 Is it possible that the mathematics behind large language models cannot reach the same
 level of reasoning as a human because there are experiences and capacities that, even
 if they can be formalized, cannot be implemented into a computable function?"
 I would not be as harsh as that.
 I think these models, they can learn basically anything, given the size of these models.
 Do you want to say something?
 Okay, so again, I don't think they cannot learn it.
 I think they might learn it in a way differently than we do because they start with different
 priors.
 I don't think there's anything wrong about being able to do mathematics or reasoning
 with LLMs.
 It's more our learning algorithms that should probably be thought again.
 And in fact, let me just give you one recent result from a paper we just submitted where
 we use an LLM again to solve the problem.
 Now, these hard problems, say, let's talk about parity or let's talk about any problem
 that is hard.
 As I said, there are multiple solutions to solve it, and what an LLM does is produce
 one token, then given that token and the rest, produce a second token, and it produces a
 sequence that, at the end of the day, solves the problem.
 And I talked about these techniques that people use.
 It's called the scratch pad.
 It's a way to say, "Well, don't answer me right away.
 Don't tell me whether it's parity, yes or no.
 Show me your work."
 Try to decompose the problem in a way that will then get to the solution, rather than
 just give me the solution.
 And these usually succeed far longer, so they can solve problems of longer length, much
 more than if you don't ask the model to try to decompose the problem, to try to reason
 about the problem.
 We've seen many papers in the literature, they have multiple names, but they all find
 this fascinating thing that the LLM, if you ask a question about anything and you just
 say, "Please answer by yes or no, or please just give us the answer," it's going to
 fail most of the time.
 But if you instead push it to reason, to decompose the problem, it will work much more.
 And we actually showed in the paper why this was the case.
 You can show that hard problems, if they are decomposed, each of the composing functions
 of the decomposition, if each of these functions are simpler, then actually the complexity
 of the whole problem becomes just the max of this small decomposition.
 So suddenly a problem that was very hard is now just the sum or the max of the sequence
 of small simpler problems.
 And so you've decomposed a hard problem into just simpler problems.
 You can show that if the model has seen that kind of decomposition in the past, it will
 do it again, and that will suddenly solve these hard problems.
 So there's a path.
 The only thing that we haven't solved is how do we instill this process of learning to
 decompose things.
 So when you showed it, when you show it with some examples, you say, "Well, if you want
 to solve parity problems, do this like this," your hand held the model, it will learn it
 and it will solve it.
 So that's the good news.
 And it can solve this hard problem.
 For now, you either need to have shown it before how to decompose or you need to show
 it for every single example, which is not practical.
 So we're not there yet, but we have a proof of concept and we have a proof that if you
 know how to decompose, you can decompose.
 So that's the answer.
 - So I wanted to employ- - No, come on, come here so we can...
 I wanted to refer to an old concept, which is delta learning or second degree learning.
 For what I see, isn't it the problem that we don't have a way to teach these language
 models a way of learning?
 So teach them how to learn search and this is the crux of the matter in the end.
 - It's totally the crux of the matter.
 And in fact, we do with these techniques where instead of just telling them to learn if this
 is the input, this is the output.
 If instead you say, "This is the input and here are the steps to solve it," it will indeed
 learn the steps.
 So it will learn it.
 Now it will learn it only for problems where it's the same set of steps to solve it.
 If suddenly it's a different combinations of steps, you're still back to the problem
 and that's where we are now struggling, of course, so there's more work to be done.
 But you're right, models can learn to decompose if you show them how to.
 - But they can't generalize- - Well, they will generalize to all the similar
 problems of decomposition, not a different kind of decomposition.
 So it's one step in the right direction, but it's still not there.
 All right, let me move on to anonymous.
 Anonymous asks, "In the generalization of the unseen work, would pre-training on higher
 degree function for some problems allow the model to learn higher degree Boolean functions
 on other problems?"
 I think it's a very interesting questions.
 I think we did some experiments, trying to remember.
 I think it does help, but not for higher degree, though, for similar degree function, though.
 Oh, okay.
 "If you pre-train on higher degree, will it work on higher degree?"
 Yes, the answer is it will be easier to learn, as usual, and that's compatible with what
 I just said.
 If you pre-train with similar problems, it will be more easy to solve a problem of the
 same category, but not a longer one, a higher degree one.
 Question for Sami, again, from anonymous, about RASP, "Would access to a scratchpad
 memory allow the transformer to represent more complex functions?"
 I just answered that question from the previous question.
 Yes, scratchpads are very powerful mechanism, which gives more memory to the model, instead
 of answering right away.
 It can do partial work and use that partial work to, in the end, answer the question,
 but it's not any kind of scratchpad.
 There are many ways you can implement scratchpad.
 One of them is just to say, use as many tokens you want to answer.
 It's what we call an uninformative scratchpad, where the only thing you say is you can use
 like a blank sheet of paper and write whatever you want, and then answer the question.
 That doesn't work.
 If instead you give a scratchpad with instruction, do this, then do that, and you will find the
 solution, and then the model can actually decompose the problem by doing this and then that, it
 will find the solution.
 So a scratchpad as such, which is just the ability to have more compute power because
 you can compute more intermediate symbols before you get to the right symbol, that's
 not enough.
 But if it's compute power with guidance about what to do with that compute power, then that
 works.
 And depending on the guidance, it will work.
 And we showed multiple ways of doing this guidance.
 I won't go into the details, but it can be pretty good.
 For instance, if the solution you want is to apply some recurrence to the same decomposition
 multiple times, if you just show the recurrence, it will actually learn it.
 So it can be very compact in the scratchpad.
 And there's a question here.
 So, I'm just wondering about the emergent properties, do you think they really exist?
 And if they don't, how does, you know, relate to the graphs tell, you know, I don't think
 they exist.
 Okay.
 No, I mean, it's more about how you talk about them.
 So there were a couple of papers trying to explain this emergent.
 So backtrack, what is emergent properties?
 People have observed that as you train bigger and bigger models, you know, from a hundred
 million parameters to one billion parameters to 10 billion parameters to a hundred billion
 parameters, on just what LLM is trained on, which is predicting the next token on a huge
 corpora of text.
 As you do that, you can see that it's able to solve new tasks that you didn't think it
 would be able to solve.
 So you train it on just predicting the next token, but suddenly it's able to solve a translation
 task from the English to French, suddenly later, it's able to solve a summarization
 task.
 And these things you've never trained to do it.
 So that's the emergent property.
 There are people who have like written a long list of these tasks and tried to see what
 happens.
 And indeed, as the bigger models become bigger, they are able to solve more of these tasks.
 People were puzzled.
 Now, what they were puzzled on is the fact that suddenly at a given point, we are able
 to solve a task that right before they were not able.
 So it just suddenly emerged before that it was not able.
 Now it is able.
 So with a hundred million, it's not able to solve it.
 With a billion, it is able to solve it.
 Now, the truth is that it's not like this, that it works.
 It's actually all these tasks, you can measure their performance as you train bigger and
 bigger model.
 And they are going to be able to solve, to reach a solution that is more and more near
 the right solution.
 So suppose that you're asking a simple task, is the sum of these two numbers bigger than
 a number.
 You can, instead of answering yes or no, you can say, say the probability that it's yes
 or and that probability will grow slowly to the actual threshold of being more than 50%.
 So if you look at the probability, instead of whether it's bigger than threshold, then
 that probability always grows.
 So as the models are trained on more and more data, they are just able indeed to solve more
 and more of these tasks, more likely because there's more and more examples in the training
 set that are similar to the questions you're asking.
 Because as you train on a larger corpus of data, there's one example in that corpus that
 is similar to the question you just asked.
 And it's near and near.
 So as you go, it is getting nearer to solve the problem.
 But if your question is yes or no, it looks like suddenly it's yes and the day before
 it was no just because you just crossed a threshold.
 It's a non-linearity that is arbitrary, that doesn't mean anything.
 The model is just slightly better than yesterday, or slightly better than the smaller model,
 just because we're trained with slightly more data.
 As you train with more and more data, you are more and more near any question you're
 going to ask.
 And that's the real explanation of what's going on.
 Okay, so no NFS improper.
 That's in the sense of suddenly it's better.
 Yes.
 Okay, that was for the question that was asked about the scratchpad.
 If you have access to scratchpad memory, will you be able to represent more complex functions?
 And as I said, depending on the kind of scratchpad, the answer is yes.
 But for just the memory, the answer is no.
 So if you just say you can have this blank sheet of paper, can you do something with
 it?
 It's not going to help.
 Sarah is saying, I asked CiaoGPT4 about parity.
 At first it said wrongly and then when I said it's wrong, it wrote the Python code for it
 and answered correctly.
 That's good.
 Yes, that's true.
 Do you think that it will be possible that they just start coding for everything?
 Also isn't coding understanding the problem?
 Does it mean that they understand the question and just don't know how to do that?
 So that's a very interesting thing.
 First, there are many things to say about this.
 First of all, parity is definitely a problem that exists.
 I didn't make that problem, it exists.
 You can find code about it.
 You can find people talking about parity on the internet and so GPT4 has been trained
 on parity.
 It's not something new for it.
 It has seen numerous examples of code writing parity problems.
 It's not something new.
 So can it write code about parity?
 Yes, because it has hundreds of code versions of parity that exist.
 So it's very easy for it to write code for this problem because it saw it already.
 There's nothing new about that problem.
 The real problem now is to think about the problem that is not on the internet and for
 which you can ask the question.
 So parity here would be much too easy because everybody talks about parity on the internet.
 So it's very well known.
 And these large language models are trained on everything they find on the internet.
 So again, they learned everything by heart.
 Coding parity is actually a very simple program.
 You can see it in many ways.
 So did it actually understand parity?
 I don't think so.
 It understood that what you are looking for is the program for parity and it can show
 it to you.
 That's a very different question.
 And people have tried this kind of trick, for instance.
 So there's this colleague was telling me about this, the Monte-Hall problem.
 I don't know if you know this.
 It's a very well known problem.
 I'll explain it.
 Okay.
 So the Monte-Hall problem is the following problem.
 We have time?
 Yes.
 Yes.
 All right.
 You're confronted to a problem where there are three doors and you have to pick one of
 them.
 One of them has, I don't know, a big brand new car behind it.
 And two others are going to die or something bad happens.
 And so I'm asking you, okay, which of the doors you will pick.
 You pick one.
 You have no prior.
 There's no reason you would pick any other one.
 And then that's the then that is interesting.
 I open one of the two other doors.
 So you pick the first one and I open the second one and I show you it's not behind that second
 door.
 You have to say importantly, you know where it is and you will not open a door where it
 is.
 Where it is.
 Yes.
 Right.
 Of course.
 I didn't do it randomly.
 I know where it is.
 You asked me to open the first one.
 I said, well, wait a minute.
 I'm going to show you.
 It's not behind the second one.
 And then I ask you, do you want to change your mind?
 You want to stick to the first one or go to the third one, given that I opened the second
 one.
 You wouldn't.
 Okay.
 So that's a that's an interesting conditional probability problem that everybody studies
 at school and everybody knows how to solve it.
 We're not going to go into the details.
 If you ask CHUDGPT to solve it, it's going to solve it like hands down.
 No problem.
 Why?
 Because it's on internet.
 Because you can see numerous versions of this.
 Now if you change the question slightly, for instance, indeed, you know where the thing
 is.
 So suppose it's behind door two, you say door one.
 Now I know where it is.
 I am now going to tell you I opened the second door and the car is behind it.
 Do you want to change your mind?
 And it will fail the problem even though I told the model here is the solution.
 It will fail it because the problem is never asked like that, of course, because it would
 be a dumb problem.
 So it learned the Monteho as a hard problem.
 But that's this unique hard problem.
 Any problem around it, it has not seen it.
 Any transformation of that problem, it has not seen it and it's going to failing.
 You didn't understand the problem?
 I don't think so.
 How would your method cure it?
 No, no, no.
 I'm not saying I cure everything.
 No, no, no.
 It's the one that worked.
 You have to train.
 Oh, if you're trying to do it, then it might make it easier, yes.
 So it's likely that there's a scratch pad, an informed scratch pad that can help to solve
 it by saying, well, you know that when you choose the first one, you had a priority of
 one over three, think about what's the priority of the second one and then answer the question,
 something like that.
 Relax.
 This is going so well that I'm going to use executive to continue the discussion to almost
 the next speaker.
 So do as many as these.
 All right, let's move on to Armin.
 Armin is asking, I didn't understand how you determine that the RASP language is one of
 the transformers should be able to implement in order to solve the problem.
 So we, as I said, we actually don't have a proof that this is the case.
 It's only a conjecture that if there is a RASP solution to the problem, then the transformer
 will learn it.
 We only did it experimentally.
 We don't have any way to prove that that's the case, unfortunately.
 And what we know is that, well, this RASP is interesting is that this RASP language, if
 you can write the code in this RASP language, you can actually find the parameters of your
 transformer that implements that code.
 So it's not just that you can learn it, you can show, you can precisely write the parameters
 of the transformer that implements that.
 That we know that was proven like in 2021.
 What our conjecture is, not only it exists, but it's what it is learned.
 And that's what we showed you, frankly.
 Theo is saying, regarding bias and priors, could you get them by real world experience?
 Why try to solve something in theory if it may turn out that you can only get your answer
 by confronting the world?
 Try and narrow learning in the real world.
 Do you consider solutions of that kind?
 So that's the grounding question.
 You said, I hope you're going to talk about grounding.
 I didn't, but Theo did.
 Yes.
 There's a lot of people saying that you will never be able to solve interesting problems
 of interest to humans if your model is not grounded in the real world.
 Grounded just means in this case that it's going to be exposed to the world in many ways
 similar to what we are.
 So we experience the world through senses, vision, touches, smell, whatever your senses
 are.
 And these LLMs, they experience the world through text only.
 They don't have any vision, they don't have any other sense.
 They don't see an apple falling in the tree.
 They don't see someone coming to you.
 They don't experience the world like you.
 And the question is, maybe if they were grounded, maybe if they were seeing the world or experiencing
 the world as we do, they would get these things.
 It's possible.
 Definitely, there's a lot of people interested in that area.
 There's a lot of, for instance, there's a lot of work in these vision language models.
 So it's already adding one dimension, but it's still not like an object that would be put
 in the middle of a classroom and then would wander around and ask questions, touch the
 thing by itself.
 I think we're not there yet, but there's good reasons to think that it's a good way to go.
 It's just not efficient.
 Train these models on billions of learning examples very efficiently.
 So these LLMs are trained on data that you will never see in your whole life.
 So quantity of data that is just crazy.
 If we had a machine that can experience the world, it would be as slow as we are all.
 So it might not be efficient to do these experiments.
 So hopefully we find a way to be more efficient, but today that's the reason why branding.
 As slow as we are from zero, but not from today.
 Absolutely.
 Yes.
 But if we had to start from scratch, that's hard.
 Is it possible, Ali is asking, is it possible to minimize the LLM biases 100%?
 I am not sure I understand that question.
 It is possible that the LLM learns by heart everything.
 So if that was the question, yes, LLMs have biases.
 We all have biases.
 Every machine, every learning machine has a bias.
 It is used to complement the training example that it sees.
 For humans, it might be based on the genetic or your whatever genes you have or any approach
 like that.
 For machines, it can be priors that come from the learning algorithm.
 Maybe it will have some kind of reducing the weights to zero or something like that.
 Some prior that says if you have two functions, which one would you choose?
 So all models that learn, all machines that learn have priors.
 And it's good.
 The question is to find the right ones, not to remove them.
 Anna asks maybe a naive question.
 Certainly not.
 If you post treat a transformer with reinforcement learning, as it is often done, is it conceivable
 that it changes its original neural architecture so that it becomes more similar to the human
 way of generalizing and you can make it forget its original priors?
 So I will answer the last part, make it forget its original priors.
 Yes, learning is always going to be a gentle combination of prior and posterior.
 Prior is everything you see the world before you see any example.
 Posterior is how you will change your parameters given what you've seen in the world.
 And so you have both, you have priors and posteriors.
 Every time you see a new example that tells you how to change your parameters, you're
 slightly changing your biases from all of the prior to more of the posterior.
 So now she's asking about the techniques that are called like human feedback reinforcement
 learning or reinforcement learning with human feedback, or HF, where after you've trained
 these LLMs, you show the LLM examples of the form, for this question, humans prefer that
 answer to this answer.
 And you will show a lot of these kinds of triplets.
 For this question, humans prefer answer A to answer B. That's the human feedback.
 And you use reinforcement learning to train that, no reason to go into that detail.
 It's hard, but it is not that good.
 It is going to be very important.
 It will try to do the same thing as those humans for these questions and questions similar
 to those, but it's going to be very narrow.
 So for any other question that is so far from the questions you showed it, it will probably
 not work well.
 It will do what we call overfitting.
 We'll be very good on all the pairs of examples you showed it during that part of training.
 So that's not very satisfying.
 We haven't found a good way to do RLHF in a way that generalizes to other very far away
 pairs of examples.
 So it does remove the priors, but only erode these questions that were shown during RLHF.
 Ali is asking again, that's the one who asked whether the biases are minimized 100%.
 He says is LLM, if LLM bias minimized, can LLM reason like humans?
 I think the answer is the same.
 It's not that they should be minimized.
 I think they should be changed.
 We should try to find biases, priors, that are more aligned with what humans do.
 And in a way, that's what people were trying with these RLHF.
 People often call that the alignment problem where they say, well, there are many answers
 to questions.
 We want the model to answer something that is aligned with the values of humans.
 Do good, don't kill, whatever.
 And that's what we try to teach the model with these post-training approaches.
 So it's not like you're minimizing the bias, you're changing them towards what we humans
 as a community would prefer a model to say.
 But it's not working as well as we'd like.
 And also maybe your values are different than their values.
 And that's why I'm trying a difficult place.
 Vishnu is asking, how the trained data is stored internally, whether it is algorithmically
 or in the form of binary bits or any other representation?
 So an LLM is, as I said, a transformer.
 A transformer is a very specific kind of neural network, and a neural network is basically
 only meant to be a collection of weights and biases or parameters.
 And these parameters are real valued numbers.
 And usually they are the form, you do the weighted sum of terms, and you pass it through
 some non-linearity if it's higher than some number, and this happens or that happens.
 That's the only thing that is in a neural network.
 So they are stored internally in terms of real numbers, because it's a machine, not really
 real.
 It's quantized in a way that could be each number is represented in 16 bits or 32 bits
 or 8 bits, depending on the kind of memory you have.
 But there is a representation.
 It's more of a computer science question.
 So at the end of the day, you're asking, are they binary bits?
 Yes.
 And at the end of the day, in a computer, everything is binary.
 Everything is bits.
 There's nothing else.
 But it's not a very satisfying question, but sorry, it sounds like that.
 But with binary representation, you can represent a lot of things.
 So it's not too much of a concern.
 Matthew is asking a broader question.
 Numbers are often used as a reference point for understanding generalization in AI/ML
 models.
 Do you think the field might be overestimating the degree of which humans are novel in their
 generalization?
 Well, the problem is what we want to do with these LLMs, I think.
 It's more that when people think of these LLMs, they actually have -- humans have priors about
 what they expect from these machines.
 They expect these machines to answer as humans.
 And they are amazed when they do, and they are scandalized when they don't.
 But it's the expectations of the humans.
 The model just learns to predict the next token.
 There's nothing -- there's no magic here.
 The model is only predicting the next token with some biases.
 And with humans, we may be in trouble with more files, and we say, oh, these machines
 is generalizing, is a human, is intelligent, use the term you want.
 It's only predicting the next token.
 There's no magic behind it.
 Yes, Gary?
 So I think that's a really important question to ask and to -- there are many -- a Twitter
 thread where people are talking past one another because one person is thinking of LLMs as
 a sort of production system that should be -- could have sound reasoning and provably
 correct and another person is interested in the kinds of things that don't have ground
 truth.
 Here's an open-ended question.
 What I care about is whether it gives me a satisfying answer that's not obviously false
 but like who's to judge what's true.
 And those are very, very different kinds of tasks.
 And there are lots of domains where when we interact with machines, we don't want them
 to be human-like.
 If you want to turn on the windshield wipers in your car, you want to push a lever and
 have it turn on.
 You don't want a language system that is trying to interpret what you might mean in that context
 and asks you, can you say -- like the way a person would, right?
 Might not hear you right, ask you to repeat yourself.
 And so I think, yeah, as a field, we all need to be clearer about what the use case is.
 Are we thinking of it as a cognitive model?
 Are we thinking of it as a way of better understanding human function?
 Or do we just care about deploying it and having it be a reliable tool?
 >> Yeah, I agree with you.
 I think there's a wide variety of what people want from these models.
 What people expect, what people think they are doing.
 They are only doing one thing, and it's predicting the next token.
 The question is whether it's useful, whether we can use that prediction of the next token
 to solve an interesting task.
 So that's my view of just a computer scientist, I want to solve the task, can this thing solve
 the task?
 That's what I'm asking.
 Of course, plenty of other people have very different questions.
 People may want to say, well, this looks like it reasons like a human.
 Maybe it can help us understanding how humans reason.
 And you know, maybe, I don't know, it's an interesting question.
 That's not the one I've been trying to answer, but there's plenty of interesting questions
 like that.
 But as long as we all understand that this is not a human, it is just a machine, and
 we pretty much understand how it is trained.
 We don't necessarily understand to which extent it will be working well on new problems, and
 that's the questions we're trying to answer.
 But a lot of people see more than what they should.
 They want to see things that they haven't seen.
 They want to think that it's intelligent, but it's not being defined.
 And I think we have to be very careful with this.
 It is debates about that, and people are concerned about things that they shouldn't, definitely,
 but they're also concerned about things that they shouldn't, just because they don't understand
 what's happening.
 So lots of debate.
 Are you exhausted?
 No, never.
 Anna is asking, "Can you hypothesize from an output of an LLM whether the performance
 is an effect of the supervised learning, the pre-training, or an effect of later tweaking
 methods?"
 It's not easy to answer.
 So these LLMs are trained in multiple stages.
 The first stage is just predicting the next token.
 The second one could be to do the same thing, but for a very narrow task.
 So you want to be good on that task.
 That's called fine-tuning.
 And then another one can be what we call the RNHF, trying to say, "Well, if you have to
 answer and there are many options, pick this one rather than that one."
 So that's the alignment with values or with humans.
 At the end of the day, do you know when the model answers whether it is coming from the
 pre-training phase, from the fine-tuning phase for the RNHF?
 I don't think we have studied that as far as I know.
 So I don't think we know.
 One thing that I would like people to study more is more related to this notion of hallucinations.
 You've seen that people use this term in the common language that these models are hallucinating.
 And what they mean is that the model is basically saying false things.
 But it seems to be more trendy to see hallucinating than just saying it's false.
 And the question is, can we detect whether a model is saying something false?
 That would be very useful.
 And more generally, can we calibrate what the model is saying?
 And by calibrate, I mean when it says something, can you compute the probability that what
 it said was true?
 And if that probability is high, you can believe it.
 If it's low, you may want to set it aside.
 And that would help you being able to combine two things, for instance, two things that
 have low probability.
 When you multiply them, it's going to be false with high probability.
 But we don't know how to do that well.
 That's an open question.
 And that would be very useful if we knew how to.
 Stefan says, you seem to imply that the LLMs don't generalize, but rather do a similar
 association with nearby examples.
 In the '70s, we refer to this an exemplar-based learning versus prototype learning.
 If this is what you are claiming, it seems contrary to the generalized structures in
 the attention head that represent various circuits, sort, move, et cetera, that seem
 to be operations found by Antropik or other researchers like L, Pavlik, et cetera.
 How do you square these ideas with the attention head circuit that I'm writing?
 I think these are not pretty antagonist.
 What one says is an exemplar, the other one would say it's a prototype.
 It all depends on where you look at.
 I think a model takes an input, transforms it into another representation, then transforms
 it into another representation and does that in multiple stages.
 If you look at the last stage, the only thing that this model will do is do a nearest neighbor
 in that last stage.
 So a nearest neighbor is just look at something that is near in that space.
 And yes, you can show that after transformations, even the more complex rule that you are thinking
 of will be just at the end of the transformation, a nearest neighbor in some space.
 So it all depends on which space you're looking at to define whether it's a nearby exemplar
 or it's a prototype.
 Both are the same thing just in different spaces.
 And I don't think they are different.
 It's just a degree to which they are doing more.
 So I don't think it changes anything.
 Sadly, I think people often say, well, my mother is just memorizing, it's not generalizing.
 This is not, again, dichotomy.
 Models that are good at generalizing, they also memorize well.
 And we had a paper many years ago showing indeed that memorization not only is important,
 it's actually happening in good models and it's very important.
 So memorization, generalization is not one or the other.
 You actually want both at the right time.
 And that's pretty much what these models are doing.
 Anna is saying, in other words, how can you prove that your fine tuning really made a
 difference?
 Well, that's, so we are back to, Anna was asking whether the answer you get is because
 of fine tuning or pre-training and the only thing you cannot prove, the only thing you
 can say is that now on examples that are similar to the one you asked, it's getting better.
 So you can measure with a test set whether it's getting better, and that's going to
 show that your fine tuning was worthy of doing.
 Hamid says, as many of us are intrigued and eager to advance various facets of the LLMs,
 where is the line of the concerns and precautions associated with LLMs, as well as to have regulations
 related?
 Oh, that's a loaded question.
 But let's see.
 As these models get more powerful, appear to be more powerful, they can do a lot of
 things that are very useful, but at the same time, like any tool, they can also do things
 that can be dangerous, just because it's a more powerful tool means that they can be
 used in bad ways as well as in good ways.
 Where do you draw the line?
 Should we have regulations that say, well, your model should not do this, or we shouldn't
 have models that can do that?
 I think we should.
 I think this is important.
 I think, you know, you said or, and you said yes to or, what do you mean?
 Oh, I mean, we should have regulations.
 I think it's independent of the fact that we should keep working on understanding what
 they do.
 We should also make sure that they do good for our societies as large.
 It doesn't mean that today we should stop everything.
 I think we should educate the people, we should educate the lawmakers, we should educate those
 who understand or need to understand how society will be changed with these models.
 That's necessary.
 I don't think anyone disagrees with that.
 The question is now to which degree it could be enforced into blocking you to do this or
 blocking you to do that research or blocking you to publish your model.
 There are plenty of things where regulation could happen, and I think each society should
 think about what that means.
 Maybe there should be something more global.
 I agree that we should definitely think about that and have the best for our society.
 We should not let it go unregulated, but that doesn't mean stopping everything.
 That means doing what we do now, which is trying to understand what they do and maybe
 stopping the things that are dangerous, what we understand.
 Today we don't really understand much.
 We don't really understand why a model suddenly is able to do this and not that.
 There's still a lot to do, so we're not going to stop it until we understand which ones
 we should stop.
 Why?
 We should definitely regulate.
 Anonymous says, "Assuming that we could put the whole training data from an LLM into
 a vector database and do retrievable augmented generation with it, would we find out why
 an LLM is hallucinating?"
 This retrieval of augmented generation, in case it has not been explained before, it's
 the idea that an LLM has been trained on some data and is able to reason about that data
 in a way.
 That's what the LLM does, but you could think that they are separate from the LLM database
 of facts.
 Barack Obama was born in this country, the Prime Minister of Canada is this person, at
 least at that date.
 These facts can change.
 Of course, the Prime Minister of Canada can change in five years, it might not be the
 same as five years ago, and so these facts are evolving while your LLM might be fixed
 and trained on some data that you've seen at some given day.
 Many people said, "Well, we can do this in two stages.
 We can use the LLM as a way to use language, because language is hard to get, but with
 a lot of data, we are able to have language, but the facts should come from something that
 evolves separately from a database that is updated as soon as the new facts arrive or
 facts are changed."
 So that's the promise of this retrieval augmented method, where when there is a question, the
 first thing that the model does is go onto that database or search or anything, retrieve
 relevant facts, put it in the context of the LLM, and then ask the question, given the
 fact that Obama was born in Hawaii and that Nelson Trudeau was primarily served from that
 day to that day, tell me today who is the Prime Minister of Canada.
 So suddenly you have the fact, you can use it, and it's easier.
 Of course, that's a promise.
 It works sometimes.
 It doesn't work.
 It depends on how you retrieve, which facts you retrieve, how do you know which facts
 you have to retrieve, whether the facts are updated, and also as the context gets longer,
 LLMs are less and less good at actually using any needle in the stack of your context.
 So the context might have like a million tokens, which is very a lot, but only 20 tokens are
 important for the question.
 So how do you make that work?
 It's still an open question.
 So that's a good approach, but it's still research.
 I believe it or not, but there's no more.
 No one cares.
 Fantastic.
 I'll give you one last one, in another direction.
 We have computer scientists here, and we have psychologists and neuroscientists, and as
 Gary said, their motivations are somewhat different.
 There was something that you said in the end about strategy of using the LLM, the one where
 you said break it into, they can repeat the thing and break it into chunks.
 It strikes me, and I'm a human subject in this, I use chat GPT a lot, and I feel that
 compared to last year, I am much better at it, because it has been training me on how
 to use it.
 Absolutely.
 Are there any generalizations to be drawn?
 Well, first, this is not new.
 I worked for Google for like 40 years, and I studied how people answer a query on Google
 search.
 And 10 years ago, the queries had an average length of 2.3 words, so queries were very
 simple.
 You say two words or three words, and you would get answers.
 And as the models got better, as the humans got better at asking the questions, their
 queries got longer, and they were able to express things that are more nuanced and then answers
 that are more precise.
 I think it's been a joint effort between this search engine and the humans using it to actually
 focus on what works, and it's the same for chat GPT, and in both cases, the data is used.
 Every time you ask a question to Google, first the question is scanned, as well as the answer.
 Every time you ask a question to chat GPT, of course, OpenAI keeps the question and the
 answer, and it will add it to the database.
 So it is a joint effort.
 It's not just length of question that's an issue, but also what series, how you chunk
 up your--
 Yes, but of course, back then in Google, it was a single question and simple.
 Now it's a sequence of questions, much better.
 With that, I want to thank you.
 The Bengio family is definitely leading us into the future in this.
 Thank you very much.
 [END OF RECORDING]
