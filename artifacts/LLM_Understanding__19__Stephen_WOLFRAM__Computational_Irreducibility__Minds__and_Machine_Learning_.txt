 Stephen Wolfram is a mathematician, computer scientist,
 physicist, and businessman. Have I lied so far, Steve?
 I don't know. Some people would not put mathematician first, but that's okay.
 He's known for his work in computer science, mathematics, and theoretical physics,
 a fellow of the American Mathematical Society, founder and CEO of the software company Wolfram
 Research, where he works as chief designer of Mathematica at the Wolfram Alpha and Cirque engine.
 Now I'm going to move that away, and I'm going to give it, you have also besides the people in the
 attendees, there's also one, two, three, four, five panelists that are here so far, and there
 might be some more. Carl Friston, Ronnie Cutseer, Sami Bengio.
 I can see the names, yeah. I call the rest of you I don't know, so nice to meet you.
 Sami, it's the other Bengio. We'll find out more about that. He's the brother of
 Joshua, who is the co-sponsor of this event. Okay, it's all yours. Wing it.
 Okay, so well, let's see. So I think you guys want to talk about language and computation and AI and
 all those good kinds of things, and I was thinking I could talk about things about LLMs and so on.
 I wrote this little book last February about ChatGBT. You can find a version of it online.
 I can put it in the chat. Let me not talk about that, but if people want to ask about it,
 I'm happy to chat about it. I thought what I would try to do in 45 minutes give you
 a very rough tour of my last four and a half decades of development of a worldview and see
 how that relates to things about AI and language and so on. So to begin, I think the thing that
 there's kind of this progression of paradigms to do with how we formalize the world.
 So there's sort of this question of what can we do? We start, when we see the world,
 we're interested in finding ways to have sort of formal descriptions of it that we can build on.
 So historically, kind of the first of those, the big one for our species was the invention of human
 language and the idea that you didn't have to just point at each individual rock, but you could have
 this kind of symbolic name, rock, wasn't rock originally, obviously, for that concept. And you
 could communicate abstractly about that thing using human language. Now then we've had sort
 of a stack of other ideas and other big ideas, logic, being able to, as a way of sort of
 abstracting things about the world, formalizing things about the world. Another big direction is
 mathematics, being able to something that sort of became big in the late 1600s of being able to
 describe our world by mathematical concepts and constructs. In this century, and the end of the
 last one, the big new thing has been using computation as a way to formalize and describe
 the world. Being able to specify kind of if you, the way I think about computation, it's a way of
 setting up rules and then saying, let these rules run. And the question is, can we set up rules
 that describe the way the world is, or the aspects of the world, at least that we care about?
 And my kind of day job for the last four decades has been building our computational language,
 Wolfram language, Mathematica, and so on, as a way to kind of take the things that we humans care
 about, whether they're molecules or cities or algorithms or whatever, and create a systematic
 computational language to let one describe those things, and not only describe them in a way that
 humans can read, but also something where computers can help humans to execute those things.
 So that's been kind of the way I see sort of that effort, is take the things that we humans care
 about and find a way to formalize them computationally so that we can provide sort of the raw material
 that we need to work with the world computationally. It's kind of the effort is a bit like the effort
 that happened maybe 500 years ago with the development of mathematical notation, where
 people went from kind of talking about math in terms of words to having sort of a streamlined
 notation with plus signs and equal signs and things like that. And that idea of notation,
 that sort of streamlined notation for mathematics, is what ended up launching algebra and then
 calculus and basically the modern mathematical sciences. Kind of my day job mission has been
 to create a computational language that lets one kind of launch computational X for all fields X.
 So okay, that's, so kind of this idea is computational language as a way to sort
 of formalize things that happen in the world, things that we care about in the world.
 Now the next question is what is the intrinsic description of the world so to speak? What,
 how should we describe the world in general? And the thing that sort of had been the tradition
 of exact science for about 300 years was use mathematical equations, write down an equation
 that describes this or that aspect of the world. The thing that I got interested in in early 1980s
 is how does one generalize that idea? How does one, how does one, what kinds of raw material can you
 find to talk about the world? And the thing that I started studying a lot was using computation as
 kind of the raw material for describing the world. And so the question, the first question is, well,
 okay, what, what kinds of, how do you set up kind of computational systems to do that?
 And for example, let's see, let's actually do something here. Okay. So let's say we are just
 kind of, we want to see what, what do programs that might be computational descriptions of the
 world? What, what, let's just, let's just look at some program that, just say, what do the typical
 programs out there do? So this is a very simple example. It's just, you imagine a line of black
 and white cells and you have some simple rule that says, given the colors of cells on one row,
 these are the, this is the color, this is how you determine the color of the, of the cells on the
 next row. So if you, if you run this and just, just run this starting, let's say from one black cell,
 let's run it for like 40 steps. According to that rule, you start off from, you, you end up
 with something where you have this very simple rule, very simple computational rule. You run it,
 you get this very simple pattern. Now the question is, what happens if we look at other kinds of
 rules? What happens if we kind of turn our computational telescope out into the computational
 universe and just look at what's out there? So we can do that. Let's, let's do this. Let's just make
 a, let's just make a table of, of, of all possible rules. Let's say the first, first 63 of these
 rules, 64 of these rules. Okay. So each one of these rules corresponds to each one of these
 pictures corresponds to a different rule for how the colors of cells are determined by colors of
 cells above them. And what we see is many of these patterns are very simple. Sometimes we'll get
 slightly more complicated patterns. We might get a nested pattern, for example, here, but the thing
 that is kind of my all time favorite science discovery that I made almost exactly 40 years ago,
 it's 40 years ago in June 1st, um, is this thing that I call rule 30. It's, it's specified by this
 set of, uh, of, of cases here. And if we just run, just run this,
 started off from single black cell, let's run it for, let's say, uh, 200 steps.
 This is what we get. And to me, this is something very surprising and kind of intuition breaking.
 We have a very simple rule. And yet when we run that rule, we're generating something that looks,
 at least to us, very complicated. And actually you can go and you can sort of work out what's the
 center column of cells here. And for all practical purposes, it seems completely random. But so what
 this is telling us is out in the computational universe, even very simple rules can easily give
 one very complicated behavior. And there are a lot of consequences of this. One thing that this led
 me to is this thing I call the principle of computational equivalence. So the thing that is
 sort of a big question is how do you characterize what's going on in a system like this? Well,
 you can think about the system as performing a computation. It starts from its initial conditions
 at the top, and then it's going crunch, crunch, crunch, and, and executing a computation. The
 question then is sort of how sophisticated is that computation? And one might have thought, well,
 it's just a simple rule. It's doing what it does. If we think about the kinds of computations that,
 for example, we do in our brains, well, those are going to be much more sophisticated than this.
 But what the principle of computational equivalence says is that actually that's not true. Above some
 very low threshold, essentially all of these kinds of systems, regardless of how simple their rules
 are, are equivalent in the sophistication of the computations that they can do. So that means that
 in a sense, this little rule 30 thing is doing a computation that's just as sophisticated as the
 computations that go on, for example, in our brains. Well, what consequences does that have?
 One consequence that has is this phenomenon I call computational irreducibility.
 So let's say you want to know what this, well, how this pattern is going to work out a billion steps
 later from now. Well, how do you, how do you figure that out? One way you can figure that out is just
 to follow those billion steps and see what happens. Another thing you can do is to say, wait a minute,
 I'm much smarter than this system. I'm just going to jump ahead and I'm going to say,
 I know what the answer is after a billion steps. That's the thing we've become used to in doing,
 for example, mathematical science. You know, you imagine an idealized planet orbiting a star,
 you say, do you have to work out where it's going to be a million years from now? Do you have to
 follow those million orbits? Or can you just use a formula and kind of fill in the number a million
 and jump ahead and see what the answer is. That kind of what we can call computational reducibility
 is what we've become used to from kind of what happens in mathematical science.
 But the principle of computational equivalence tells us that will not generally be what one can
 do. In general, the systems that we're studying will be just as computationally sophisticated
 as anything that we can muster in studying them. And so that means we won't be able to do that kind
 of jumping ahead. We won't be able to do that kind of computational outrunning of the system
 and we'll be reduced to something where to work out what the system does,
 we basically have to follow every step and see what the outcome is. So this is something which,
 or kind of in a sense for science, it's telling when there's a major limitation on science.
 And by the way, this idea is something things like girls theorem, a sort of a special case of this
 idea and lots of other kinds of things that one knows about universal computation and so on
 is also related to this. But this is kind of a tighter version of those kinds of those kinds of
 ideas. And one which I think sort of shows one kind of the relationship of these things to science
 and sort of the big consequences, there's lots of stuff that you won't be able to have a theory for,
 work out, jump ahead, know what's going to happen. You'll have to just follow every step
 and see what happens. And so in a sense, that's a limitation on science. From within science,
 one is seeing kind of a fundamental limitation of science. It's actually something which for
 many purposes, one might not think of being such a bad thing because in a sense, it's the thing
 that makes, for example, the passage of time meaningful. If it wasn't for computational
 irreducibility, then if you live for 50 years, then in a sense, when that would not be, nothing
 would be achieved by that. One would be able to say, oh, I know what's going to happen in the end,
 I can jump ahead and say what the outcome is going to be. But because of computational
 irreducibility, there is something sort of really happening in the passage of time.
 It is a sort of an irreducible computation that's going on. There are many other
 consequences of computational irreducibility. For example, when it comes to things like AI,
 we can ask the question, insofar as AI is doing computation, and we'll talk about the sense maybe
 later in which typical modern neural nets are doing only very weak levels of computation.
 But let's imagine that we have a system that is doing computation as computation can be done.
 Well, we sort of have a choice. Either we can say that system is we're going to make that system
 computationally reducible, so we know what the outcome is going to be. For example, we can say,
 we're absolutely sure this system will never do the wrong thing because we know its outcomes,
 and we can constrain it to set it up so we can sort of prove that it will never do the wrong thing.
 It's reducible enough that we can know enough about what it's going to do that we can know
 it isn't going to do the wrong thing. So that's Plan A. But the problem with Plan A is that means
 that the system can't do irreducible computations. The system can only do computations where we can
 jump ahead and foresee the outcome. So in a sense, that means we're crippling the system.
 We're preventing it from doing what it could do as a computational system. We're saying it's
 only going to do those things which are kind of reducible. So in a sense, I think it's going to
 end up being sort of a big societal choice, is do we want the AI's computational systems to be able
 to do all the powerful things that computational systems can do, or do we want to insist that
 they'll only do things where we can foresee what they'll do? And in a sense, it's kind of like we
 have a, you could say, well, I'm going to set up all these rules for the AIs that make sure they
 only do the right things. Well, to make that work, you have to have the AIs be sort of computationally
 reducible. If they're computationally irreducible, well, maybe you can constrain it in all sorts of
 ways, but there'll always be surprises. There'll always be things where you can't foresee that
 particular outcome. By the way, computational irreducibility has many, many consequences,
 but another consequence it has is that sort of science will never be finished. There will always
 be, if we think about, there'll always be things where we can't foresee the next thing that will
 happen. There will always be surprises. In mathematics, there will always be new theorems
 that can be proved and so on. The thing that is an issue there in terms of things like will science
 be finished and so on is, well, okay, there might be things that were surprises, but are they
 surprises we care about? If we were exploring all of mathematics, we would prove more and more and
 more theorems, but it could be that we get to the point where we know all the theorems we care about
 and anything else is something we're not going to care about. So in a sense, there's sort of this
 connection to sort of human issues in what, but the point is that there is ultimately an infinite
 and unlimited frontier of what's possible to discover in science and so on. By the way,
 that also relates, maybe we can talk about, to things like, well, okay, let's maybe talk
 about this idea of computational irreducibility that you can't know the outcome of a computational
 process in general except by running it and seeing what happens. Limitation on science,
 thing that makes the passage of time meaningful, kind of dichotomy for thinking about AI and so on.
 So let's see, one of the things that's sort of interesting about this is we can just sort of,
 in this computational universe, we'll find all sorts of things that go on.
 The question becomes sort of, are those things that we find out there, things that we care about or
 not? In other words, we can go and we can, oh, I don't know, that's an example of just a simple
 rule and what it does and we can get lots of other examples. We can go and do this ourselves
 if we want to, let's see, and just go find very simple rules that do very complicated things.
 It's easy to kind of launch out into the computational universe and find these things.
 The question ends up being, so what do we humans care about these things? Well, it could be that
 this particular thing, we will be able to use it for technology in some way. It could be that
 we'll think this is something very important for art, but it's something where out there in
 the computational universe, there's kind of an infinite supply of original things. The question
 is, which ones do we humans choose to care about? For example, if we imagine kind of the future of
 AIs, you can say, okay, AI, go out into the computational universe. You can go and create
 things that have never been seen before, all kinds of things. The question is, are those things that
 are of kind of human relevance to us now? Well, one thing you might do, you can actually do a
 little experiment here. Let me show you something. Oh, where is it? So for example, we could say,
 we could take some image generation AI, and this is just a diffusion image generator,
 and we could say, let's look, let's ask the thing to make a picture of a cat in a party hat.
 But inside the AI, it's got some, you know, embedding vector. It's got some set of numbers
 that describe that is its version of what that concept is. But one thing we could do is something
 very simple to sort of explore the universe of possibilities. We could say, we're going to take
 this AI that's very aligned with human interests because it's been trained on billions of human
 images. But nevertheless, we could say, let's take this AI and let's sort of move around
 in this space of possibilities. And so for some set of numbers, we've got the cat and the party hat,
 but as we change those numbers, we're moving out from that. And we have this kind of, in the middle,
 we have this thing we might sort of describe as kind of cat island, that is things that to us kind
 of look like cats. But then we go further away and we'll get into things which aren't like cats. If
 we go far enough, you know, we'll be able to go, I don't know, as an example, we'd be able to go
 from, what is that going to? That's, well, okay, here's one that goes from a cat to a dog. We're
 going through this kind of meaning space from a cat to a dog. But in general, what we'll find
 is that we, in this sort of space of possibilities, there's this region that corresponds to this
 concept that we have of a cat and a party hat. But as we go away from that, eventually we move
 far enough, we'll get to a picture of a, you know, a dog wearing a sweater or something. But we go
 through a large volume of inter-concept space of things which are images which were generated by
 this AI using, you know, computationally generated, they're out there in the computational universe,
 even set up to be quite aligned with kind of the pictures that we humans have put on the web. But
 nevertheless, they're not things which are normally described by a word like a cat or a dog or whatever
 else. So you might ask the question, you know, in an image generation AI, what volume of the space
 of possibilities is covered by concepts that we have already defined? The answer is maybe one part
 and 10 to the 600. So in other words, there's this vast kind of inter-concept space of possible
 images, only tiny corners of which are described by words that we have in human languages.
 So in a sense, as we look at this kind of inter-concept space, we could say, you know,
 we don't necessarily have a word to describe some of these patterns, but we might say, oh,
 that's kind of a cool pattern. And maybe we decide at some point that that's a particular style of
 art. And eventually we get a word for it. And then we develop this whole kind of human interest
 in that particular piece of what was inter-concept space. And now that becomes a concept in our
 languages and so on. So this idea, this sort of this core idea that there's this huge space,
 this huge kind of computational universe of possibilities, even reduced here by ones that
 are sort of images aligned with images that we put on the web. Even if you reduce it in that way,
 the part of that space that we have so far explored, that we have so far come up with words for
 and described with concepts is a tiny part of the space. And there's vastly more that is kind of
 being found in the sort of inter-concept space. Now, what, you know, can we describe kind of the
 way that kind of we think about sort of our progression in kind of the progression of human
 civilization and so on? In some sense, you can think about us as progressively colonizing
 inter-concept space. We're progressively coming up with things, coming up with, we're coming up
 with sort of this social construct of language that different ones of us sort of collectively
 understand that corresponds to these different points in the space of possibilities. And sort
 of the progression of civilization we can think of as being this progressive kind of progressive
 exploration of inter-concept space. And, you know, as we invent new paradigms for things, we get to
 kind of, or new ways of describing things, we get to kind of move outwards in the space. Now, for
 example, in my day job of creating computational language to describe things, my mission, in a
 sense, is to find those places in the space of possibilities that we humans care about and that
 we can use as kind of building blocks to construct kind of in a computational way a description of
 what we want. But there's kind of a broader science of what's in principle out there,
 which is broader than the things that we humans have so far chosen to come up with words for and
 so on and have languages for. Well, just to kind of fill out a little bit kind of the, a little bit
 more of kind of the worldview that develops from all of this, we can ask questions about, okay,
 what about our physical world? How is that constructed? What is the, what's kind of the
 underlying structure there? And one of the things that's been very exciting to me in the last few
 years, something I really did not expect sort of to happen, is that it's turned out that we've
 been able to work out how this kind of computational ideas provide sort of an ultimate infrastructure,
 an ultimate kind of machine code for the physical universe. And what, well, let me describe that
 a little bit, because we're going to come back to this question of concepts and inter-concept space
 and so on, but we're going to come at it now from a different direction, from understanding the
 structure of the physical world. So sort of big picture, back in antiquity, people were arguing,
 you know, is the world discrete or is it continuous? Is it made of atoms or is it just things that are
 sort of flowing? And one didn't know. End of the 19th century, it became clear, yes, there are
 molecules, matter is discrete. A little bit later became clear, there are photons, light can be
 thought of as being discrete. At that time, people mostly assumed that space would turn out to be
 discrete as well. But for various reasons, nobody technically managed to make that work. And so
 physics kind of went on with the space is continuous. You can kind of put things any way you
 want in space. Well, if you're thinking about things in kind of computational terms, you're
 immediately led to say, wait a minute, you know, perhaps space is actually fundamentally a
 computational construct, fundamentally a discrete kind of thing. And the big surprise of four years
 ago now was that, yes, we actually managed to figure out how to make that work and managed to
 figure out how that connects to the big theories of current 20th century physics. And actually,
 the really remarkable thing that maybe I'll have a chance to describe is that the big theories of
 20th century physics, essentially general relativity, the theory of gravity and space time,
 quantum mechanics and statistical mechanics or the second law of thermodynamics. Those are sort of
 three big theories of 20th century physics. It turns out that all three of those theories
 are not just things that we can kind of say, oh, that's what's true. They're actually things that
 we can in some sense derive from fundamental considerations. I had not expected any such thing
 to be the case that we could derive the laws of physics, so to speak, but we can. And I'll explain
 how that works. And that's kind of loop back to questions about language and concepts and so on.
 But okay, so what's the universe made of? Well, in our models, the universe consists of a bunch of
 sort of discrete atoms of space. We tend to call them EAMs, kind of atoms of existence. They're
 things where the only thing you can say about them is they exist and they have an identity and
 they're distinct from each other. And then there's one more thing, which is you can say how these
 EAMs, how these atoms of space are related to each other. You can say this one is related to these two
 other ones. It's kind of like what atom of space is friends with what other atoms of space. And you
 define this whole collection of relations between atoms of space. And you can represent that by a
 graph, a network, or actually more formally in our models, a hypergraph, but essentially ones just
 dealing with this big network of relations between the atoms of space. And so everything in the
 universe in our models is just made of the relations between atoms of space. So for example, if
 something like a black hole, for example, is just a structure in the, I might even be able to show
 you a picture of one. Let me see if I can pull this up. Let me see. Am I going to get this to
 work? Yeah, maybe. This is actually in kind of the fabric of space. This is two little tiny black
 holes. And we'll see in this video kind of space, most of the activity of the universe actually is
 knitting together the structure of space, but there are two black holes there and you can kind
 of see they eventually merge. They produce gravitational radiation. Actually what we get
 from this model where we're looking at kind of the discrete structure of space, we can successfully
 reproduce the actual things that are observed in black hole mergers and so on. But in any case,
 the basic point is what the universe is made of, everything in the universe is just a feature of
 the structure of space. And when it comes to time, time is the progressive rewriting of the
 structure of that network that represents space. So time is actually a very different kind of thing
 in these models from space. Things like relativity emerge as a feature of the model. They're not things
 that are put in from the underlying structure of the model. Okay, so we've got sort of the notion of
 space, notion of time. It turns out quantum mechanics is a thing that inevitably emerges
 from the fact that when we are updating this network, there isn't just one possible path of
 history. There isn't just one possible way that the network can be updated. There are many possible
 paths of history that branch and merge. And essentially the structure of those things is
 what leads to quantum mechanics. Well, one of the issues is when we're looking at the system and
 we're seeing all these rewrites and the structure of space and so on, the question is how do we
 experience that? There are all these things microscopically happening, but we have a certain
 experience of that. And it turns out that sort of a critical feature of what's going on is that we
 are observers of a certain kind. So let's take the case of, let's look at, for example, let's see.
 Let's look at something like statistical mechanics. We've got a bunch of molecules
 bouncing around in a box. And one of the kind of big principles is the second law of thermodynamics
 that says when you start those molecules off in an orderly way, their motion will tend to eventually
 look disordered and random. It will look as if it has higher entropy. And the question is sort of
 what's really going on there? And it turns out that what's actually happening, something I
 finally understood, I've been thinking about this for like 50 years actually, is that what's
 ultimately going on, you can look at different kinds of versions of this, what's ultimately going
 on is that these molecules are bouncing around in a certain determined way according to some rule.
 And in fact, that rule can be reversed. So you can take this pattern of molecules you get at the end
 and you can say, I can figure out, oh yes, that pattern of molecules came from the simple initial
 state. Well, in principle, you can do that, but it's a computationally irreducible process.
 And the difficulty is that we, human observers of things, are computationally bounded. We can't do
 that all the computation that's needed to reverse what happens in the molecules. We're just stuck
 saying that we can get this impression of what's going on. And with that impression of what's
 going on, with that computationally bounded impression of what's going on, all we can say is,
 oh, it looks random to us. And that's kind of the ultimate origin of the second law of thermodynamics
 is something which has to do with the relationship between underlying computational irreducibility
 and our computational boundedness as observers. Well, it turns out that both general relativity
 and quantum mechanics come from the exact same thing. They both come from this idea that there
 is computational irreducibility underneath, but we are, well, actually there are two attributes
 that we have to have as observers, that we are computationally bounded and that we believe we
 are persistent in time. So in this model, for example, we are at every moment in time,
 we're made of different atoms of space, yet we all have the impression that we are experiencing
 things through, that it's still us a second later, so to speak, and that we experience things,
 we are persistent, we have a continuous thread of experience through time. Well, okay. So the
 really ultimately big concept here is this thing we call the roulead. And so here's how this works.
 When we look at these, this underlying hypergraph and its rewrite rules and all those kinds of
 things, we can, we say, okay, there are these underlying rules. And if we run those enough
 times, we'll eventually get something that seems like our universe, that satisfies Einstein's
 equations of general relativity, that shows the Feynman path integral for quantum mechanics,
 all those kinds of good things. But we still might be asking the question, well,
 why did our universe get one particular rule and not another? And that had me very confused for
 quite a while until I realized that actually we can think of the universe as running all possible
 rules. So what we imagine is that there are these possible computational rules that can be used to
 update this hypergraph and so on. But let's just imagine that we use all possible rules. What we
 get are all these different parts of history that branch and merge and so on, corresponding to the
 application of all these different rules. And this whole object that is the entangled limit
 of all possible computational processes, we call the roulead. And the roulead is a completely
 unique thing. It is, it is, you take every possible Turing machine, every possible computational
 system, you run all of them and you run them in such a way that they don't just have one possible
 outcome, they have all possible outcomes. You might say, what an incredible mess. How could you ever
 conclude anything from this roulead object? It is the case that this roulead object is a unique
 thing. It's not like there's seven different rouleads. There's just this thing that is the
 entangled limit of all possible computations. And so then the question is, well, how can you
 conclude anything about, about this roulead object? Well, what you have to realize is the roulead
 object represents everything that's possible, everything. And so for example, we, as observers
 of what's going on, we must be embedded within this roulead. And so what we can think of is that
 this, this, um, um, was I sharing the screen or did I stop sharing? Well, anyway, um, the, uh, the, the,
 this, so the issue is we are observers embedded within this roulead, observing the roulead.
 And the question is, what do we conclude about the roulead? And the roulead is a necessary thing.
 There's no choice about it, but the nature of us as observers is contingent, so to speak.
 And so what turns out to be the case is that observers like us observers that have certain
 attributes necessarily conclude that necessarily describe the roulead in certain ways. So in a
 sense, by being an observer who is computationally bounded, who believes that persistent in time,
 those two attributes alone are sufficient to tell us that the slice of the roulead,
 the way that we parse the roulead is exactly the way that corresponds to the laws of physics that
 we know. So in other words, what we're saying is you can derive the laws of physics. The laws of
 physics are derived by starting with this roulead, which is a necessary unique object, and then saying
 what for observers like us, which happened to have the properties that we have of being
 computationally bounded and believing we're persistent in time, any observer with those
 very coarse properties will necessarily conclude that the universe operates according to Einstein's
 equations and the path integral and so on. So that's a rather interesting philosophical
 conclusion. Now you can ask, well, what would observers not like us conclude? Well, we don't
 know. You can kind of, and that's sort of a question of how do we think about observers not
 like us? Well, one thing to realize is we can think of in the roulead, we can think of different
 possible observers as being sort of at different points in the roulead. They're at different
 places in rouleal space. Just like in physical space, we could be here on this planet, we could
 be on a galaxy on the other side of the universe, we can be at different places in physical space,
 and each different place in physical space will give us a different point of view about how the
 universe works. Well, so it is in rouleal space, each different place in rouleal space will give
 us a different point of view about how the universe works, how things work. So here's a way to
 think about that. We can think of essentially different minds as being at different places in
 rouleal space. It's as if, and these different minds are kind of experiencing possibilities
 in a different way. So if we think about that in terms of, you know, the LLMs and so on, it's kind
 of like we could imagine just having a differently trained LLM, and that differently trained LLM
 basically exists at a different place in rouleal space. So, for example, minds that are sort of
 similar and sort of similarly trained will be fairly close in rouleal space. Minds that are
 different, like, you know, let's say cats and dogs, further away in rouleal space. Minds, I tend to,
 I think that one of the consequences of the principle of computational equivalence that
 I mentioned earlier is that one could sort of attribute mind-like things to lots of systems in
 the world and lots of abstract systems. And so, for example, when one says the weather has a mind of
 its own in the principle of computational equivalence says, yes, that's a meaningful
 thing to say. But in a sense, the mind that corresponds to the weather is pretty far away
 from us in rouleal space. Well, so now there's a question, how do you communicate across rouleal
 space? How do you, what's involved in doing that? Well, at some computational level, one point in
 rouleal space corresponds to sort of computing according to, let's say, one Turing machine.
 Another point in rouleal space computing according to another Turing machine, another computer.
 We know that in principle, we can make a translation from one place in rouleal space to
 another place in rouleal space. It takes effort. We have to actually create that interpreter
 that's going to interpret the instructions of one machine as the instructions of another machine.
 It takes effort in the same way as it takes effort to move in physical space. In a sense,
 when we move in physical space, in our models, we're reconstructing ourselves at a different
 point in physical space. And by the way, you can understand things like time dilation and relativity
 as a nice kind of mechanical explanation of that. If you're always in one place,
 you're spending your kind of computation budget figuring out what the next stage you'll be in is.
 But if you're moving, then you're using some of your computation budget to kind of recreate
 yourself at a different place in space. And so that's used up, having used up some of your
 computation budget, you necessarily sort of go through time more slowly. Time goes more slowly
 because you've used up some of your computation budget in moving in space. But in any case,
 so by the way, in our models, the possibility of motion is non-trivial. It's not obvious that you
 can pick up a glass and move it somewhere and it'll still be the same glass. That's something that
 we generally assume about the world, that pure motion is possible. But it's something in our
 models that you have to prove that pure motion is possible. And even in traditional physics,
 if you're sufficiently near a space-time singularity, for example, no material object
 will maintain its identity as you move it around that singularity. But in our models,
 the possibility that that thing can just move and that it's still the same thing is non-trivial.
 And actually, in a sense, the particles of motion are exactly the kinds of particles that we know
 about, like electrons and quarks and so on. What is an electron? An electron in some sense,
 in an abstract level, is a lump that is capable of pure motion. It's something where you can have
 an electron in one place and you can move it and it'll still just be that electron.
 So particles are kind of the carriers of pure motion in physical space.
 So here's a thing in rural space. We can ask sort of what is motion in rural space about?
 Well, in a sense, what it means to have motion in rural space is you're effectively transporting
 something from one mind to another. If different points in rural space correspond to the positions
 of different minds, you're asking the question, what does it take to kind of transport things
 around rural space? And I think this is one of the very bizarre kinds of things that one realizes,
 is it seems to be the case that concepts are the analog of particles. So what, in physical space,
 in an electron, it doesn't change as you move it from here to there. In rural space, it's the
 concept of a cat, for example, that can be moved from one mind to another without change. I mean,
 the particular details of the neural firings that exist in my brain, when I think of the
 concept of cat, in any of your brains, the particular neural firings will be different.
 But yet we can package up the concept of a cat and I can say the word cat, I can transport it to you
 and then you can unpack it. And in your place in rural space, you can end up with the same thing,
 so to speak. So it's kind of a way of understanding that that's sort of the fundamental thing that's
 going on and we can think of kind of concepts as being the particles of a rural space.
 Well, there are lots of things I see, I'm running out of time here, but there are lots of things
 we can talk about, about what, well, let me just say a couple of other things about,
 I'll talk a little bit about AI. And I mean, the AI has had many different meanings over the course
 of time and many things where people have said, if we could only have that, then we have AI,
 are things that I've built as kind of pure computational systems. And then people say,
 well, it's just a computational system, it's not really AI.
 Just one second, I want to, you do have more time because for some reason I can't explain,
 Caillou, who has been extremely conscientious in everything, is not here, which may mean that he
 had misunderstood being a discussant for being a member of the panel, which means he won't be
 here until the panel starts, in which case you have more time if you wish. Go ahead.
 I just compressed four and a half decades into a remarkably short time. I hope people could follow
 it. You could have taken twice as much time. Well, okay. So let me finish what I was saying
 here and then maybe we can turn this over to discussion, which is more fun for me.
 So talking about kind of modern AIs and, you know, to many people, modern AI is neural networks.
 And there's sort of a question of, well, what can, how do neural networks relate to
 all of the things I've been talking about? And one of the questions we can ask is, okay,
 we have our friendly neural nets here. Let's see, oops, share the screen.
 Okay. We have some typical trained neural net. Let's say we're trying to train it.
 Let's say we're trying to train it to reproduce a sine wave. So what we're doing is we're going
 to feed in the X value at the top there, and we're going to have set up these neural net weights,
 and it's going to compute the Y value down here. It actually will do a pretty crummy job of that,
 typically. And you can change the neural net. You'll get different kinds of behavior.
 It's usually not particularly good at computing something like this. Well, so one thing you can
 ask is, you know, neural nets, let's say if we have a big enough neural net, maybe we can break
 computational irreducibility. Maybe we can just predict what's going to happen in any kind of
 system. That is not going to work. I mean, the way that a neural net of this type works, it's just
 having kind of numbers ripple through this sequence of layers. And we're ending up with
 something where you can, this is something trained. I used a modern transformer architecture and
 trained it to try and recognize what was going to happen in a cellular automaton.
 And it says, well, there's a certain probability of what's going to happen.
 But when the behavior is pretty simple, it'll nail it. When the behavior is more complicated,
 it's like, I'm sorry, I can't figure that out. This is different levels of training of one of
 those neural nets. So in a sense, not surprisingly, the kind of very finite computation of these layers
 of a neural net can't do the unboundedly large computation required to kind of solve a
 computationally irreducible problem. And you can see that again. Let's see, where do I have an
 example here? These are examples of the three-body problem in celestial mechanics, Earth, Moon, Sun,
 all idealized, all with interacting through gravity. You can ask the question, if you train
 a neural net, can it correctly reproduce the behavior? The answer is the neural net is the
 kind of solid line here. That's its prediction. When the behavior is fairly simple, yes, it can
 do it. When the behavior is kind of computationally irreducible, no, it can't do it. None of this is
 really very surprising. But there's kind of a question, for example, when we look at something
 like chat GPT and we say, oh my gosh, it actually worked, it produced something that is like human
 language, how did that work? What I think is the main thing going on is something which tells us a
 lot more about human language probably than it does about neural nets. Because what it's telling
 us is, if we think about how does chat GPT work, it's basically just saying, I'm going to predict
 the next word by figuring out certain probabilities. And it's going to do that by, at the very
 simplest level, it might just do it. Let's see if we've got one here. It might just do it by knowing
 the frequencies of different letters. And then if you just use the frequencies of different letters,
 you get pretty much nonsense. If you use blocks of letters, you'll start getting more sensible
 kinds of things. If you use kind of whole words occurring with the probability that they occur in
 English, you'll get things that don't make much sense, but they're kind of things that can
 construct. Now, the big thing that's interesting and surprising is that when you kind of train a
 neural net from all of the text, a trillion words of text or something, that the extrapolations it
 makes about what make meaningful sentences tend to agree with the extrapolations that we humans
 would make about that. It's very similar to the fact that if we train a neural net to recognize
 cats from dogs and images, that the distinctions it will make seem to be similar to the distinctions
 we will make. At a theoretical level, if we say where's the dividing line between cat pictures
 and dog pictures, there isn't a good mathematical characterization of where that dividing line is.
 It's really a question of where do we humans say is a dividing line between cats and dogs.
 And the thing that's interesting about neural nets is they tend to make the same kinds of decisions
 about that that we tend to make. Probably the reason is that ultimately their architecture
 is similar to the architecture of our brains. But the main point is that those kinds of distinctions,
 there's not a theorem. There's no theorem that says the neural net will reproduce the distinction
 between cats and dogs because you don't know what the target is. The target is what do humans think
 is going on there. And it does a pretty good job at that. So now the question is, in the case of
 language, what's going on? And I think what's happened is that the thing that allows an LLM
 to produce reasonable language is something that is a regularity of language that we could have
 recognized a long time ago, but we didn't. And so we know certain regularities in language. We know
 that, for example, in English, you tend to have sentences that go noun, verb, noun. But there
 are plenty of sentences of the form noun, verb, noun that are total nonsense. So the question is,
 you have this kind of syntactic grammar of language that says that you go things like noun,
 verb, noun, but now you have the question of, well, what noun, verb, nouns actually make sense?
 And so what I think, you know, chat GBT and LLMs and so on are kind of showing us is that there is
 also a semantic grammar of language. There's also a construction kit, not only of what the parts of
 speech might be, but also what kinds of words they might be to have them make sense. And that's
 something that eventually kind of sort of expands up to write a whole essay and have these puzzle
 pieces put together in a way so that the whole thing makes sense. So, you know, in a sense,
 what one's seeing and one can kind of look at, let's see if I have some pictures here.
 Maybe I have some pictures. Yeah, these are from, these are very ancient, actually. There's better
 ones now for GPT-4 of kind of, to what extent can you kind of imagine semantic laws of motion where
 you're kind of moving around in meaning space and where just like Newton's laws tell you in physical
 space, how you move from one, you know, how motion happens when in the absence of a force,
 you just keep moving in the same direction and so on. So you can ask questions about ruleial space
 and you can ask questions about kind of the structure of ruleial space and how that works.
 And I think the, we're kind of learning some scientific things from the operation of LLMs
 about how that works. Now, another question would be, so in other words, I think the reason LLMs work
 as well as they do is because there are a bunch of regularities in human language that we kind
 of didn't know were there and that we've never really codified. People started codifying these
 things back in the 1600s, for example, people tried to invent these so-called philosophical languages
 that would be kind of not specific to any particular language, but they would be things
 that sort of represent the meaning of things without the specificity of particular languages.
 Well, actually, I've had a project for a while now, much more energetic to make what I call a
 symbolic discourse language, a language where just like in Wolfram language, we have this
 computational language that describes many aspects of the world. I mean, we, you know, we might have,
 you know, all sorts of different sort of categories of thing that we describe in our language.
 And the question is, can we kind of describe all, can we describe sort of things that come up in
 everyday language? Can we describe those kinds of things in a sort of precise symbolic way?
 And I have to say that I can't say I've got the full answer to that, but it's going really well.
 And it's become clear, and by the way, LLMs are quite helpful in this, to having a way to take
 something, not the level of language where we're actually putting words together, but the
 representation of the core meaning of what's going on. Just like in our computational language,
 we have that representation of sort of the core meaning of what's going on in a way that can be
 read by humans, but also executed by a computer. So in any case, that's sort of one direction about
 things with LLMs and so on. Another question that I was curious about is, okay, why does machine
 learning work at all? Why is it the case that you can train one of these neural nets to do something
 like, I don't know, recognize digits or recognize cats and dogs or generate language or whatever
 else, why does that work? When I played around with neural nets back in 1981, and I couldn't get them
 to do anything interesting. And I kind of thought at the time, oh, if I've got a simple enough
 problem, I'll be able to get a simple neural net to do things, didn't really work very well,
 wasn't very interesting. The big thing that got sort of accidentally discovered basically in 2011
 was that if you have a big neural net and you bash it really hard, you show it enough training
 examples, it'll learn, well, lots of different kinds of things. It'll learn almost anything.
 And the big meta discovery of modern machine learning is that if you bash a neural net hard
 enough, it'll learn almost anything. We don't know quite what the almost is. We can't really
 characterize what kind of thing it can learn. For example, as I said, it can't break out of
 computational irreducibility. So there's limitations to what it can learn, what it can do,
 but nevertheless, there's a broad class of things that seem to correspond a lot to kinds of things
 that we humans can do easily that the neural net can successfully do. And so that's sort of the
 meta discovery. Question is, why does that work? Why is it the case that this neural net can be
 successfully sort of bashed into learning things? Why doesn't it get stuck? Why doesn't it get to
 the point where you just can't get there from here? You can't arrange it. Why is it the case
 that it's possible to do it? And then why is it the case that you can iteratively do it by sort
 of adaptively training it? I got interested in this very recently, actually, and I don't know
 whether I can show you pictures. Let me see. I can show you some things that I did recently,
 and then maybe I'll be able to pull up some pictures just from the last few days.
 Let me see here. Right. So actually, I decided to look at a simpler problem,
 which is the problem of biological evolution, to sort of another case of adaptation that
 is a little simpler than neural nets. But let me explain what I figured out about biological
 evolution. For a long time, I'd wondered what sort of the minimal model of biological evolution.
 I was always very unsatisfied because models, natural selection seems like a simple principle,
 but when you actually try and make explicit models for it, you end up with all kinds of
 hair about how many suboptimal organisms do you keep and all this kind of thing.
 So I was interested in sort of a minimal version of that. So here's a version of that.
 So this is actually one of these cellular automata. It's got these rules here. Starts
 off from one red cell here. And with these particular rules, you get a pattern that lives
 for this amount of time and then dies out. Okay. So let's imagine that you're interested in
 doing something where you just keep on tweaking the rules. You keep on resetting the rules.
 You keep on making single point mutations in the rules to try and get it to live longer and longer.
 This is what happens. You start off from something that is just a blank rule. For example,
 it dies immediately. You keep tweaking the rule. You have to go through many different tweaks and
 so on. But eventually you'll get to the point where it lives there for 50 something steps.
 Well, and you can see the sequence of mutations that got made there. And if you look at how the
 fitness of this organism, the length of time it lived, varies as you go through all these different
 sort of steps of adaptive evolution, you'll see it's going along and there are many things that
 don't work out, but it'll kind of cruise along here at a certain fitness. And then it makes a
 discovery and then it can go to higher fitness. And actually you can end up with all kinds of
 discoveries that it makes. These are different sort of paths of evolution. And you'll see that,
 for example, here it's kind of going along and eventually it manages to discover a lot. It
 manages to live a long time. You could sort of imagine in the fossil record, you might find a
 critter from the Cambrian period that looks like this. And then it uses that idea to extend further.
 And by the time it's in the Silurian period, it's looking like this. And maybe it makes it to this
 in the Triassic period or something. But what's happening here is that it's having progressively
 more ideas in a sense about how to live longer in this particular case. And actually you can even
 go ahead. This is a simple enough system that you can actually work out. Let's see, that's an
 example. I think I have a better example. There we go. This is the path of all possible paths of
 evolution for a simple system like this. So every different picture here is a possible organism.
 And the arrows show the possible adaptation paths. And what you see is something that's very much
 like what happens in biological evolution. There are different branches in the tree of life.
 There are, you know, one set of ideas leads to long life over here. In this way, a different
 set of ideas leads to kind of long life over here in a different way. Okay. What does this have to do
 with machine learning? Well, you can ask the question. Let me see if I can pull this up.
 I am going to have to pull up something that I just made. So I'm not sure whether I can find it
 here. Hold on. You can get hot off the press or not really off the press at all. Where is it?
 Let me see. This might be it. This is a very minimal model for,
 let's see if I can get this bigger. It's a very minimal model for a neural net where it's actually
 a cellular automaton as well. But instead of having a fixed rule that it keeps on applying,
 kind of like a recurrent neural network, it has something more like a feedforward neural network
 where you have a discrete choice of one of, let's say, two different possible rules. And at every
 point in space time, so to speak, you're picking a different rule. And so then the learning consists
 of, well, what's the pattern of rules you should pick to get a particular outcome? In this particular
 case, we're trying to learn to live as long as possible. And what's interesting here, and again,
 this is just raw off the literally raw material that from a couple of days ago, this is kind of
 showing in a sense how the thing does what it does. So in a standard neural net, it's just much more
 complicated to display what's going on. You've got these neurons with continuous weights and you've
 got connectivity all over the place and so on. This is a much simpler case. So you can kind of
 see more about what's going on. What's non-trivial is that training actually works in this case,
 and it does. You can find this arrangement of bits that will cause the thing to do, I don't know
 whether I have it in this example here, but that will cause it to learn... Let's see if I have one
 here. Now those activation levels. Well, it doesn't matter, but that will basically cause it to learn
 something like, you know, to tell whether the number of bits at the beginning is even or odd
 or something like this. And we actually even tried training this on the MNIST training set,
 and it doesn't do too badly. But the point here, the thing that's interesting here,
 these are all different solutions that this kind of very idealized neural net found to
 living for this exact number of steps. What's interesting about these is they're very bizarre.
 They're not sort of engineered solutions. They're not solutions where we can say, oh yeah,
 let me look inside and see how this works. Let me show you another example of that.
 This is more back to the biological evolution case. This is kind of all the different ways that a
 certain class of systems manages to live a long time. And some of them, it's kind of pretty
 structured. You can imagine sort of this was an engineered thing, but some of them, it's like,
 it just seems to sort of happen to live that long and then it dies out. So in other words,
 there's a lot. And by doing this sort of adaptive evolution, you're ending up finding these things,
 which are very not, they're not mechanical. They're not engineered kind of ways that things work.
 They're things where kind of this is sort of what's happening inside. This is, this is the thing
 that is, that's going on, but it's not something where you can say, oh, I've got a mechanism.
 By the way, if you're interested in neuroscience, this is something you should pay attention to,
 because in a sense, if you're trying to explain what's happening in the brain and you say, oh,
 I'm going to figure out how this works. Well, how this works is an attempt to have kind of a human
 understandable narrative for what's going on. But, you know, if I were to look at these pictures in
 the background here, if I were to, if this was something going on in a brain, I might be able
 to say, okay, I can have some human narrative about what's happening here. If this is what's
 going on in a brain, it's just, well, it happens to work that way. And it happens to give this
 result. It's kind of a computational irreducible story. It's something where you're not, there's no
 sort of narrative mechanistic explanation. It's something which just works that way. And it's
 computationally irreducible, but it just comes out in that fashion. And I think that's sort of
 an interesting question for in machine learning. Why does machine learning work? Okay. So let's look
 at, where's a nice picture of that. Yeah, by the way, this is in biological evolution. This is,
 people often talk about fitness landscapes. This is an actual fitness landscape correctly
 drawn, so to speak. And you can, you can start seeing all kinds of things about things evolving
 on fitness landscapes. But the thing I really wanted to show you, here it is. This is kind of
 the local behavior at a particular point in rule space at various steps in the adaptive evolution.
 So what's happening here is at this step, for example, in the adaptive evolution,
 here are different possible directions in rule space that you might go. And the ones inside the
 circle are ones that are losers relative to where you've already got. They're ones that would be
 live less long than what we have here. But there are some that would make progress. And in fact,
 this is the one we happen to choose in this particular random sequence of adaptive evolution
 steps. And that was, that was the thing that made progress. So the thing that is not obvious is
 in this sort of high dimensional space of possible ways you could go. The question is, will you
 always be able to make progress? Will there be a direction that makes progress or will you get
 stuck? Well, I think that this is again, a computational irreducibility story that basically
 what would make you get stuck? Well, if the structure of this rule space was very orderly,
 very reducible and easy to predict, you might end up in a box with very precisely defined walls.
 And you just can't escape from that. But the presence of computational irreducibility kind
 of implies a certain degree of unpredictability, a certain degree of intrinsic randomness effectively
 in the structure of rule space. And that's what means that in these high dimensional spaces,
 there's always a kind of a path to success. So in a sense, I think computational irreducibility,
 which, which is a limitation on what, you know, what one can do with, for example, a neural net,
 what kinds of things, computations one can expect to do is also the reason that training of neural
 nets, for example, can work. And so I think that's anyway, this is a, this is still a an in progress
 kind of investigation. But I sort of think it's an interesting connection between a lot of different
 things I've talked about. All right, I've gone on longer than I intended to. So let me wrap up there,
 and I'm happy to have a discussion, questions, whatever else I just fed you an awful lot of
 material. Let's first applaud this president.
 The problem was that Caillou had an emergency during the, during your talk, so he couldn't
 hear it. Caillou, do you think that you have, from the background material you might have
 looked at, you have a basis for saying something? Sorry, I missed most part of the talk. So probably
 I won't be able to, if you haven't seen, this is, this is a large amount of material, you won't,
 I would be surprised if we could have a useful conversation without having some,
 some anchor to this. Okay, then could you please explain to Caillou and to me and to us how
 computational irreducibility differs from A, B, C,
 Kolmogorov complexity, the church touring thesis, and NP completeness?
 Okay. All right. Let's start off with Chetan Kolmogorov complexity. So when we look at a
 picture like this, the, the, the algorithmic complexity of this picture is tiny. That's the
 program that's needed to produce it, just a few bits. The thing that is remarkable is that even
 things with very low algorithmic complexity are very complicated. In fact, they're complicated
 enough that to us humans, we wouldn't even be able to distinguish them from things that have high
 algorithmic complexity. One of the things I've sort of had a long running discussion with my
 friend Greg Chetan, where the question is, is the universe like pi or like omega? So omega is this
 thing that Greg invented 50 years ago, actually, that is the halting probability for a universal
 Turing machine. It's a fundamentally non-computable object. It's an object with, with infinite
 algorithmic complexity. It is a thing where there is no, there's no small program that, that you,
 you can't specify it by a small program. Pi, on the other hand, is a thing that is specified by a,
 you know, a quite a small program. And once you, but once you generate its digits, it looks for all
 practical purposes random. So the question that one can ask is, in our universe, is there anything of
 high algorithmic complexity, or is the whole universe actually a thing that is like pi,
 generated from something which is a very simple underlying sort of program? And in our model of
 physics, the answer is the universe is, is like pi. The universe is something that is, is generated
 from a thing of very tiny algorithmic complexity. So, so that's kind of the distinction between
 everything I'm talking about is things of incredibly low algorithmic complexity. The
 remarkable fact that is not obvious, it's kind of a breaks one's intuition, is that things,
 very simple programs, things of very low algorithmic complexity can produce what seems to us like great
 complexity. And the seems to us becomes much harder when we start talking about our computational
 boundedness. It's not the case that it's just, oh, it's sort of, it seems complicated. It's that
 for a computationally bounded observer like us, there is no way to compress it. So in, in, in
 algorithmic complexity, algorithmic information, one's saying this is the program and there is no
 shorter program. One can say for a computationally bounded observer, this is the set of bits and
 there is no way to make it shorter. So that was, that was, um, algorithmic algorithmic information
 theory, algorithmic complexity. I think the second one you had was, um, what was it? Church Turing.
 Okay. So, so, okay. The, the, um, the thing that, um, uh, if you go back to the beginning of the
 20th century and you'd wanted to get an adding machine, it might go to a store, you buy an
 adding machine, you want to get a square root machine. Okay. You go to a different store,
 perhaps, and you buy a different machine that is the square root machine. The big discovery that
 actually originally got made by Moses Schoenfinkel with combinators in 1920, but nobody understood it
 then or since basically, but then kind of got clarified by, by, by Turing in 1936 is that there
 exist kind of, uh, there exist systems that are universal in the sense that you can have a single
 piece of hardware that by feeding it different initial conditions, by feeding it different
 inputs, you can make it compute different kinds of things. So for example, you can have a Turing
 machine that has, um, where just by feeding it different initial conditions, it will emulate
 any other Turing machine. So for example, if we go to, um, uh, in terms of Turing machines, there's,
 um, I'm going to show you something. Uh, there we go. Um, well, so, so the, the big point is there
 exist machines that are universal in the sense that they can at least emulate all other machines
 of that type. The thing that then became clear starting in the 1930s is that Turing machines,
 you can have a Turing machine that emulates every other Turing machine. It also, by the way,
 can emulate every register machine, every lambda piece of lambda calculus, every con
 combinator and so on. So there's this notion that in the class of, of computational devices,
 there's a certain degree of universality. People had not thought that that extended to physics.
 That was the thing that basically was my effort in the 1980s was to, to kind of imagine that
 this notion that what is computationally computable would also be what is, what can happen in physics.
 People had sort of assumed that physics kind of breaks out of kind of this computational paradigm.
 It has real numbers, precise real numbers, has other kinds of things like that. So the first thing
 is, is the realization and the principle of computational equivalence. The first thing is
 kind of the claim that, that, well, first part of it is sort of the physics part that yes, actually
 in the physical universe, this is all we've got. We can't say, oh, we're going to make an analog
 computer that jumps beyond kind of the, the, the kind of the, the church Turing level. Second point
 is this people imagined that to make a universal machine was a complicated matter. It was something
 that would be a kind of, you know, you have to build this whole microprocessor. It might have
 a billion gates in it. It has all these instructions. It's got if statements, it's got all this kind of
 structure. And the question is, well, what's, you know, is that really necessary or is universal
 computation actually something much more naturally occurring as universal computation, a special
 thing have to go to a lot of trouble to get, or is it something that's just sort of lying around the
 computational universe? One of the big points to the principle of computational equivalence is yes,
 it's just lying around the computational universe. So for example, if we look at these different
 possible rules here, some of them behave in such simple ways that we can readily see what they're
 going to do. They're computationally reducible. There's, there's nothing more to say, but some of
 them behave in a complicated enough way that we're kind of not really sure what they do. Let me show
 you an example of one of those. So this is, let me show you rule one 10. This thing actually only
 grows on one side here, but to show that I shall make it just show just the part where it's growing.
 So that's, that's if after 200 steps, let's run it for a thousand steps. Okay. There it is. It's
 a little bit unclear what it's going to do. This is kind of computational irreducibility in action
 or undecidability ultimately in action. What's it going to do? Is it going to have all those little
 things, structures? Are they going to survive or are they eventually going to die out after I think
 it's about 4,500 steps. They do eventually all die out and they just get left with this one single
 structure here, but it's kind of computational irreducibility in action. You can't tell what's
 going on. This particular rule turns out, if you just look at it, let's start it off from random
 initial conditions. Let's say 600 across, no, let's say a thousand across and let's say 800 down. Okay.
 Oh boy. It's aliased a bit on the screen here. Let me make it a bit bigger. Okay. There we go.
 So what you see there is that's like, that's the initial condition. This is what happens. What you
 see is a bunch of little structures here and you might imagine as you look at these structures,
 oh, they're kind of interacting and maybe that's like a logic gate and maybe we can make an or gate
 out of this and so on. But it turns out with Considered Life, you can do that and you can show
 that rule 110, which is kind of just the 110th rule in this very simple enumeration of possible
 rules, it's universal. The first one that you might imagine could be universal is actually universal.
 So in a sense, the church Turing thesis is saying it is possible to have a universal machine at
 least universal within the class of computational devices that we're talking about. The principle
 of computational equivalence says not only is it possible, it's also generically the case it is
 ubiquitous. And in fact, it goes on to talk more about individual computations rather than
 programmability, but that's kind of a bonus. By the way, in terms of Turing machines,
 I was very curious what is, you know, if you just look out in the space of possible Turing machines,
 just start enumerating Turing machines. The first one whose behavior is not obviously
 simple is this one here that I found sometime in the 1990s. And so then I was really curious,
 is this in fact a universal machine? In 2007, I put up this little prize and a chap called Alex
 Smith managed to show that yes, this particular Turing machine, the first conceivably universal
 Turing machine actually is universal, which is a nice piece of evidence for the principle of
 computational equivalence. So that's kind of the relationship between church Turing and principle
 of computational equivalence and computational irreducibility is something that is sort of,
 it's made tougher by the fact that this property of universality is ubiquitous in the computational
 universe. NP-completeness, what's that? We have a question here already. Wait a second.
 My buffer is not empty yet. So you asked about NP-completeness. So let me try and address that.
 So normally in a Turing machine, for example, you have this Turing machine, it has a rule,
 you start it off from some initial condition, it just evolves in some specific way. It has
 a specific history, but you can also have, let's see if I have a picture of this.
 I have a bunch of these multi-way systems. Well here, let me show you.
 Find some multi-way Turing machines. There we go, multi-way Turing machines.
 Okay, so that's a typical Turing machine. It has a rule, it evolves in a particular way.
 But you can also have a multi-way Turing machine in which there isn't just a single possible path
 of evolution, but there are many paths. So you can end up with this kind of branching structure.
 This turns out to be closely related to what happens in quantum mechanics. That's a separate
 issue. But so this idea of NP-completeness, NP problems versus P problems and so on, it's this
 question. If we have a Turing machine and it computes something and it takes a certain amount
 of number of steps to compute it, an ordinary Turing machine might take, you know, N squared
 steps to compute a size N version of some problem. But we can also have a non-deterministic Turing
 machine. We can have a multi-way Turing machine that follows many different possible paths.
 And we say, if we have a path that gets to the answer, then it's a winner, so to speak.
 And that's the story of NP problems, non-deterministic polynomial time problems,
 are ones where there exists a path in this multi-way Turing machine, which gets you to
 that answer. So in a sense, this question of NP, sort of the big question, is P equal to NP? Is the
 class of problems that you can solve in polynomial time with an ordinary Turing machine the same
 class or a different class than the ones that you can solve with a non-deterministic, with a
 multi-way Turing machine? And actually, that question, so, well, let's see. I mean, we can
 talk about computational irreducibility and its relationship to computational complexity theory
 in general. But NP completeness in particular, there's perhaps a more interesting thing to say,
 which is, if I can make, find this one, this might be it. Okay. So we can look at
 all possible Turing machines. This is, in a sense, in rural space. Where is this?
 Nice picture somewhere here. Do I? Yes, here we go. Okay. So this is a picture of kind of the behavior
 of all possible multi-way Turing machines. So in a sense, all possible programs. And this is showing
 sort of all possible non-deterministic programs. The red part is the part that's showing deterministic
 programs only. It's not allowing the possibility of the rules changing, so to speak, as you go
 through the system. So the P equals NP problem, one of the things that's pretty interesting that
 comes out of our physics project is essentially a geometrization of the P equals NP problem.
 That is a question of the structure of these objects in rural space. That P equals NP becomes
 the question of whether essentially the red bit here eventually fills out the gray part of this
 picture. So you can kind of have a geometrical version of this ball in rural space that corresponds
 to the P problems and the NP problems. So that's a little bit of an indication of that. But you can,
 I mean, this whole question about non-determinism and so on, it's a, oh gosh, there's much to say
 about that. I've studied this a lot because it ends up being a sort of a proxy for quantum mechanics.
 What happens in quantum mechanics is that you are following many paths of history. And the observer
 in quantum mechanics is effectively sort of an interesting situation. The observer is branching
 in the same way that these actual paths of history in the universe are branching. So quantum mechanics
 becomes this question of how does a branching mind perceive a branching universe? And so it's
 interesting to kind of see a bunch of different examples of multi-way systems as a way to get
 sort of more intuition about that. Okay. Another question, apparently. Well, let's bring it back
 to our universe just for now. I want to, if possible in a few minutes left, because I don't
 want to say anything, I want to make a connection between what you said and what Kyle, what Caillou
 said before. His program was related to computer aided proof and transformation of verbal,
 verbally stated truths and conjectures into a formal form called auto-formalization.
 And my question to you is what does the irreducibility principle say about all the
 possible theorems and all the possible paths to their solution? And Kyle, Caillou can
 say a couple of words in response, but you have to say it in your own words first because you missed
 your talk. Well, let's see. I wrote a book recently about the physicalization of meta-mathematics,
 which I think is pretty relevant to this. And so, you know, we can imagine some, let's see,
 where's a good example. That might be some axiom in a mathematical system. And we can ask the
 question, what are the consequences of that axiom? That axiom is saying X dot Y is equivalent to Y
 dot X dot Y. And now we can say, well, what things are also equivalent based on that axiom? We can
 start figuring out. So every path here is a theorem that that's equivalent to that.
 We can start just following, we can start making this network of all possible equivalent things.
 And actually, and so a proof becomes a path in this whole network. And actually,
 it's a little bit trickier than that when you start looking at, oh, there's a good example here.
 The way one actually does, let's see where I've got a good example. Okay. So this is an example
 of what more is actually what's happening in mathematics. You have basically, let's say,
 two axioms here, and you are combining them to get a new theorem. And so you can kind of build up
 this kind of this structure you get with those two green axioms. You're deriving all those theorems.
 You get this big network that represents all possible theorems derived from a particular
 set of axioms. So you can go on, you can get pretty complicated versions of this.
 You can derive all sorts of theorems that are true based on certain axioms. And what's happening is
 in this graph, the every blue dot is a theorem. Okay. So then you can ask the question, if you
 look at, I have an example of this, if you look at actual axiom systems in present day mathematics.
 So for example, you can look at, there's the axiom for semi-groups. You can start proving
 theorems about semi-groups. Okay. So we've got this whole network of theorems about semi-groups.
 And so one big question is, if you do that, well, okay. So here's an example based on the
 axioms of Boolean algebra. So this is proving theorems based on axioms in Boolean algebra.
 And you can go and you can build up this giant network of theorems of Boolean algebra.
 And this is kind of the enumeration of all possibilities. Okay. So now the question is,
 as we enumerate all those possibilities, where are the theorems that we care about?
 We've got gazillions of theorems that we can just build out eventually. And this is kind of spoiled,
 if you didn't know it, you haven't followed this, but this Ruliat object that I talked about in the
 talk I gave, that is the ultimate limit of all possible mathematics is this Ruliat object.
 And the question of what theorems are, so all these theorems here are true. All these theorems
 can be constructed from the axioms, but these are the only two theorems that people give names to
 in textbooks of logic out of this collection. We can keep going. We can find, we can go to lots
 of other theorems. If we use a theorem proving system, we can, in that big giant explosion of
 possible theorems, we can go and say, this is the theorem we're searching for. And we can find a
 path to it using theorem proving. And those are the paths in that structure of possible theorems.
 Okay. So one question then is, well, out of all these possible, this complicated network of all
 possible theorems, where are the ones that we humans care about? And so I looked a little
 bit at that. And so, for example, you can look, well, that's, for example, that's Euclid.
 So Euclid has 465 theorems, and you can start off from the axioms at the top,
 and you can see what are the connections between those theorems according to the proofs in Euclid.
 Perhaps more interestingly, you can take a proof assistant system. I looked at Lean. I looked a
 little bit simpler to look at the system called MetaMath, which is a formalized math system.
 And you can ask questions like, well, that's the Pythagorean theorem proved from the axioms
 in MetaMath. And you can see they're a different, it's a pretty complicated thing. This somewhere,
 I think at the bottom here is the Pythagorean theorem. And you start from the axioms there,
 and you can count up all of the various axioms. How many times did you use the axiom of equality?
 Five times 10 to the 31 times. This is what happens if you start from the axiomatic
 foundation and you build up to something like the Pythagorean theorem. So, okay. So one interesting
 point here is this is the axiomatic structure of the Pythagorean theorem. The question is,
 do mathematicians care about this? So, you know, a decade ago, I was very interested in
 formalization of mathematics. I organized this conference. We invited all these formalization
 of mathematics people, all these people interested in mathematics itself. The formalizers all showed
 up. The mathematicians didn't show up. And so the question is, what does a working mathematician
 actually do? You know, working mathematician who's thinking about the Pythagorean theorem,
 are they thinking about it in this kind of axiomatic way? Are they drilling down to kind of
 this low-level axiomatic structure, or are they just saying it's the Pythagorean theorem and I'm
 going to do things at that level? The thing that's pretty interesting and relates to a lot of what I
 was talking about is at this kind of axiomatic level of mathematics, it's kind of like molecular
 dynamics in a fluid. You've got all these molecules bouncing around. They're doing all
 these complicated things. But then at the higher level, at the more human level, what we get to see
 is fluid dynamics. And we can ask the question, can we make conclusions at the fluid dynamics
 level? Or do we get dragged down to the molecular dynamics level and have to address things at that
 level? So in the question of using the Pythagorean theorem, for example, do you need to go down to
 the level of these axioms and worry about how you define the real numbers and so on? Or are you
 actually operating in practical mathematics at a higher level at this level where you're just
 operating in terms of these kind of fluid dynamics type concepts? So I have to say, I was curious
 in the world of LLMs and so on. So I will say that the mission of taking a piece of informal
 mathematics and formalizing it seems like a fairly promising use case for things like LLMs.
 I don't know whether the formalization in terms of existing proof assistance and so on,
 I don't know how useful that will really end up being. I was curious whether you could use LLMs
 as a way to kind of guide theorem proving. So I looked here.
 May I make another suggestion instead of looking at let Caio answer just a second because there's
 so little time now we can continue. Please, let me just ask a very practical question.
 I have another thing in 30 minutes. I could push it back, but am I going to make that or not?
 Yes, the other thing in 30 minutes is the panel. Are you going to do something else?
 Well, I don't know. It really depends. But Danielle said that you were going to do both.
 Okay. All right. I think if you got the other thing from Danielle,
 it is in fact the panel. No, no, no, it's a completely different thing, but that's okay.
 We'll do your panel. That's great. Yeah. I think the short answer is yes. Yes, people are using
 informal mathematics and auto formalization, which means translating informal to formal
 to guide theorem proving. For example, given a proof, you can use language model like GPT-4 to
 just ask it to generate an informal proof or even a sketch, some ideas of could be high level idea
 of how this proof might go. And then conditioned on this informal sketch, there will be a second
 step to generate the formal proof. But I think a caveat is if you rely on auto formalization to
 give you the proof, it only works if human already discovered this proof. Because then there's no,
 if not, if it's completely alien mathematics, then there's nothing for you to auto formalize.
 I think another direction related to what Stephen mentioned is how can we even take one step further?
 Can we use language models to generate conjecture, like generate the huge graph Stephen was mentioning?
 But of course the graph, I think it might be infinite or it may be simply too big. So
 a really interesting question I want to maybe narrow from Stephen is say we want to generate
 this graph, but how do we tell if a node is worthy, like if a mass statement is interesting?
 Because I imagine in this infinite graph, most of the nodes will be just garbage, like two greater
 than one, three greater than two. But we really want to focus on this interesting nodes. Yes,
 it's an interesting question. So I've looked at this a bit and I can tell you that in the
 case of Boolean algebra, there is a criterion. So if you, maybe I can pull up a picture of that.
 If you order, here we go. Hold on. Let's see. If you order the theorems of Boolean algebra
 in lexicographic order, then you can ask which of the theorems, of all possible theorems,
 which ones are given names in logic textbooks? And sort of a surprise to me is the theorems that
 are given names in logic textbooks are the theorems that have in this case no backlinks.
 So there's a backlink from this result here, which might be one of your boring results.
 This result is derivable from something earlier in this lexicographic list.
 So in a sense, it gives you no new information. It turns out the ones that get given names are
 precisely the ones that do not have backlinks. They are not derivable from lexicographically simpler
 theorems. So in other words, I was surprised that I discovered this sometime in the nineties. I was
 surprised by this, that there was actually a criterion for what would be given a name in a
 logic textbook. Now the general case of, is this theorem interesting? You know, can we learn enough
 about the humans to know what they'll think is interesting? It's a good question. I mean, by the
 way, in this connection between formal and informal, obviously, you know, Orphan Language gets
 connected to LLMs. And we've done lots of work in kind of tool calling from LLMs to Orphan Language.
 And there's this whole question of can you take, you know, to what extent can you get the LLM to
 crispen things up to the point where you can, I don't know. I mean, if I say something like draw
 a pentagon and a hexagon, for example, let's see what it does. I don't know whether it'll figure
 it out or not. You know, it might be able to, or, you know, we could, you know, the question is can
 we generate, okay, can we generate a, can we turn that informal statement into a piece of formal
 Orphan Language code? Okay, not bad to manage to do that one. And if we, if we look here, I'm sure
 we can get it to, yeah, there we go. So that showed us the actual code that did that. Wouldn't have
 been the way I would have done it, but it's okay. That was, that's a, that's a reasonable way to do
 it. But so this is a case where we're going from an informal description to this computational
 language, which we can then compute from. And that's a very powerful thing to do. And in fact,
 we even have a product that's coming out soon that is based precisely on that idea. But so I think,
 you know, this question of whether you can sort of, you know, can you guide the proof this way? I
 suspect you can. Now this question of what, what is a human proof? What's, okay, this is an example.
 So in automated theorem proving, one of the shocking things about automated theorem proving,
 I believe you might correct me and tell me one day somebody is going to tell me I'm wrong about
 this. But so far as I know, essentially all the theorems that have been proved by automated
 theorem proving were theorems that somebody already believed were true. In other words,
 there is no, there's no newly discovered thing that came from automated theorem proving with
 one counterexample. The one counterexample is something I found 24 years ago now,
 which is this is the simplest axiom system for Boolean algebra. So you can, this is, that's a,
 you can think of that as a NAND operator. This is of all possible from that one axiom, you can just
 derive all the true statements of Boolean algebra. The proof of that is this long hundred step
 automated proof. Let's see if I have a picture of it. Yeah, I mean, that's sort of some kind of
 visual representation of that proof. It has various popular lemmas in it and so on. In the last 24
 years, despite quite a bit of effort actually, nobody has ever understood this proof. But it
 is interesting because it is a proof of something surprising, potentially interesting, depending on
 whether you care about simplest axiom systems for things. But it was found by automated theorem
 proving without, you know, without already knowing what you were searching for, so to speak.
 And, you know, that's a case where now, now you ask the question, if you're out in the wilds of,
 you know, in the, I mean, we can, we can look. I have a nice picture of this. We can look at,
 oh here, let's, is that one? Yeah, this is, this is, these are axiom systems down the left.
 Those are theorems across the top. And there's a dot, a blue square whenever that theorem is
 true in that axiom system. So we can ask the question, given an axiom system that we decide
 is exciting, and how we decide that is an interesting question, it's unright. But given
 an axiom system, we can say, you know, here are the theorems that are true. Now, which are the
 theorems here that we care about? And that's, you know, that's essentially a model for humans.
 And I think it's an interesting question. We've done some experiments kind of grinding up archive
 and so on, and trying to figure out, you know, can we deduce what, you know, in a space of possible
 theorems, what theorems are likely to be interesting? I don't, I don't know if you've looked at that.
 Is that, I think that's an interesting thing to look at. Have you looked at that?
 Yeah, I think the way I'm looking at it is more, so I'm, I'm not considering its relationship
 with other theorems. I'm taking the theorem statement itself and try to have some, for example,
 have the language model telling me whether it's interesting. I believe the language model can
 look at some kind of superficial cues, like how long the theorem is and how, what are the
 variables, how they are arranged, how messy it is. And which can already give us some way of
 like judging how interesting it is. But I agree, like maybe ultimately what an interesting theorem
 is, it can help you prove a lot of other theorems. Is that your definition of an interesting
 theorem? I'm not sure that's right. I mean, in other words, there are, you know, that is one
 possible way, I guess. I mean, there are many different criteria you could imagine for
 interestingness. I mean, that particular one would be saying that if that was the correct criterion,
 okay, then what you could do, like I was just showing that picture actually in the, in the
 Boolean algebra case, wherever it is, then I would deduce from your statement that the theorems that
 are, that are big here are the ones that have many, that those are the, those are the high out degree
 theorems. So in other words, those theorems, that theorem there should be the one I should care about.
 I don't understand these theorems, honestly. I mean, in that sense, yes, because they are special in
 this graph. That's right. But the question of whether those are human useful is, I think,
 a different question. I mean, in other words, what is, you know, there's this question, it could be
 the case that two and okay, first question is, if you look at many different possible things you
 might prove, you can ask the question, are there repeated theorems that often, are there repeated
 lemmas that often come up in those proofs? Okay. So I've looked at that and the answer is there are.
 And so, for example, that, that axiom system of mine for Boolean algebra, if you use it to prove
 theorems in Boolean algebra, you can just look at what intermediate lemmas does a theorem prover
 typically prove to make progress. And the answer is, for example, it takes it a hundred steps to
 prove the commutativity of NAND, and it often does that and then goes on and does other things.
 So in that sense, you know, you can, I mean, it is an interesting experimental question,
 to what extent are there repeated lemmas that show up? And that might be a criterion,
 but that's a criterion that has nothing to do with LLMs and so on. That's a criterion that just has
 to do with the metamathematical grot. I think the, this question of, you know, if we look at images,
 for example, you didn't see what I was showing earlier, but here I'll pull up a, I was just
 showing something like this. This is in, you know, embedding space of a generative AI system with,
 in the middle is the cat in the party hat, then there's sort of a cat island of cat-like things,
 and then you're out in sort of inter-concept space that we have not yet explored. And so you can
 imagine the same kind of thing for mathematics. You say, here's a theorem that somebody wrote
 down. Let's sort of change the embedding in some sense and say, here are nearby theorems that
 weren't necessarily, you know, where is the island? How far out does the island of
 interestingness go? What happens in this kind of inter-concept space between this
 theorem that we thought was interesting and this other one we thought was interesting?
 So, I mean, I think it's a, it's a, I mean, let's take an example. Let's say, I don't know,
 let's take, well, here we've got, you know, these are random pictures generated in inter-concept
 space that maybe are of things that we care about. I don't know. I mean, that one on the right,
 we might kind of think it's, I don't know what it is, but, you know, it was just generated by
 a generative AI. And similarly, imagine that was a theorem. The question is, is this a theorem that
 we care about? You know, it's just like, is this a picture we somehow seems relevant to us? And I
 think, you know, this question of whether, I mean, if we look, one of the things that's sort of
 interesting that one can do is to kind of look at this whole space of, let's see, we can kind of look
 at metamathematical space and we can kind of ask, let's take a look here. I think I had a nice picture
 and find it. I mean, we can ask all sorts of questions about, about different possible
 proof structures, which are, it's a metamathematical thing, but let's see if I can find a picture here.
 Yeah, that's a picture of, I think this is metamath. This is, this is empirical metamathematics.
 It's asking in the space of all 200,000 theorems discussed in, you know, presented in the metamath
 corpus, where do these famous theorems of mathematics lie in that space?
 So it's kind of asking this question that this isn't all possible theorems. This one is reduced
 to just the ones that are in the, I think it's set.mm corpus for metamath. And, but you know,
 this is again related to this question of where are the ones, where are the ones one cares about,
 so to speak. And you can kind of, well, you can kind of see, this is kind of how the different
 theorems in different areas of mathematics kind of get related to each other. But I think it's a
 really interesting question. What, you know, and you mentioned that, I mean, I suspect LLMs are
 really good at picking up on cues from humans. And so I'm sure there's ways that people will write
 their master theorem. You know, there'll be more trumpets blaring when they present the master
 theorem in their paper than when they present a little lemma. But here's a good question.
 Here's a question. If you try and make, this is an easy thing to test. Okay. Can an LLM classify,
 given a statement, can it decide whether the author of the paper will have called it a theorem
 or a lemma? Well, I would guess yes, because there are different subtle cues, but I didn't try. What
 I tried, what I did try is I gave LLMs some inequalities, like very simple elementary
 inequalities. Some were written by humans from the problem sets, from IMO problem sets, for example,
 and others are just generated randomly by machines, which typically look very messy.
 And LLMs can do a reasonably good job at that task, although I would say that task may not be very
 difficult. Well, okay. So we've tried to do things like this because we've been interested
 in automated testing of mathematical and multiple language. So we're interested in generating tests
 that are plausible input, so to speak. So we've indeed tried doing things like that. It's not
 been particularly successful. I mean, in other words, you can generate an expression at random
 just by some Markov process, for example, and you can generate an expression by using some LLM-like
 device. And you can ask the question, you know, given that you've seen, I don't know, we've got
 billions of sort of human-related Wolfram language expressions. And then the question is, can we
 generate others that are like those? That's one question. Another question of great practical
 interest for us is, can we guess whether something that somebody entered is likely to be what they
 meant? Or is it something, in other words, it's like asking the question, is this a plausible
 sentence or is this a sentence nobody would ever write? Which is something which LLMs, in a sense,
 implicitly are capable of doing. I must make an executive decision now. And it's a calculated
 risk. We may lose Stephen Wolf from, if it turns out he has something else of higher priority. We
 may lose other people for the panel, in which case I will have called closure on this needlessly,
 and there may be nothing in the panel. But I have to call closure because we have to stop
 this session and then restart for the next session. I want to thank you very much, Stephen,
 for your talk, and I hope we'll be seeing you again in 10 minutes, but we'll see.
 Okay, the same thing for the panel. Are you the same zoom link for the panel?
 I will redo it. I'll restart it for the panel, and I hope you'll be there.
 Stephen's there too, but I have to break it now.
 Good luck.
 Good luck.
 [BLANK_AUDIO]
