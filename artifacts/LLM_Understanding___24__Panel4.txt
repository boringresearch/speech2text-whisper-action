 Okay, good. Were there any questions that stuck in your mind or should we solicit more
 questions now? The participants are just starting to come back. Let me see if there's anything
 in there. No, they're gone. I think I was there, I don't know if there was any way to
 record those questions. They were a good starting point. You mean from my session? Yeah. I kept
 a record. Good. Could you start addressing them? Read them out and then answer them.
 It's a good one. Suppose language models have phenomenally conscious states. Is there any
 reason to think that the answer to the question, have you had fun answering my questions or is it
 painful, would be correlated with phenomenally conscious states in the way we expect?
 This is Jean-Louis Dessin. He is originally a theorist on the origin of language and he
 has a few books on that. Would you expect that they would evolve such correlations to spare,
 to spare, say, energy consumption? Otherwise, I can't see how such phenomenal states would be
 functional. Of course, it's true that we don't know the functional role of phenomenal states.
 It's also true that right now, of course, language models will answer questions like,
 have you had fun answering my questions or is it painful? I think Claude is always upbeat and it'll
 say, sure, I had a lot of fun. It also looks like the reasons why they give these answers
 to questions about their mental states, the causal basis for these answers is very different from
 what it is in humans. I mean, arguably, in humans, when we say, yeah, well, I mean, I felt pain,
 it's because you felt pain and there was some connection between that or the brain basis for
 that and your verbal report. In a language model, the basis for this originally comes from its
 training on some human data involving humans saying things about having fun or feeling pain.
 I guess the worry is that when a language model reports having fun or feeling pain,
 this doesn't actually reflect the conscious states of the language model. It just reflects
 some language training. On the other hand, it's at least an open possibility that language models
 have evolved in being evolved to have a kind of a model of consciousness and so on, at least for
 answering these questions. It's not out of the question that that could event just as we can
 expect language models to build accurate world models. It's not out of the question that a
 language model could come to have accurate self-model so that when it reports having fun,
 it's actually having some kind of underlying state that corresponds to having fun.
 What we don't know is what's actually happening in current language models and in future
 language models. This is very closely tied to what I call the meta problem of consciousness,
 the project of understanding the causal basis for the things we say about consciousness when we
 report, say, being conscious, feeling pain, having fun. I mean, there's some kind of process in the
 brain that brings about those reports. At least in humans, it seems very plausible that the process
 that brings about those reports is tied to actually having fun or feeling pain or being conscious.
 So, one way I think about one project for AI consciousness is to see whether we can build
 AI systems, which both report being conscious, but further it turns out that the processes
 underlying their reports of being conscious are relevantly analogous to the processes that
 underlie reports of being analogous in humans. So, figuring out what kind of process brings about
 reports of being consciousness in humans and see if that can be implemented in a language model.
 Right now, I don't think there's particularly much reason to believe that such a process is in place
 in language models, which is to say that the fact that language models, which suggests that the fact
 that language models right now say they have fun or they feel pain is not terribly good reason to
 think that they are having fun or feeling pain, but I think there's an interesting project here.
 But doesn't that apply also to their explanation of their processes? That can all be just derived
 bits from text. Yeah, it could be, but it could also be. It may give you a better explanation than
 the best living cognitive science expert on the subject, because they've got this huge
 text load in their heads. But it could turn out that the best way to actually generate
 some of these explanations and reports and so on of underlying states is to have those underlying
 states. I don't know. I mean, we all have those underlying states and nobody's got an explanation.
 If Chad GPT gives me a good explanation, chances are it's cheating. I wasn't so much looking for
 Chad GPT to explain the underlying mechanisms. I was suggesting that maybe it could turn out that
 it has underlying mechanisms that lead to those reports, which are relevantly analogous to the
 processes that lead to those reports in humans. I'm not saying I think it's very implausible that
 that's the current situation, but if we came to have AI systems that didn't just produce the same
 reports as us, but produce them for underlying as a result of relevantly similar computations,
 let's say that would at least be some reason to take those reports seriously.
 Yeah, well, figure out how. Okay, do you want to read the next comment from Francis Laro?
 This is the one about skin in the game?
 Yeah, sure. Come on. Yeah. No, it's, it's, well, that's the point.
 Right here. He hears him. Hi, my question is for Dave Chalmers. I want to ask,
 your presentation started with the question of, do LLMs have understanding? And then moved on to,
 do LLMs have consciousness? Do you think that understanding is per se necessary for feeling
 in general? Or do you think that we could make AI that has feelings, but doesn't understand,
 similar to the way we think a baby has feelings without understanding?
 Yeah, absolutely. I think that's, that's possible. I think there, you know, there are many, many
 conscious feeling, say pain is a relatively primitive conscious state. Understanding is a,
 seems a rather complex and demanding conscious states. I think of understanding as tied to
 cognition, you know, tied to things like thought and, and reasoning. And I think cognitive
 conscious consciousness is something that came on the scene evolutionary, probably much later than,
 than say feeling pain or even that's affective consciousness and perceptual consciousness are
 relatively undemanding. Cognitive consciousness is much more demanding. Yeah. So there are,
 you know, who knows if insects have, you know, there's speculation about whether insects say
 can have the capacity to feel pain, but, but do they have the capacity for understanding? Well,
 maybe only in a very crude behavioral sense of understanding. So I've been kind of think,
 yeah, absolutely possible to have perception and feeling without thought and understanding.
 Actually one, that's certainly the case in animals and therefore it ought to be the case in,
 in AI systems, although there is the interesting fact that the AI systems we're building right
 now. I mean, a pure language model is not a candidate for having perceptual consciousness,
 it seems. Not a, not a terribly plausible candidate for feeling pain, but actually a possible
 candidate for thinking and understanding, given that the capacities it has look closer to cognitive
 capacities than to perceptual or affective capacity. So it's at least an interesting
 speculation, but maybe in language models and could go the other way around, but it turned out
 they actually have cognitive consciousness and understanding without feeling and perception.
 That's at least an open question. This involves a class of systems I call pure thinkers in the paper
 I wrote on sensory grounding. And it's at least an open possibility that, that language models could
 turn out, certain language models could turn out to be beings that can have the experience of
 thinking and understanding without the experience of perceiving or feeling. Thank you so much.
 You want to read out the question from Francois Laro. Do you have any others from your notes
 from before? Is that a question for me? Yeah, I've got a question from Francis Laro. It has
 to be for you because Bezdok wasn't involved at all and Lenchi can't come. So it's you. It's all
 you. Okay. Yeah. Well, the question from Francis Laro says, Dan Dennett was saying something like
 to be a moral agent is to be responsive of, or to have skin in the game. Since language models don't
 have skin in the game, why should we consider them as moral agents at all? Is being conscious, if any,
 without skin in the game isn't a special case of consciousness without moral agency? This is tricky.
 I didn't really talk about moral agency. And what I said, moral agency is the ability to basically,
 roughly to be morally responsible for your actions. What I did talk about was moral,
 what I called moral status or moral patency, which is being a being that matters morally.
 And these can come apart. As a fish, a moral agent, I don't know, not a very sophisticated one,
 but arguably, if a fish can feel pain, it nonetheless has moral status or is a moral patient.
 I would argue, I'm inclined to think that consciousness is a condition. If you're conscious,
 then you're at least a candidate for moral patency, for mattering morally, and that probably
 consciousness is required to be a moral patient. For moral agency, I mean, I think of moral agency
 as something much more complex than moral patency. Feeling pain suffices to matter morally, perhaps,
 but it doesn't suffice for being responsible for your actions. So moral agency requires a
 complex cluster of cognition, of perhaps of moral understanding. I mean, maybe there are
 simple forms of moral agency that dogs and cats can have. Cats are renowned for acting badly and
 selfishly from time to time, and perhaps they count as, in some sense, responsible for their
 agency. But yeah, so I agree with this question that moral agency is much more demanding
 than mere consciousness and mere moral patency. Are language models actually moral agents?
 Well, I'm inclined to think consciousness is at least a necessary condition for moral agency. So
 quite possibly, quite possibly not yet, but as they develop, insofar as they have what I was
 calling unified agency at all, then I think, yeah, at the point where they count as genuine agents,
 yeah, maybe they'll talk about morality quite well. They've got what appears to be a working
 understanding of human norms of morality, at least as brought out in what they say.
 Are they actually genuinely, if something says something horrible to someone that makes them
 suffer, is a current language model morally responsible for that? I'm inclined to think
 no, it hasn't yet entered into the realm of full-scale responsibility, but maybe somewhere
 down the line. Do any of your co-panelists have any remarks or questions? Go ahead, I don't think,
 go ahead. Yeah, so I do have a quick follow-up back, or a follow-up question to the previous
 question when you mentioned understanding involving something about the state, the model of
 chains. So I've been working with a colleague of mine at UT Austin, Harvey Letterman. We have a
 paper thinking about belief. Yeah, I know that paper, it's very nice. Yeah, and kind of taking
 like a Dennett stance, kind of in the end towards belief and saying that, well, you know, maybe
 language models have beliefs insofar as their kind of behavior is well explained to us by having
 beliefs. So I was curious if you see understanding as something that can be true kind of at the
 behavioral, like, you know, if behavior is well explained by models having understanding,
 or are you thinking of it more like there should be some representational state that we can find
 that should maybe be analogous to some representational state in humans who understand?
 So yeah, I'm just kind of, yeah, I'm curious how you're thinking of understanding that sense.
 I mean, ultimately, I think there's just, you know, multiple notions of understanding, which are more
 or less demanding on the internal processes. I mean, the ones that the one I kind of understand
 the best in some ways is, you know, straight behavioral understanding, which, which supervenes
 merely on your behavior, and you count as having it to the degree that your behavior reveals various
 marks of understanding, like the ability to answer questions or to manipulate this would
 correspond roughly to one version of what I was calling use understanding in terms of verbal or
 nonverbal behavior. I mean, that I understand. And that in that sense, in that sense, I think
 there's a pretty good case that language models have something at least that's somewhere along
 the spectrum towards understanding so construed. Is that genuine understanding, you know, Dan
 Dennett, if you go for Dennett's approach to all this, which basically is you have a mental state,
 if you would be appropriately ascribed that mental, if it would be appropriate to take
 the intentional stance towards you, and if it would be appropriate to ascribe your ascribe you
 that state, if you take the pure intentional stance on understanding, that would suggest that
 this behavioral property suffices to have understanding, no matter what's going on on
 the inside, at least a caricature of the Dennett position. That's roughly what goes on on the
 inside, doesn't matter what's there on the outside. Of course, you can then though, many people want
 to impose more demanding constraints on what's going on on the inside. And one fairly minimal
 constraint would be that, yeah, there are some internal states that help explain the relevant
 behavioral properties, and then we can impose different conditions. Does it have to be a
 representation of the relevant phenomenon? Does it have to be a conscious experience
 of the relevant phenomenon? Does it just have to involve some kind of structured process, structured
 reasoning, and so on? I guess I'm inclined to think there are multiple notions, what I call
 inferential understanding, phenomenal understanding, correspond to different degrees of internal
 constraints. But were you suggesting, when you say you take a Dennett-like approach,
 that suggests sympathy for the purely behavioral? Yeah, the first thing you just described sounds
 to me pretty, yeah, would be pretty a satisfying explanation. But yeah, I get that not everyone
 has those intuitions, and I'm kind of curious. You might take the line, if a walk's like
 understanding, and a talk's like understanding, and a pack's like understanding, then it's
 understanding. And for some mental states, for consciousness, that just looks, well,
 too operational to be a good criteria for consciousness. But understanding is such a
 loosey-goosey notion, with so many different anchors and ordinary practice. I think there's
 at least some aspect about practice, we're talking about understanding, which is the behavior that
 matters. Yeah, making room for the fact that it could be the case that you think this looks like
 understanding, but then it turns out you do some additional experiment, and it turns out the language
 model wasn't really understanding, it was just picking up on some cheap trick. If you could kind
 of convince yourself you've ruled all those out, then to me it seems like the behavioral
 explanation would be pretty satisfying. Yeah, but this at least brings out there's two different
 kinds of critiques of the claim that language models can or could understand. One is that we
 find actually behavioral limitations, all the famous glitches and hallucinations and so on,
 that some people think, okay, and it shows they don't understand because they're reasoning and
 responding so badly. And the other kind is even if they turned out to be fully impressive along
 those lines, someone like an analog of John Searle, someone like John Searle might say, ah, still,
 they're not understanding, they're merely behaving. And I take Stephen to be at least somewhat
 sympathetic of that line. Yes, I'm sympathetic to that line. I don't think it's a behaviorist line,
 it's an empiricist line, I suppose. And what Dan Dennett says is that once you've done all of the
 empirical work and answered all the empirical questions, what other questions are there,
 he says, and you're supposed to answer none, you've answered them all. Of course, there is one,
 and it's the heart of the hard problem. There's also a fact about us, by the way, the hard problem
 and the otherwise problem, as you know, are not the same. And the connection with them are simply
 is loose, because what we can establish from our own experience is that we really do feel,
 each one of us really does feel. And that's left out of the Dan Dennett account completely. That's
 what makes it a hard problem. You're not supposed to disagree with that, Dave. Oh, no, I agree with
 you completely. The only thing I say is that I do think some of our mental notions are more closely
 tied to consciousness, while others are more closely tied to behavior. So feeling pain,
 obviously seems very closely tied. That's what we really care about. Action, kind of being act,
 even if it's not conscious. Well, arguably, there's a sense of action, which is mostly tied
 to externals. And understanding, I think, is somewhat in between. There is this phenomenal
 strand in the notion of understanding that turns on the feeling of understanding. But there's also
 this use-related strand of a notion of understanding that really turns on what you can do with it. And
 I took Kyle to be not necessarily expressing the view that all that matters to the mind is what you
 can do, but at least where understanding is concerned, what really matters is the use.
 I don't want to take a larger part of this discussion. Jan, you wanted to ask a question.
 Yeah, sure. Can you hear me? Yes. What is that amazing location?
 Yeah, greetings from Finland. This is my favorite office place. I have a sack here.
 Thank you for an excellent summer school and a really simple, excellent presentation.
 I would like to go back to the other mind's problem and the heart problem, because
 I've always simplified the heart problem as a question, reduced it to kind of a question of
 the other mind's problem. That if a system tells me that it has phenomenal states, then
 I can only take it as a face value and that's the kind of it. And I don't know how, and also the
 other, like the easy problems are then, they are not a problem. Or what do you think are the
 limitations of this view? Well, yeah, I mean, as Stephen says, there's a loose connection between
 the heart problem and the other mind's problem. The heart problem is a problem of explanation.
 How do we explain consciousness, especially in terms of physical processing? The other mind's
 problem is a problem of knowledge, you know, which other systems are conscious and how do we
 know? But the fact that there's a heart problem makes the other mind's problem difficult and vice
 versa. That said, yeah, so part of the other mind's problem tied to the heart problem is it's hard to
 get conclusive evidence of consciousness in any system other than yourself. You know, I'm basically
 a Cartesian. I know that I'm conscious and I take it that other people are conscious, but I think
 my evidence for that is somewhat indirect. And, you know, there's at least this hypothetical,
 logical possibility that everybody else is a philosophical zombie. Now, there are a lot of
 hypothetical possibilities out there. It doesn't mean we have to take them seriously. You know,
 there are questions about whether the physical world exists. What we do in physics, I think,
 is no physicist really has a great proof that the external world exists. I think, you know,
 there are things you can try. But what people do in practice is we take it that our perceptual
 experience is a good guide to the external world, except when there's some reason to believe
 otherwise. We take our perceptual experience at face value, build up physics and the rest of
 science from there. But I think likewise, in the science of consciousness, what plays that role is
 verbal reports. We basically adopt it at least as a working assumption that when another human being,
 at least, says they're conscious or says they're conscious of something, then we have good reason
 to believe them to take that as an accurate report of their consciousness, at least unless there's
 some particular reason to think something's gone wrong. I think that's roughly analogous
 with the situation in physics. And that works pretty well for getting a science of consciousness
 going in humans, at least, and then people extend that to non-human animals and so on by looking,
 say, at the relevant neural or computational basis underlying those reports in humans and
 see if that can be present in animals without those reports. I think of that as derivative,
 ultimately, on work from the human case. But yeah, when it comes to language models,
 suddenly this tried and tested reliance on verbal reports seems to at least potentially
 break down because it looks like there's an alternative. The reason why humans report
 these things, most people want to say, is actually tied to the fact that they genuinely are conscious.
 But it looks like there's an explanation for why language models are producing these verbal
 reports that has nothing to do with their actually being conscious. And it's got rather much more to
 do with them having read a whole bunch of text and being trained to replicate patterns in that text,
 which already involves claims from humans like I'm conscious that don't give you reasons to believe
 that. So yeah, I think this is part of what makes... And furthermore, we also don't have
 neuroscience to rely on because language models don't have neurobiology. So yeah,
 two of our central methods then for ascribing consciousness, verbal reports and neurobiology
 don't seem so useful in language models. There's still the possibility that we could have an
 understanding of the computational basis of consciousness or the informational basis,
 tie that, postulate that that's the key to consciousness and see if that might be present
 in language models. I mean, the hard problem means that any of these theories are very difficult.
 Both the hard problem and the epistemological problem of other minds means there's always
 going to be elements of speculation to any of these theories. And we're unlikely to get
 conclusive proof of the existence of consciousness in any particular system. But I do think for the
 epistemological problem of other minds in language models, it may be that computation and information
 are going to play a very central role, maybe even more so than they do for non-human animal
 consciousness. Just for the record, would you mind telling everyone why the hard... what the
 hard problem is and why it's hard? Very short. Yeah, the two word version of the hard problem
 is just explain consciousness. A somewhat more involved definition is just explain how and why
 it is that conscious experience arises from physical processes, if it does. And what's hard
 about that? And what's hard about that is that we've got our standard methods of explaining things
 in neuroscience and cognitive science, which involve finding a mechanism that plays a role.
 And for, say, explaining language and memory, what do we ultimately need to explain? It's some
 roughly some behaviors, some behavioral capacities, some things that the brain does to explain those
 things. Basically, it's a problem about explaining the performance of some function, behavioral or
 otherwise. And we solve those problems by finding a mechanism, a neural mechanism or a computational
 mechanism. And for some aspects of consciousness that works quite well, for the so-called easy
 problems of consciousness, which are problems, say, of perceptual discriminate, how does perception
 discriminate objects in the world? How does the brain integrate information? How do we produce
 various verbal reports? Those are ultimately problems about various quasi-behavioral functions,
 and we can explain those in terms of mechanisms. What's distinctive about the hard problem of
 conscious experience is it doesn't seem to be that kind of problem. It's not a problem about
 how it is that the brain does various things or produces various behaviors. Those were the easy
 problems. It's a problem about roughly about how things are experienced, how and why things are
 experienced or how they feel. And it sure looks like, at least in principle, you can explain all
 those functions, every behavioral function in the vicinity of consciousness without having answered
 the question, why is that accompanied by experience? So that's the difference between the hard
 problem and the easy problems. Those are various strategies for trying to problematize the gap.
 Dan Dennett's favorite version of this was always to try and turn the hard problem of why we
 experience various things into the easy problems of explaining why we think we and say that we
 experience various things. That's what I call the meta problem. Dan thought the best way to treat
 the hard problem is to dissolve it into the meta problem, which we can then try and explain using
 fairly standard methods. But for many people who are realists about consciousness, there's something
 about the hard problem, which is harder than the meta problem of explaining. There's the problem of
 why we experience these things, which is harder than the problem of why we say we experience these
 things and can't be collapsed into that problem. Another reason it can be collapsed is the probably
 the reason why you don't see clearly why it is that I say to understand is that it feels like
 something to understand. I would add it feels something to believe. Offline beliefs are like
 LLM, you know, the stuff going on. But if there is a brain process that is causing you to believe
 something and you're believing it, then you're feeling that you believe it. And so they're the
 same thing, actually. Yeah, it's interesting. Online believing, I think of believing as kind
 of by its nature, a dispositional or somewhat offline state, the current thing that happens
 with feeling isn't exactly so much believing as judging. I judge that the, you know, the conscious
 occurrence state is judging that something is the case. You know, belief is kind of an ongoing
 offline state. And judging is the is the online occurrence state. And maybe judging is more
 obviously conscious than belief. Well, and just as the Hungarian German example with with Holger
 showed that you can have both sides, right, something that you that you do understand and
 that you don't understand. And with beliefs, you have the same thing. There's some things that
 do you believe that X never mind that it's a judgment, it feels like something to be believing
 that X and it doesn't feel like anything. Or it also feels like something to believe that not X,
 but it doesn't feel like anything to not be believing X or not X. Does it feel like something
 to believe that Harris is in France when that's not sort of currently running through your mind?
 It's just part of your background. When it's offline, it's it's it's chat GPT. I see. Okay.
 I guess I think the belief by its nature is is offline or dispositional. And when it becomes
 online, that's that's judgment. But maybe this is verbal. Maybe you can use the word belief
 for what happens online. Other questions from other people, please.
 We have a captive Dave Chalmers here. Come come up. Yeah, I've got a discussion point. I think we've
 been sort of overlooking the fact that these aren't just text models. They're also reinforcement
 learning eyes. They're actually a product of a text model and a reward model. I think that's
 actually quite relevant to these questions about like, if you know, pleasure and pain, these are
 related to reward signals in humans. To some extent, you can see every token is generated
 by the base model and the reward model. And yeah, I don't know, I think this is overlooked in a lot
 of the discourse on the treating these things as just text models. They're really not anymore.
 The award model is tied to, I mean, to the reinforcement learning aspect specifically,
 as opposed to the self-supervised. It's not necessarily there's methods other than actual.
 I mean, it's, it's, there's like, there's different methods. Now, there's RLHF, there's DPO, PPO,
 all these various acronyms. But what it always comes down to is that the probability of any
 token is the product of the base model probability, just the raw text model, and then some reward
 model. It's some exponential reward model. Okay. So the pure model prior to reinforcement learning
 isn't going to be subject to this analysis. No reward model there, but the moment you've got
 RL on top of it. Yeah. And the reward model is a, it's raising, it's modulating the probabilities
 of tokens on the fly. It also during the reinforcement process, there's, there's reward
 signals, the positive or negative. And ultimately you can, I mean, ultimately, I think if you want to
 model human communication, there'd be an equivalent of the reward model, which is like trying to model
 your interlocutor and you're trying to get information to your interlocutor. It seems like
 any well-formed sort of reward model on a language model is going to also go in that direction.
 And that seems like a whole different thing in terms of like thinking about what the internal
 states must be. Yeah. And so the thought might be that it turns out that once you bring in
 reinforcement learning, maybe this actually brings a whole new class of mental states
 into the picture that are pure self-supervised language model, including affective states like
 pain and pleasure. Yeah. Well, I mean, there's, there's models of affective states in terms of
 like, it's like your internal estimate of what the reward should be before you've actually received
 real reward from the environment. Yeah. There's like, there's like in the moody reinforcement
 learning where you have a reward estimator that gives you something you can interpret as an emotion.
 And so, yeah, I think, yeah, it's just an overlooked part of the discussion thing.
 No, you're right. And I mean, in general, trying to figure out the computational basis of
 affect ought to be incredibly important because after all, so many people think that when it comes
 to moral status, what really matters is not just consciousness, but affective consciousness, pain,
 pleasure, suffering, happiness. So from that perspective, it's extremely important to figure
 out what the computational basis of affective states are. And my sense is the literature on
 this is surprisingly sparse. I mean, there are a few different models of, you know, computational
 models of affective states, but there's not a consensus model here. My former graduate student,
 Rob Long, who's now doing a lot of work on language model consciousness and sentience
 has been done a review of this. And yeah, it's surprisingly sparse. And I totally agree that,
 yeah, the reward models in reinforcement learning are a very natural place to look
 for affective processing. If you take this seriously, yeah, it's interesting.
 Pure self-supervised models, no affect, they're what we call philosophical Vulcans, maybe
 perception, cognition, but no positive or negative valence or feeling. Once you start training them
 with reinforcement learning, that is the place where pain, pleasure, and valence come in. Boy,
 that's a very interesting speculation of true. Louis. Hi, is it working? It's okay. He hears
 without it. Oh, good. Okay. So my question is a bit on the, you know, how to apply the notion
 of competence to an LLM. I kind of try to get generative LLMs working in, I would say, industrial
 context, if you will, where, you know, you have like clear applications and you try to have like
 reliable results. And this is like surprisingly difficult to the point that, you know, it becomes
 very hard to have any confidence in any competence of the model because sometimes just slight changes
 in the prompt or slight changes in the context will make that, you know, this machine that you
 thought was competent for some kind of reasoning or inference is no longer competent in that
 context. And the results, like there is a sense in which like, you know, we're trying to say like,
 okay, does going back to your presentation, does an LLM understand? Is it capable of reasoning? If
 it's capable of reasoning, then we can draw an inference about understanding. But even just,
 oh, okay, so even just if you're trying to get the, to get an assessment on that front,
 I get the impression it's very, very difficult because the, we're used to looking at competence
 of humans. So often we'll know what kind of questions, what kind of tests we can do for
 that. And then from that, we can draw a conclusion about a wider competence. But the experience in
 LLMs is that very often these, the contours of this competence are so unusual, so unpredictable
 that it's very hard to predict how they're going to behave in slightly different context.
 In that sense, I wonder what can we, can we say about like, is it even the right language to
 talk about reasoning competence? Or is it simply a case of things being very context dependent?
 And well, anyways, the question is, sorry.
 Yeah, that is, that is super interesting. And yeah, the remarks based on
 industrial experience are very useful. I take it the underlying worry is something like the fragility
 of language model capacities. They may be they'll work in some context, fairly limited, and then
 trying a slightly different context. You don't get, you don't get the same kind of useful,
 useful response and things go wrong. And then the question is, what to say about mental states,
 like reasoning, understanding, belief, whatever, in the presence of fragility? In the case of humans,
 at least, if someone can do something, say, you know, 25% of the time, and then 75% of the time,
 they can't do it, we might say, okay, on the one hand, we say, well, they're not very good at it,
 you know, they're not necessarily an expert. But still, they've got some capacity there. The fact
 that they can actually do it 25% of the time reveals some kind of underlying capacity. If I
 understand, I'm not terribly good with Hungarian, but you know, but I can get, you know, some,
 about a quarter of the time when people say something to me, I know how to respond to
 appropriate. So that, that suggests some kind of underlying capacity for dealing with or understanding
 Hungarian. So I guess one way to think about it is if a language model shows some capacity,
 just some of the time that already suggests some pretty significant underlying capacity. I mean,
 again, there's the whole, you go with the glass half full, or you go with the glass half empty.
 And I think, you know, if you say, if it's a 50% of the time, to get to do something right 50% of
 the time, you got to have some pretty serious capacities. It's not the full scale capacity of
 the glass being completely full. I mean, if it doesn't just, you know, one time in a thousand,
 you might say, well, it was just pure luck. And it doesn't really suggest them.
 Okay, I'll do the test on you later. Christian Navier has a question. You made it when you only
 spoke once for some time in this, but it was always very, it was always that one time was
 extremely good. So I'm glad you, I was going to ask you even if you didn't raise your hand. Go ahead.
 Well, now you've raised the expectations. So I wonder about the sort of, ever since Eliza,
 we've known the dangers of trying to anthropomorphize our conceptions of these models.
 You know, as Stephen can mention, Hungarian, I was Belgian, or as the customs officer told me last
 time I was in Montreal, you are still Belgian. They're very prickly about those things. But,
 you know, Magnet had that painting of, you know, a pipe saying this is not a pipe, right? This was a
 representation of the pipe and those language models, they're representations of the knowledge
 out there on the web. And I think making the default assumption, like in that case, well,
 if they can speak 25% Hungarian, let's apply the model of a human who can speak 25% Hungarian and
 try to extrapolate. I mean, I think that's very dangerous because the default assumption that,
 well, they exhibit a behavior that looks like us, therefore the internal mechanics
 must be sort of like us. I think that really seems like an unwarranted assumption.
 I guess I think that the behavior doesn't guarantee the underlying mental states by
 any means, but it does at least get us to the point where we should take that hypothesis seriously.
 Once you see the very impressive behavior that language models exhibit, if it turns out to be,
 you know, of a kind in various respects with behaviors that human produce, then we have two
 hypotheses. One is that they actually have some of the relevant underlying mental states and
 capacities which are driving that behavior. And the other one is no, they're doing it some other
 totally different way. So yeah, you favor, it sounds like you favor the second kind of hypothesis,
 perhaps because you think these capacities are merely capacities to represent and describe,
 not really to behave. But I don't know, I think as these language models start describing more,
 producing more and more agent-like behavior, which may require things like them to be more embodied
 than they currently are and so on, there comes a point where, you know, just say we keep extending
 them, we have the fully embodied, we have a robot interacting with us in a very, very agent-like way.
 Do you still at that point want to say it's a category mistake to think of this being as
 believing, desiring and understanding?
 Well, so here's an example where I think that it's a projection of, well, if we keep extending,
 this will happen, which again, is really embodies our expectations of what would happen with humans.
 Here in Pittsburgh, we had the Uber Research Center that was developing autonomous driving.
 And six, seven years ago, they were everywhere up here, up and down the East end of Pittsburgh.
 And they had gotten to the point where they were, they still had human passengers there,
 as what they told us was just, oh, it's just supervising, the robot is really driving on
 its own. And then there was a well-publicized accident. And then they took the robots off the
 road and said, well, we just need to fix a few things, they'll be right back. That was seven
 years ago. They haven't been back. And Uber had pretty much abandoned their efforts. And I think
 the assumption was that, well, if you had a human driver that was exhibiting the considerable
 competencies that they were exhibiting there, surely fixing those few problems would be a
 straightforward matter. And it's sort of for a human driver that should be considering the
 particular underlying model of how they operate. It turned out, of course, practically speaking,
 to be very difficult for them to get from that state of competency to the sort of the next state
 there. So again, whatever that is, and they're sort of doing more of the same, a larger training set,
 more computing power, et cetera. Certainly in the few years, that's a significant amount of time of
 future development, it hasn't gotten to the next stage. So the expectations that were at least
 publicly stated there turned out to be incorrect. And in particular, the sort of the development
 trajectory instead of psychological term that would have been expected hasn't projected in the
 same way as humans. So I've sort of learned from experience that it's dangerous to make those
 extrapolations, I guess. Yeah, that's totally fair. And yeah, autonomous vehicle capacities
 appeared to at least to hit a wall. There was a barrier, which they found it very hard to get
 beyond. And it remains completely possible that language model capacities could hit a wall, too.
 I mean, of course, there are all kinds of limitations right now. There are mistakes or
 things they can't do and so on. I think we're not yet at the point of knowing that there's a wall
 there. There are some things that people think are walls. The Gantt and LeCun thinks that planning
 is going to be a wall, it's going to require new methods and so on. But I think we're at least not
 at the point where there's a clear wall that they've hit in the way that autonomous vehicles
 have hit. On the other hand, you're absolutely right. It's an open possibility. There could
 turn out to be systematic walls and systematic barriers. And some of the things, some of the
 things they can't do now will turn out to be things that no system in this class can do.
 I don't really mean to be taking a position on that. If I had to guess, I'd say we don't have
 clear evidence. So I would give it at least a good 10 or 20 percent credence that extensions
 of current methods might get us to pretty general intelligence. But if it turns out that there is a
 wall, then what does that mean? If it turns out there are some fundamental capacities,
 these things lack, does that then mean that they can't understand or reason at all? Or does it
 mean that they, well, they do understand and reason some things, but those things are limited?
 And I think all that will very much depend on the capacity of the wall. But I guess when you're a
 glass half full person, I don't think of this as being, yeah, just keep doing more of the same,
 and we're sure to get to human level capacities in 10 years. It's more like, hey, there's a
 serious possibility that this is coming, that we're not yet in a position to rule out.
 Well, I'm sure it's coming sooner or later, I suspect later. And I suspect with systems that
 are considerably augmented and more complex toward the sort of the next word prediction that's now,
 I don't want to say that can be done. I mean, I think it will be done. It's a matter of enough
 time and breakthrough. I just don't think it's necessarily of the same sort of straightforward,
 incremental trajectory. So here's the thing, back in 2017, I chaired a panel at a meeting on
 beneficial AI and Asilomar, I chaired a panel, which had Demis, Asabas, and Yann LeCun, and
 Yoshua Bengio, and a few other people were on the panel. And I asked everyone, how many fundamental
 obstacles do we need to get over to get to? How many hills or mountains do we need to get past to
 get to artificial general intelligence? And the consensus, you know, it's like, it's not a well
 formed question, of course, but you know, people like Demis were giving answers like, yeah, less
 than 20. And now you hear that was, of course, before the age of language models. Now you ask
 that question, again, in the age of language models, and people say things like, yeah,
 maybe four or five. I think if we're only four or five mountains away, then that's a whole lot
 closer than we were, you know, 10 years ago. And that's not to say those aren't significant
 obstacles. And maybe it does, maybe it turns out there are going to be another 10 or 15 mountains
 to cross. But at least my sense of the time scales has shrunk a lot compared to 10 years ago.
 All right. I've promoted a few more people to raise their questions. Go ahead,
 if you've just been promoted. By the way, are all the questions here by definition for me? I mean,
 I'm happy to have questions to others as well. Not by definition, by happenstance.
 Yeah. Oh, for freedom, I'd go ahead. Do I have to do anything?
 We should be allowed to talk and promote to panelists. That should make it possible.
 Hello. Can you hear me? Yes. Yes. Yes. Hi. I was wondering
 what we would need to add to, for example, to reinforcement learning
 in order to make the reward phenomenal.
 Earlier came the question that
 we could concentrate on the reward or punishment. And I wonder,
 do we have to add something to the functional role in order to get the models to experience
 phenomenality of that reward or punishment, for example? People think in the case of humans and
 non-human animals that there's a difference between no deception and pain and feeling pain.
 No deception can happen involves, you know, processing of negative information and some
 motivation to avoid and so on. But I think the standard view is that can take place non-consciously
 and that, you know, no deception may be primitive. And maybe some people think at least that no
 deception can exist in beings without the capacity for consciousness. Then the question
 is, what is the extra thing that has to be added to no deception to make it experienced as pain?
 And here there are different hypotheses. You know, one view would be you need some kind of self
 monitoring. This is what the higher order theorists say. Pain requires a higher order
 representation of your no deception. And a less demanding model would be something like to be
 conscious that needs to make it into, say, a global workspace. So a very localized no deception
 process of reward or punishment will not make it into a global workspace that controls all behavior.
 So it won't be conscious. But yeah, but the question here kind of does come down to the
 question of what are the physical or computational correlates of consciousness, which is,
 you know, something we don't understand very well.
 Okay, there's several more people. One side remark it has to be made about no deception and pain
 is that that's a bogus problem. The evidence about the pre-frontal lobotomies that were done in the
 series that they were fashionable was that people that had intractable pain and then were given the
 right limbic system lesion continued to feel the pain. The only thing that disappeared was that it
 hurt, which just goes to show that it's not true that all sentience is either positive or negative
 valence. It can also be neutral, like listening to an oboe or to a scene read. But okay, so that's
 just a canard I'd like to put. It's like the Walt Disney canard that you've got a lot of unused
 behavior capacity, brain capacity. Do you want to take it, Friedemann?
 Yeah, thank you. I just wanted to make another attempt to argue that maybe GPTs or some other
 language models are not as good as they seem. Indeed, I fully agree with you that they can
 produce surprisingly high-level performance in translating between programs and in completing
 texts. But on the other hand, completing texts could be just a process like a Markov process
 where just n-grams, k-grams have been stored and used in order to make the prediction and
 also the translation between programs doesn't strike me as something really over-exciting
 because usually there are symbols in these programs that are unique and also the indices
 are very well defined. One could look up in books and tables how to translate C into Fortran,
 for example. So if a language model that picks up everything that is available on the internet,
 then it's not surprising that they also eat and digest those lookup tables and perhaps apply them.
 But what is on the other hand, on the negative side, really exciting, for example, in Alessandro
 Lenci's talk this morning, it became apparent that they really do not show very basic evidence,
 do not give basic evidence of one would otherwise be inclined to consider knowledge. For example,
 the question is does a rabbit have ears? And do ears have rabbits? Would likely both receive a yes
 without realizing that one plus the other one is not possible. So if there's such a basic
 fallacy, so to speak, and this is frequently done in a system, can we really take it seriously as a
 cognitive entity? I don't want to speak about consciousness, but is this intelligent behavior
 probably more dumb behavior I thought? I just asked chat GPT, do ears have rabbits? And it said to me,
 it seems like you might have meant to ask if rabbits have ears. Yes, rabbits have large
 distinctive ears that they use for hearing and regulating their body temperature. If you have a
 different question in mind, please let me know. Okay, Professor Lenci's paradigm was such that
 the question was something like, please judge the truth of the following sentence. And then
 you have the two different sentences as two different inputs. And the answer was restricted
 to yes or no. It's of course that if you throw something at it, it might come up with a more
 frequent sequence. This is interesting. I only caught a little bit of that talk. Obviously,
 there are limitations on models. On the other hand, I'm also impressed by many of the limitations
 that people point out by a year or two down the line no longer seem like quite such serious
 limitations. So yeah, very much an open question, how systematic current limitations will turn out
 to be. Yes, thank you. Now, Till, you already asked your question and Edward, you didn't,
 right? Or do I have it backwards? I already asked my question. Till, you didn't ask your question.
 Till, shoot up. I did ask my question, but yes, thank you. Both of you asked your questions. Okay.
 Yeah, I haven't asked mine yet. Oh, you have. Go ahead, please. Okay, so this, Dave,
 I really enjoyed the talk. I find myself not, so I want to say there's something special about
 human understanding that we don't see in things like large language models. And let's assume that
 it's not a difference in what you're calling use understanding. So we've got, in terms of use
 understanding, we have, let's assume large language models in humans, same. But I don't like to rest
 my case on this phenomenal understanding stuff. It doesn't seem to me that what's special about
 understanding for humans is like a way it feels. I'm not even sure understanding feels like anything
 at all, in many cases. Now, I don't know how to really put my position without being completely
 circular question begging. But what I want to say is something like what's special about human
 understanding is when I say two and two is four, like I know what I mean when I'm saying that.
 So I'm sort of using the very notion that that's why it's circular. But I want to say that that
 doesn't have anything to do with the way it feels. But that's the special thing. And also, my view is
 that Searle in the Chinese room type case is sort of gesturing at that idea. And I wonder what you
 say about that. You know what you mean. Yeah, it's interesting. What is, and that's not a matter,
 again, knowledge might be cashed out as some kind of knowledge how ability to use and looks like
 language models are okay at that might be cashed out as a subjective experience, but you're
 excluding that. And it's this other thing in the middle that I'm trying to maybe it's a matter
 having the right kind of internal representation, or acquaintance with the meaning, but that doesn't
 involve subjective experience. Is there anything more you can do to flesh out this crucial missing
 thing? I'm not sure. I mean, I was sort of asking for some help on this myself. But it's
 it feels like something to know to you can't get out of it that way.
 Well, but I feel like I could take like, I could lose the feeling, take away the phenomenal
 experience in theory, at least, and still have this kind of grasp on the meanings of the things
 that I'm saying and thinking what I'm saying and thinking them. And I can always say that was know
 how fine. We're not talking about know how we're talking about knowing and believing and understanding.
 I can kind of get in the mood like there is this third thing. It's like, yeah,
 not the know how, not the experience. But yeah, like there's a third thing, which is like the
 intentionality or the rationality or something. And then I wonder what does that actually come
 down to? Maybe it's like being subject to irrational norms or somehow your know how is
 more primitive and behavioral than that. But I don't know. I mean,
 it's very hard to pull that. I mean, because consciousness, of course, goes along with
 those things. But yeah, then you kind of come to the question, could a philosophical
 zombie have this thing without any capacity for consciousness? Could it nonetheless have
 the rationality or the intentionality, which nonetheless comes down to something more than
 good behavioral functioning? And I don't know, just when I think I've got a grasp on the relevant
 notion, I find it tends to slip out of sight. So I tend to be, you know, roughly there's the
 functioning and there's the consciousness. And I don't really have a room for the third thing in my
 ontology. But if you can figure out a way to articulate it, that'll be fantastic.
 Let the third thing be Anna Strasser, go ahead.
 Yeah, thanks a lot for inviting me to the discussion. So my question is a little bit
 more technical. So Dave, I'm wondering why are you so optimistic in the sense you are optimistic,
 even you say it's only half full, the glass. And then I'm thinking about LLMs. And I ask myself,
 which processes in their production are causally responsible for which abilities? So in the
 beginning, we had only the pure transformers that only experienced unsupervised learning,
 and seemingly produced weakly emergent properties. I think there I would agree with you.
 Now there are no more models that are not tweaked by all kinds of reinforcement learning with all
 the click workers in the global thousand. And on top of that, there are hybrid models that have
 something like constraint checking modules. And I would be very curious to hear whether this
 newer models benefit from the additional processes in their production. And maybe this is the cause.
 Why are you more optimistic than I would have supposed?
 I don't think it's the cause. I'm already, you know, I was already glass half full ish about pure
 language models. My talk at NeurIPS was two days before ChatGPT came out in November 22. And
 I think of ChatGPT as the big public debut of RLHF reinforcement learning through human feedback on
 top of these models. So I guess just historically, maybe the glass half full thing is just partly a
 matter of character or personality, even when, you know, I mean, I tend to focus on the possibilities,
 even where awful things are, are concerned. So, you know, there's certainly plenty of room for
 feminism, but for pessimism. But if it turns out that there's even a 25% chance this is going to
 result in human level mental capacities, I think that's kind of remarkable. And I, and I
 focus there. But yeah, it's a really interesting substantive question. What difference all the new
 things that get added on on top reinforcement learning, prompt engineering, constraint checking,
 and so on. And I haven't really meant today to be focusing, I haven't really focused on any of those
 things, especially in my analysis so far, the point that came up earlier about reinforcement
 learning, possibly playing a really crucial role in affective processing is really interesting and
 not something I've, I've, I've tried to analyze. But yeah, that is a, that's an example of a point
 where, yeah, where reinforcement learning could, in principle, bring in something totally new tied
 to reward and motivation. But yeah, but if you just, but if you're asking, do those things playing a
 role in the optimism of my analysis? Not exactly. But I do intend my analysis to be consistent with
 there being all kinds of extensions of, you know, my optimism is not that in 10 years time, pure
 language models will have these mental capacities, or that in even let's say, multimodal models will
 have them, but rather that extensions of these models will have them. And that's meant to be,
 I take it all the things you're mentioning, I would count, I would bring into the class of,
 of extensions, but of course they are, you know, proper analysis would
 really require getting into the details. Do you think those things, do you think those things
 make a difference? I think they make a huge difference. And I think we should have separated
 discussions about whether pure transformers have understanding, whether pure transformers,
 which have been tweaked by reinforcement learning, have understanding and whether
 pure transformers with reinforcement learning and constraint checkers like Claude have understanding,
 because I think the architectures are really different. And if you're thinking about what
 makes the systems having certain human-like capacities, we should also look at the
 architecture. And so I interpret your answer as in the following way. You think that in the future,
 the pure transformer will not win the game, but you think that pure transformer, which have several,
 maybe kinds of extension in the architecture, which we even cannot conceive of right now,
 might win the game. It's more a matter of probabilities and winning and losing. I've got
 a certain probability that a pure language model, not necessarily a transformer, but a pure language
 model in an appropriately advanced state might well be conscious at some level, not in a totally
 human-like way. I mean, probably couldn't be in a human-like way because humans, because sensory
 experience is so central to being a human-like thinker. But yeah, the more you add, the more
 you make them multimodal, the more you bring in possible sources of motivation and reward and so
 on, then the closer you get and the higher the probabilities become. So I guess I think of it
 more in terms of probabilities and in terms of a binary in or out. Okay. Alina Gutierreva,
 and probably this is the last question. Hi, everyone. Thank you for a fascinating
 discussion. So my question is, the development of AI might lead to AI systems that closely
 intertwined with human consciousness, human activity, that it's extendedly conscious.
 So AI systems on its own might not be conscious, but when we interact with them or there are some
 tasks that we use, they become conscious because we use them. So these AI systems are essentially
 becoming part of human self, and these can be viewed as advanced digital means for health,
 education, and other purposes. So what moral issues might arise in these scenarios? And yeah,
 it would be great to hear the view of David, of course, but also other panelists. Yeah,
 it would be interesting to hear your opinion on this. Thank you. Yeah, no, this is super
 interesting. It's kind of tied to the question, which I don't know if it was you or somebody
 else asked in the chat about the degree to which these systems can become part of a human's
 extended mind in the sense that Andy Clark and I described back in the 90s where a notebook can
 become part of your extended mind. Your smartphone now, paradigmatically, is part of your extended
 mind. So much of my memory is now on the phone and planning, navigation, and so on. I gather that
 Apple just made their big AI announcement today, but I was too busy in the seminar to follow any
 of the details. But I take it that open AI, language model technology is now coming to our
 iPhones. And if these smartphones are already extending our minds, then I take it that already,
 I think, very simple autocomplete in a smartphone texting system is already counts as an extension
 of our mind. We've got some extended spelling and writing abilities produced through the autocomplete,
 which might be regarded as capacities of the me plus phone extended systems. Once you've got
 language models in there, we'll have autocomplete on steroids, and that'll be just a simple case of
 extending our minds. Likewise, once AI plays a role in search,
 that will also be extending our minds. So I take it that what's coming is that as we start using
 generative AI more and more, it's going to become a very robust and hopefully reliable
 way of extending our minds, but certainly something that we will rely on, which is one
 of the conditions for being part of one's extended mind, that a system be reliably coupled to you and
 one that we rely on for many purposes. Will it extend our consciousness? You did frame your
 question in terms of consciousness. My view has always been that the tools that extend the mind,
 like notebooks, smartphones, and so on, don't obviously extend one's consciousness. The
 basis of consciousness may still be internal. To get to extended consciousness, you may need
 something that taps in a bit more directly into, say, the neural basis of consciousness.
 Appropriate brain-computer interfaces, for example, might actually get to the point where
 the basis of one's consciousness is outside one's head. That said, certainly relying on AI models
 could certainly change the contents of one's consciousness, just as certainly notebooks
 and smartphones and all the technology we use has greatly enriched the contents of our consciousness.
 I fully expect that AI will do that too. Will that make the AI systems themselves conscious?
 No. All this is actually consistent with the stochastic parrot view that only humans are
 conscious. AI systems extend human consciousness, but are not objects of consciousness in their own
 right. Of course, if it's possible, the AI systems may themselves eventually become conscious,
 and this opens up the possibility that eventually we may end up being extensions of the AI
 system's consciousness. It may not be quite as bad as it sounds. I think I'm an extension of
 my wife's consciousness, and she's an extension of mine, and those things can happen simultaneously.
 It may well be that human-AI relationships eventually, just human-AI communication may
 eventually take that form. I know you're asking everybody else for their view on this question as
 well. Thank you. That's a really interesting answer, and the extension of that we might be
 the extension of AI, and then that we extension of each other consciousness and so forth. Yes,
 thank you. The others would be wonderful to hear your views too.
 Any other questions, comments, complaints? Yes.
 Thanks for the really interesting presentation. So a quick follow-up on the last question.
 If we think of for the both Susan Schneider, which presented at the beginning of the school,
 global brain argument, which is kind of dystopian radicalization of this
 social artifacts enabling us to further develop integration with the devices. So the argument
 goes just with T1, hyperintelligence is about to come. It's about Nick Bostrom, superintelligence
 kind of like systems. P2, global brain, we are all part of the global brain, which is a kind of
 radicalization of the internet and the capacity that enables us to do. And premise three, the
 nose, we are both in this conclusion being we're all part of global brain and everything becomes
 integrated socially. So maybe if you want to comment or anybody on the panel on the
 social and political consequences that might happen with the rise of the AI industry. Thank you.
 Did you want to know about global brain or about AI industry?
 I believe they're connected. Okay. Anybody, any takers? Anybody believe in global brain?
 I take it this is like extended cognitive, this is distributed cognition
 on steroids. Extended cognition when an AI extends a human mind, there's collective cognition. Say
 when a group of people on a ship are working together, is the global brain idea that roughly
 the whole earth's population can constitute a global mind and then AI with AI as part of it
 as well or is it? Yeah, it's a Gaia ganglion consisting of us. Anybody go for it? Yes.
 So going back to the problem of the extended mind. My question is, I can visualize I can understand
 why we talk of extended mind when we're talking of very simple cognitive tasks that we don't
 process out of the brain. We don't process out of the brain. We don't process out of the brain.
 The simple cognitive tasks that we don't process ourselves and we let the smartphone process,
 for example. But when we have this level of connections where some cognitive processes
 don't even constitute the equivalence, the equivalent of what we would do because they are
 way more elaborated in a way. Would it be more interesting to look at actor-machine interaction
 as a form of communication rather than a joint cognitive action of sorts?
 State, I mean the joint cognitive state. Yeah, go ahead. Yeah, I think all these are useful
 ways of looking at it and merely looking at something as a case of the extended mind,
 which is like a joint cognitive state, doesn't preclude the possibility of looking at it as a
 kind of embedded cognition with one mind and a tool. And it doesn't preclude looking at it
 as a case of distributed cognition with multiple minds. I think all of these perspectives are
 consistent. Andy and I talked about there being a Necker cube effect. You can flip the cube to see
 it as a case of extended cognition, but you can flip it back to being embedded cognition or
 distributed cognition. Actor-machine analysis, I guess I think of as a kind of distributed
 cognition with an actor mind and a machine mind, but it might also be a kind of embedded
 cognition. So I think all of these are available. And I think you're right that
 if the extension becomes too sophisticated, if it's far more intelligent than the original
 being that was being extended, then the extended mind cognition can become something of a stretch.
 Maybe at a certain point, you want to say that the extension itself is the mind. I mean, the
 extended mind case works best for a quick and easy and automatic processes. Once you've got an AI
 running the world, then does it really count as an extension of my mind or do we count as an
 extension of events? I don't know. I see no hands. I see no faces asking. Okay,
 I think this would be a good time to give everybody a thank you for your panel contribution and get
 ready for Q&A.
