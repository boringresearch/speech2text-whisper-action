 I want to welcome David Chalmers, not for the first time. He's university professor
 of philosophy and natural science and neuroscience and co-director of the Center for Mind, Brain
 and Consciousness at NYU. He's the author of The Conscious Mind Constructing the World,
 Reality Plus Virtual Worlds and the Problems of Philosophy. He's known for formulating
 the hard problem of consciousness and with, well, or formulating, naming the hard problem
 of consciousness and analyzing it further. With Andy Clark for the idea of the extended
 mind according to which the tools we use can become parts of our minds. David Chalmers
 was also the one who was giving, who was chairing a session at NYU five weeks ago when he came
 to the, I think it was five weeks and he came to the microphone to give us the sad news
 that Dan Dennett had died. I don't know, Dan, whether you saw Nick Humphreys tribute. Okay.
 So without further ado, I hand the microphone over to David Chalmers.
 Great. Well, thank you, Stephen. It's really a pleasure to be here. Thanks for organizing
 this amazing summer school. I haven't managed to attend all of it live, but I've seen some
 bits of it and I've also taken great pleasure in looking at the videos. Yesterday I caught
 up on a bunch of the events in the first week over a video and it's really been extremely,
 extremely interesting. And yeah, it's nice and sad and resonant that we're dedicating
 this event to Dan Dennett who is a long time friend and colleague of many of us in all
 these areas. Actually, I just saw Dan three weeks before he died at a workshop in Santa
 Fe and he was in tremendously good form. He was physically fragile, but just full of ideas
 and full of enthusiasm. At the end, he said, "This is the best workshop I've ever attended."
 And he was just resonating with all the ideas all around. So yeah, it's a huge unimaginable
 loss for the field. I greatly appreciated Nick's talk the other day and particularly
 enjoyed the videos and the historical bits and pieces as well as Nick's extremely interesting
 talk in its own right. So it's nice to be able to pay tribute. So my talk today, the
 title is basically some issues where language models intersect with the philosophy of mind.
 And I'm using the whole stochastic parrot idea as one way to frame it. Are these language
 models best seen as something like stochastic parrots or best with no genuine mental capacities
 or are they best seen as something like emergent reasoners with some genuine mental capacities?
 And another way to frame the question is in terms of understanding. I'm actually going
 to come at this from a few different angles. I mean, the general theme will be to what
 extent can we ascribe genuine mental states and properties to language models. I'll use
 the Turing test as just one way in to this topic and then I'll look at questions about
 reasoning, about understanding, and finally about consciousness. The material on consciousness
 will overlap a little with the paper on this I published last year. The rest will be mostly
 newish. There won't be anything on sensory grounding because I presented on that already
 to Stephen's seminar on this topic maybe around six months ago, but happy to talk about that.
 If anyone wants to, I know Holger has been thinking a lot about sensory grounding in
 this context, so my apologies to Holger who's the discussant.
 One way to come into this is through the Turing test. I grew up on the Turing test back in
 the day 30, 35 years ago when I was a graduate student. We'd all talk about the Turing test
 in the Chinese room and so on as criteria for having a mind. And then there's a long
 tradition of setting aside the Turing test and poo-pooing it as not really the central
 thing when it comes to AI. For a start, it's obviously not a necessary condition to pass
 the Turing test to have a mind, at best a sufficient condition. But it is interesting
 that some of the Turing tests seem to have somehow become relevant again in recent years
 with the advent of language models which are tremendously good, of course, in conversational
 contexts, precisely what the Turing test tests for, to the point where we're now starting
 to see claims about language models actually passing the Turing test. Here's a very recent
 example from Cameron Jones and Benjamin Bergen at UC San Diego. They've published a few things
 on this topic over the last year or so, but the most recent paper, maybe this was about
 three weeks ago, makes the strongest claim yet. People cannot distinguish GPT-4 from
 a human in a Turing test. It makes it sound like GPT-4 or versions of it have now passed
 the Turing test. Now once you actually look in the details, perhaps that claim is somewhat
 overstrong. Here's their abstract. One thing you notice immediately is human participants
 had a five-minute conversation with either a human or an AI, judged whether or not they
 thought their interlocutor was human. Obviously five-minute conversation is a relatively undemanding
 Turing test. Turing mentioned the possibility of a five-minute conversation, but clearly
 thought that conversations ought to be much longer than this. There's also the fact that
 this is set up with a format where participants judge whether or not they thought their interlocutor
 was human. This is what they call a two-player Turing test. Now the canonical Turing test
 I take to be the three-player Turing test with an interrogator talking to two other
 players, one of whom is an AI system, one of whom is a human, and they have to make
 a judgment about which is which. That was not the format of these recent Turing tests.
 The recent Turing test was the human or not format. The judge carries on a single conversation
 at any given time with one player, and at the end makes a judgment, is this human or
 is it an AI system? It's a somewhat different format. You can see here that, well, it's
 true that GPT-4 was judged to be a human 54 percent of the time. If this is actually done
 in a competitive context with a human, actual humans are judged to be human 67 percent of
 the time. When you think about it, it looks like if it was a three-player Turing test,
 it looks like humans are still going to do better, and there's pretty good reason to
 think GPT-4 is somewhere underneath 50 percent, will be somewhere underneath 50 percent on
 those comparisons. It doesn't really pass the classic Turing test, which requires at
 least 50 percent performance when up against a human. Still, you might say this is a weak
 Turing test. First of all, it's a five-minute Turing test versus the indefinite Turing test
 that the Turing required. Second, it's a two-player Turing test versus looks like GPT-4 actually
 fails the three, will fail the three-player version. There's also questions about just
 how demanding, you know, just how, what the criteria should be for judges and human respondents
 in the Turing test. Here anyone could sign up to be a judge or a player, no prudentials
 required. You might think a demanding Turing test would require an expert both as judge
 and as respondent to make things as hard as possible for the AI system. I mean, I actually
 think that now that Turing tests are becoming relevant again, we ought to have a Turing
 classification scheme for what it takes. I mean, the classic Turing test, thanks to a
 Turing is maybe, let's call it a two-hour or an indefinite time, three-player expert
 test with experts as the judge and the respondent, whereas the San Diego version is just for
 now the five-minute two-player uncredentialed, uncredentialed test with uncredentialed members
 as the player and the respondent. Of course, there are other dimensions that comes in here.
 There's Stephen's own famous work on the total Turing test that requires embodiment. And
 we have to see how the AI system does at various embodied sensory motor tasks, as well as mere
 conversation. There are questions as to whether we restrict or don't restrict the content.
 My prediction for what it's worth is that I think some of these limitations on the existing
 Turing test work are likely to be overcome. And I will not be at all surprised if before
 long we see language models passing content unrestricted, time unlimited, at least disembodied
 Turing tests in the classic Turing three-player format with the strong restriction that these
 are tests with uncredentialed judges and respondents. As ordinary people, as judges and respondents,
 I think it's very likely that we'll have systems that end up doing better than 50% against
 humans. Why? Because they'll be optimized for Turing test performance. We'll figure
 out what does well in convincing people that you're human on a Turing test, and we'll
 optimize language models for that and uncredentialed judges, at least. I think probably before
 long we'll start getting over 50% of this, of course, because the relevant humans are
 not optimized for Turing test passing. If we have expert tests with AI experts as judges
 and respondents, I think things will be much more difficult. And I do think that's the
 long term goal. I mean, AI experts, of course, right now can very easily ask questions that
 immediately unmask a language model as a language model, whether it's exploiting glitches in
 arithmetical processing or cases where the language models know far too much, even for
 these. I mean, of course, the systems which go into this are all given a very significant
 prompt to try and make them as human-like as possible. I do think that expert-expert
 tests, call them EE tests, will take longer. Nonetheless, this is kind of amazing that
 we are coming into the era where AI systems are not that far away from passing full-scale
 Turing tests with these caveats. And a theme at this workshop has been the surprise. No
 one predicted the capacities of these language models. This is just another case where I
 think even five or six years ago, the idea that we would have systems that were this
 close to passing Turing tests would have seemed laughable. So things have changed very fast.
 So yeah, so current language models are at least surprisingly close to passing the Turing
 test. And previously, when I was growing up, the Turing test was regarded as at least a
 prima facie test of all kinds of mental capacities, thinking, understanding, and, indeed, consciousness.
 I mean, you know, certainly by no means a guaranteed test of these things. And there
 were various people, including the likes of Stephen and John Searle, who questioned whether
 the classic Turing test could get them. But nonetheless, you know, everyone at least gets
 you into the vicinity where you're suddenly a serious candidate for thinking, understanding,
 and consciousness. So this is just one way, then, to open the question, can language models
 think and understand? Are they conscious? And that's what I'll look at from here.
 So yeah, one way to come at this question is through the stochastic parrot framing,
 which, of course, is one way of answering that question. A very common reaction here
 is that despite the rather impressive conversational capacities of large language models, they
 simply do not think or understand or reason at all. They are merely impressive text imitators.
 After all, that's what they're trained to do. And in that respect, they're like parrots,
 here taken as paradigms of beings that imitate without understanding. I think it's actually
 a little unfair to parrots, which understand many things very well. But yeah, but the idea
 is they've just statistically trained imitators. They don't think, they don't reason, they
 don't understand. So yeah, I mean, the classic framing of this was in the paper by Emily
 Bender, Timnit Gebru, and colleagues Margaret Mitchell and Tessa McMillan on the dangers
 of stochastic parrots. And Bender has articulated the theme in other writings. I mean, one way
 to think about this is that, yeah, the line is that language models are mere mimics trained
 only to predict text. And since they're trained only to predict text, they only really model
 text. They don't model the world. And the thought modeling the world is required for
 thinking, reasoning, understanding. That's at least one way to, I don't want to attribute
 this exact line to Bender and Gebru and colleagues. But you know, you certainly find this line
 quite frequently in the in criticisms of the mental capacities of language models. I should
 say just to frame all this, I mean, I'm certainly not on the side of saying, yeah, current language
 models have all these mental capacities straightforwardly. But since I find, you know, the dominant voices
 are often, at least in the circles I move in, are often critical. The tenor of my remarks
 is often to say, well, maybe they're not there, but they're closer than, they may be closer
 than we think, which is to say that you could frame this as glass half full, or you could
 frame this as glass half empty. But I'm at least very interested not just in the question
 of where language models are now, but where they are going. And that makes me quite often
 especially interested in the glass half full framing. So this is, but this is an argument
 for the glass is empty framing. Language models are mere mimics. They only model text. They
 don't model the world. Now, when the argument is put this starkly, you know, I think this
 is rather easy to pick holes in it. It looks like there's a potential fallacy here, which
 I and various others have articulated about, you know, maybe it's a potential fallacy.
 But if you optimize for some property X, then all you do is X. And you know, here's a possible
 counterexample. Evolution by natural selection optimizes systems for reproduction. Therefore,
 evolved systems are mere reproducers. And you might then go on to that, say, well, therefore,
 they don't breathe, they don't fly, they don't understand, they don't, they don't model the
 world because all they were optimized for was, was reproduction. That would obviously be
 a fallacy, because evolution does, in fact, produce systems that breathe, that fly, that
 think, and, and so on. So, you know, what's the diagnosis of what would be going wrong
 in that argument? Well, one obvious diagnosis is that evolution develops many novel capacities
 in order to optimize fitness, turns out to get systems that, you know, reproduce as well
 as possible. Many other capacities are extremely helpful with that. And these capacities are
 emergent in the standard language insofar as they're not directly optimized, but they're
 a byproduct of optimizing fitness. And let me just actually skip to the next slide on
 emergence. This actually fits, you know, there's a philosophical literature on emergence, which
 I've contributed to along the way. I would think of this as a paradigm of what sometimes
 gets called weak emergence. You know, strong emergence is this very special things, is
 this very special phenomenon that may apply in a very small number of domains, possibly
 consciousness, where you get something totally unpredictable. But weak emergence is a more
 common phenomenon throughout, you know, the cognitive sciences and the sciences of complex
 systems, which I defined in a paper on this 20 odd years ago, as follows a high level
 phenomenon is weakly emergent with respect to a low level domain. When the high level
 phenomenon arises from a low level domain, the truth concerning that phenomenon are unexpected,
 given the principles governing the low level domain. So it's roughly some kind of at least
 initial surprisingness or unexpectedness of the phenomenon that's the key to weak emergence.
 So you might say, for example, that flying is weakly emergent with respect to selection
 for reproductive fitness, evolution starts optimizing for reproductive fitness, and then
 you get all these other capacities, you know, and at least initially surprising along the
 way. You know, after the fact, of course, things are less surprising, but you get these
 novel capacities along the way. And then, so you might say in applying this to language
 models, we can say language model training optimizes for text prediction, but then it
 develops many weakly emergent capacities in order to optimize text prediction. You know,
 some of these emerging capacities are totally obvious, we don't need to go to mental states,
 let's say, you know, capacities for game playing operationally construed capacities for coding
 capacities for, you know, doing math. And so on. Those are all, they all go beyond straightforward
 optimization of text prediction, but it turns out in order to do text prediction, well,
 this somehow produces as a byproduct capacities for coding or game playing, and so on. So
 once you've got there, once you recognize the existence of emergent capacities, then
 it starts to look not so utterly surprising that these emergent capacities could include
 reasoning, mental capacities, such as reasoning and understanding and cognitive capacities,
 such as having world models. Nothing here implies that you will get these things as
 emergent capacities or that we have, but it starts to at least look like a theoretical
 possibility. So then the question though is about current language models, there's this
 theoretical possibility that, you know, they could have developed emergent cognitive capacities
 such as reasoning, understanding world models, or are they merely stochastic parents that
 haven't gotten there yet? And that's at some level an empirical question. But, you know,
 I think there's pretty good reason to think that language models at least, that training
 a language model can give you models of the world. One case I'm interested in is just
 say you've got a huge body of text Q&A about the New York subway system with people asking
 questions like how do I get from this station to that station? And then I'll give you answers.
 Well, you know, you go down this line and you change at this line and then you go to
 this station. To optimize prediction on that body of text, this data I think will very
 naturally lead to structural models of the New York subway system. Here's a basic structural
 model with just the lines and the stations filled in without lines. Of course, the inspiration
 for this, as Holger well knows, comes from Rudolf Kahnap's book, The Logical Structure
 of the World, where he said, you know, science is going to lead us to this abstract structure
 of the world just from predicting relevant data. One of his key examples was the railway
 map of Europe he had transposed to New York City. But, you know, it's fairly straightforward
 to see why a really well optimized data predictor of answers to questions about the New York
 subway system would lead you to a model of, you know, roughly this form with both the
 structure and some labels or stations and lines filled in. More generally, you might
 think that truly optimizing prediction would actually require some of these cognitive capacities
 such as reasoning and understanding. I mean, here the thought is, just say we had a system
 that truly optimized, truly minimized text prediction error, maybe subject to various
 constraints. I mean, I think it's, you know, brains do very well at text prediction precisely
 because they have capacity such as reasoning and understanding. You might well think that
 as long as it's possible in principle for an AI system in the relevant class to have
 these capacities, reasoning and understanding, then an algorithm that truly minimizes text
 prediction error would have, would require models of the world that have these capacities
 because without that, without genuine reasoning and understanding, you would expect we won't
 do as well. If that's right, then sufficiently optimizing text prediction error in a language
 model, maybe that's hypothetical that one could sufficiently optimize it in that way,
 but sufficiently optimizing text prediction error in a language model should lead in principle,
 you would expect to world models, reasoning and understanding. You can even actually put
 this as an argument that truly optimizing prediction would lead to models with various
 capacities. Premise one, you know, some language model in the relevant class can have this
 capacity. That's already a substantial premise that some language model could reason. And
 then premise two is just that, you know, reasoning gives you a performance advantage. Language
 models with the capacity for reasoning outperform language models without the capacity for reasoning
 at prediction, given these constraints. And then it seems to follow fairly naturally that
 truly optimizing prediction in language models in the relevant class would produce language
 models that can reason because models without reasoning won't be optimal. Now, of course,
 this is, you know, nobody should take this as a straightforward argument that, of course,
 language models understand or reason or have these capacities. There's any number of obvious
 ways to block that conclusion. First, it's a substantive claim that language models can
 support reasoning at all. And some would question that. Second, you could question the second
 premise and say that reasoning doesn't actually give you a performance advantage and a limit.
 It may be that models systems without the capacity to reason can outperform or at least
 equiperform systems with the capacity to reason at prediction tasks. That would be surprising,
 but interesting if so. And third, of course, maybe the most obvious rejoinder here is,
 yeah, well, a perfect optimizer, something which really found the optimal system in this
 class for prediction error would have these capacities, but there's very little reason
 to think that standard optimization methods are that good at optimizing. Maybe the optimal
 methods, maybe the optimal algorithms would have the capacities for reasoning, understanding,
 and so on. But standard is not much reason to think that standard optimization methods
 are actually going to lead you to truly optimal algorithm. That's just a little bit too optimistic,
 I think, about standard current machine learning methods and standard optimization methods.
 This actually connects to a question that I got interested in recently, which is, roughly,
 did anyone see language model capacities coming before five or six years ago, and why exactly
 have their capacities been so surprising? So question, why were people so surprised
 by the capacities of language models, given that in principle, everybody knew in principle
 that a truly optimal system at text prediction would lead to these, would require these capacities.
 It's like, yeah, we knew in principle that probably text prediction would be what's sometimes
 called AI-complete, that it would take an AI to do it really optimally would require
 AI systems with extraordinary abilities and capacities. In a sense, we knew that all along.
 Nonetheless, we were surprised by the capacities of language models, and presumably the answer
 to the question is that, well, nobody knew or expected that language models would optimize
 as well as they do, even that they would optimize text prediction as well as they do, and like
 that question, but that includes both optimizing over the training set, generalizing to a test
 set. I mean, language models just turn out to be much better optimizers than anyone expected,
 and that's why people were so surprised at their capacities, because this does not yet
 tell us that they are perfect optimizers. I mean, nobody thinks that a language model
 is a perfect optimizer. We don't yet have a perfect optimizer and various mathematical
 constraints that show that this is not going to happen, but nevertheless, this does bring
 out that if language models optimize well enough, you would expect that capacities like
 this to be a genuine possibility. Still, that's all theoretical. Empirically, it remains a
 genuinely open question whether language models can reason, understand, and be conscious.
 I mean, there's this nice theoretical possibility thrown open by emergent properties of optimization.
 If you think reasoning, understanding, and consciousness give you a performance advantage,
 then there's a theoretical possibility of getting them this way, but the empirical situation
 we're in is we have systems with these rather impressive capacities, and now we need to
 assess that empirical evidence, combine it with philosophical reasoning to figure out
 whether current systems can reason, understand, and be conscious, and whether the coming systems
 are likely to have those capacities. The question is partly empirical, grounded on where we
 are in the current progress of machine learning, but it's also partly philosophical involving
 questions like what is it to reason, to understand, and to be conscious? What are the criteria
 for counting as being a system that reasons, for counting as being a system that understands,
 and for counting as being a system which is conscious? That kind of gets me to the philosophical
 core of this talk, where I'm going to focus from here about half on... I'll focus a little
 bit on understanding, and then I'll focus on consciousness.
 What is understanding? I mean, really, it's the theme of this summer school. I take it
 as language model understanding. These are important questions to come to the bottom,
 to get to the bottom of. I think various speakers have at least touched on this already, but
 the word understanding is a very slippery, multiply ambiguous term that's used in many
 different ways in different contexts. What is understanding? What does understanding
 mean? Or if it means many things, what are the many different things that that word means?
 And we might phrase this question in the framework of conceptual engineering. What would be a
 useful concept of understanding for us to have? What do we want to use that term to
 mean? There is by now a fairly large philosophical literature on understanding, especially in
 the philosophy of science, where understanding and related notions like explanation are super
 important. Here's a very nice book by Hank Dirichet called
 Understanding, Scientific Understanding, came out six or seven years ago, all about
 understanding in science. And Dirichet makes a very, very useful three-way distinction
 between my terms. The terms I use here are slightly different from what he uses, but
 it's pretty close between phenomenal understanding, explanatory understanding, and what I'll call
 use understanding, or just P understanding, E understanding, and U understanding. Phenomenal
 understanding is all about understanding as being tied to a certain kind of experience.
 Explanatory understanding is about understanding as being tied to knowing an explanation and
 being able to give explanations. And use understanding is just being able to use information about
 a phenomenon in relevant ways. There's more to say about each of those.
 Expanatory understanding is maybe the least relevant to the case of language models, at
 least as the thing we're worried about when we're worried about whether language models
 can understand. You E understand X if you know an explanation of X. Maybe this is not the
 central thing we're worried about with language models. Do they know explanations or not?
 But it's certainly true that language models are at the very least good at giving explanations
 of many things. This is one of the places where they're just surprisingly good. They're
 not always on the mark, but they're surprisingly good at giving explanations of various scientific
 phenomena or social phenomena. You say, why did this happen? They've always got answers
 and often the answers are quite good. So if the criteria for explanatory understanding
 is being good at giving explanations, then you can say language models are at least on
 their way there. Of course, there'll be residual questions like, well, maybe language models
 can give explanations, but do they know these explanations? And that connects to a much
 more general question about knowledge. Do language models genuinely count as knowing
 anything or they just have this kind of superficial capacity to give outputs which fall short
 of knowledge? That's itself a tough question that I won't try and answer today, but at
 least where explanatory understanding is concerned, you know, there are at least maybe part of
 the way there. Use understanding. I think this is a key notion for many people. It's
 to say you understand or you understand X when you can use X or information about X
 or respond to X appropriately to achieve various goals. So here is kind of an operationalizing
 in terms of, you know, can you actually, is this something you can use? Maybe you understand
 the car engine when you can do various things with the car engine. You understand the mathematical
 theorem when you can use it in the right way. Now, use understanding, I think, itself is
 a very broad category which we can divide into many different classes. Here, I've just
 got three different kinds. One is what I've called V-understanding for verbal understanding,
 and this is, you know, having the linguistic capacity to answer relevant questions about
 X, for example, and to come up with further relevant information when it's needed. This
 is all in the verbal domain. Of course, this is something which language models look fairly
 good at, at least if we operationalize, you know, answering questions in some straightforward
 ways that they, you know, when they give satisfactory answers, this counts. Of course, they're,
 you know, I mean, let's not minimize the problem of hallucination and all the different glitches,
 but still, they've got rather impressive verbal capacities here. At the same time, there are
 other forms of behavior, of course, beyond verbal behavior. So, maybe I've written that
 here as V-understanding for behavioral understanding, but with a focus now on nonverbal capacities,
 like being able to manipulate X. Here, you know, V-understanding a car engine would be
 telling you, okay, in order to fix the carburettor, you know, what do I take out? What do I do?
 And they can give you all the verbal answers. V-understanding would be actually being able
 to fix the carburettor as a mechanic can, and, you know, language models right now are not
 terribly good at this kind of thing. I mean, the standard pure language model has no capacity
 for such a thing. Of course, all this, we're in principle just as interested in multimodal
 language models with sensory capacities, and, you know, increasingly, at least occasionally,
 some systems have motor capacities, you know, the various language models that control robots,
 for example. But I think it's fair to say that their nonverbal capacities are not yet
 remotely as impressive as their verbal capacity. So, where embodied understanding, the kind
 of thing which is tested by Stephen's total Turing test comes in, they're not so good.
 Also, very important here, beyond behavior, is kind of, you know, cognitive use, which
 involves being able to reason about phenomena. I've called that R-understanding, being able
 to reason and make inferences about X. You might even call it I-understanding for, you
 know, for inferential understanding, being able to make relevant inferences. Here, okay,
 I've touched on some of this already. Pure language models lack the nonverbal behavioral
 understanding completely. Multimodal models are beginning to do better, but, you know,
 they've still got a very long way to go in that domain. But, you know, they do display
 rather impressive V-verbal understanding of language and of numerous domains where they're
 very good at answering questions. At least, you know, again, they've got a long way to
 go, but at least fair to middling verbal understanding. And they at least appear to have, you know,
 some imperfect reasoning understanding. I mean, you know, full-scale reasoning and planning
 is a limitation of current language models. They're getting better, but they still have
 all kinds of limitations. Still, they appear to at least be in the game. They've got some
 kinds of... They at least, you know, respond in ways that suggest they've got some kinds
 of imperfect capacities for reasoning. If they can reason, you know, if you think language
 models are in the business of reasoning at all, I'd say at least they've... Certainly,
 if you would take an approach grounded in their behavioral capacities, their behavioral capacities
 are of a kind that suggests at least some basic ability to reason. So I guess I'd come
 at this with a glass, you know, a glass half full answer. Language models appear to display
 many aspects of explanatory understanding and use understanding. At least if these are construed
 in behavioral terms, although there are significant questions both about the extension from verbal
 domains to nonverbal domains and about the extension from behavior to, you know, the
 internal aspects of reasoning. Nonetheless, if we just focus on explanatory and use understanding,
 we'd say, okay, well, you know, there's some good reasons for thinking they're on their
 way. I think the big obstacle, you know, for many people when it comes to the question
 of, you know, do these language models genuinely understand, is a question about phenomenal
 understanding. That is, do they have, you know, the first person experience of understanding,
 what's sometimes called the feeling of understanding, and more generally is, you know, the conscious
 experience of being a system that understands. You know, famously, when you hear a language
 that you understand, you just experience it completely differently from how you experience
 a language that you don't understand. And many people take this conscious experience
 to be partly constitutive of full understanding. I mean, I would argue that something like
 this is going on in Searle's Chinese Room, where Searle argues that the person, you know,
 the person in the room, they don't understand Chinese, because nobody, the person, nobody
 has the conscious experience of understanding Chinese. Even people like, you know, Emily
 Bender in her discussions of, you know, the stochastic parrot issue have talked, has talked
 about a distinction between, you know, being, having experience, and maybe being a mere
 artifact. So experiencers versus artifacts, and the claim is that language models don't
 experience, they're mere artifacts. And then it looks like, again, it's the conscious experience
 that matters. So I think for many people, this is the single biggest block to saying
 that, to saying that language models understand. I mean, the behavior is, of course, you know,
 very far from extremely imperfect, but nonetheless, you know, increasingly getting more and more
 impressive. If there's a principled objection blockade here, many people find it precisely
 in the feeling of the conscious experience of understanding. So that gets us to the fourth
 issue, sorry, the star should be on four, fourth and final issue, which is are language
 models conscious? I mean, of course, language models will only consciously understand if
 they're conscious. And I think this is, you know, for many people, the big, the big block,
 you know, are language models conscious? Could they ever be conscious? This is really, you
 know, you might call this the big kahuna in this, in this area of the mental capacities
 of of language models. Yeah, reasoning, yeah, understanding, yeah, believing, you're desiring,
 you're acting, agency, those are all important issues. But you know, the big one, the one
 that, you know, many people have a hard time thinking that language models could ever have
 is precisely consciousness. So okay, are chat GBT and other language models conscious? And
 this is something that I talked about in a talk, I gave it in Europe, maybe a year and
 a half ago that subsequently came out in the Boston Review. So this will be just some some
 degree going over that, that territory. But I think it's important territory. And you
 know, maybe I can give some updates on the way. Um, as I use the term, you know, all
 these terms, consciousness, sentience, they have many, many, many, many uses. And that's,
 you know, not none of them have a single definitive meaning. But as I use the term, consciousness
 and sentience, both mean the same thing, which is subjective experience. From a first person
 point of view, a being is conscious in a sense, if there's something it's like to be that
 being that is if it has subjective experience. And of course, this locution goes back to
 my, my colleague, Tom Nagel, who wrote that famous paper, what is it like to be a bat,
 saying, well, there's something it's like to be a bat. And if there is, it has subjective
 experience, it's conscious, sometimes it goes by the label, phenomenally conscious, representing
 the idea this is a, you know, it's a phenomenological state from the first person point of view.
 Consciousness has many, many different aspects. It's, you know, consciousness is very complex.
 It includes sensory experience, you know, the experience of seeing red, affective experience,
 feeling pain, or feeling sad, cognitive experience, the experience of thinking hard, or maybe
 even of understanding a language, a gentive experience, the experience of acting or deciding
 to act. It also includes but it's not limited to self consciousness, awareness of oneself,
 as having these states, I think, you know, there can be, there can at least arguably
 be consciousness without self consciousness, but self consciousness is one important aspect
 of consciousness. So think of these, you know, these are all the multiple tracks in the inner
 movie or the inner virtual reality of consciousness. And the question is, you know, which, if any
 of these can language models have? One way to put the question is by asking is chat GPT
 a philosophical zombie, where, you know, philosophical zombies are these systems which are behave
 very much in very sophisticated ways, but lack consciousness entirely. I mean, the extreme
 case of a philosophical zombie is a system which is physically identical to us without
 consciousness. We don't need to bring that in here, somewhat less extreme is one which
 is functionally or behaviorally, just like us without consciousness, but there's an extended
 notion of a philosophical zombie, a system with very, very impressive behavior that nonetheless
 lacks consciousness entirely. And you know, I think quite a lot of people, you know, on
 the stochastic parrot side of things are inclined to roughly hold the view that current language
 models are versions of philosophical zombies, they've got, you know, behavior, which may
 or may not be impressive, but there's no consciousness, there's no experiencing going on. So the question
 is, is chat GPT conscious, or is it rather a philosophical zombie? Okay, I just, I just
 defined philosophical zombies already. I noticed there was a discussion of this on on Twitter
 or X, the other day where somebody, somebody basically asked three of the leading models,
 whether they are philosophical zombies. And chat GPT said, Yep, absolutely. I'm a zombie.
 Lord three said, I don't think so. But I think I'm conscious. But can I really know I feel
 conscious, but is it an illusion? And Gemini insisted is conscious. He says, Absolutely
 not. Look at me. I'm talking like this, of course, I'm conscious. So okay, I don't think
 we should be taking their their word for this. Although, you know, verbal reports are traditionally
 our best guide to whether another system is conscious, that's certainly what we use in
 the case of other other humans. But of course, you know, the whole apparatus of verbal reports
 as a guide to consciousness tends to break down in the case of language models. Why?
 Because they're precisely trained on a whole bunch of texts produced by humans, saying
 they're conscious, and it's not clear. The fact that they end up saying they're conscious,
 it's just not clear what evidential value it ends up having. But the approach I've taken
 is, okay, let's look at reasons to deny that language models might be conscious. A lot
 of people think language models are not conscious. And the furthermore, they're unlikely to be
 conscious in the near future. Then I've just, you know, approached this person. Okay, well,
 why is that? Is there some feature x, such that language models lack x. And furthermore,
 if a system lacks x, it probably isn't conscious, or sentient, and then give good reasons. So
 this is to some degree, a game of burden tennis, you know, putting the just putting the burden
 onto onto opponents say, okay, well, if it's you think it's so obvious that language models
 are not conscious, then just give me a, give me a reason. Why? What is it that they? What
 is it that they lack? You know, there's no shortage of reasons here. And the article
 I wrote on this, I ended up focusing on six. The first is biology. Some people, like my
 colleague Ned Block, think that biology, or maybe certain kinds of chemistry are required
 for consciousness. A standard silicon based language model will lack those and therefore
 won't be conscious. That's, of course, very controversial. One common line is that senses
 and embodiment are required for consciousness. And at least classic language models lack
 senses, and lack bodies, they lack sensory organs, they lack bodies, they've only got
 the very limited form of perception and action that comes from having text inputs and text
 outputs. I think that's, that's a reasonable line. But it's also the case that, you know,
 multimodal models are now developing fast. Now it's standard for language models to at
 least to have some form of sensory inputs, at least from, you know, visual and auditory
 data and increasingly getting connected to embodied systems as well. So that may be a
 temporary limitation. World models and self-models are interesting. Again, this kind of connects to
 the stochastic parrot issue of whether, whether language models have world models at all. I'm
 inclined to think that there's pretty good reason for thinking they at least have limited world
 models and that some of the work and interpretability that we've already seen suggests
 at least some basic forms of representation of the world. Self-models are trickier. You know,
 there are people like Michael Graziano and others who think self-models are
 absolutely crucial to consciousness. Do language models have good models of their own
 mental states and workings? Less clear. There are some more technical requirements here. Some
 people think that consciousness essentially requires recurrent processing. Perhaps because
 consciousness requires certain kinds of memory and you need recurrent processing for memory.
 Most interestingly, most of the leading scientific theories of consciousness right now
 do have a recurrence requirement. And then, you know, of course, transformer systems are
 largely feed-forward. They've only got some very rudimentary recurrent processes. So that's, you
 know, there's at least a principled line for saying that feed-forward systems can't be conscious.
 I'm a little skeptical myself that recurrent processing is an absolute requirement for all
 sorts of consciousness. Anyway, that, on the other hand, is obviously a temporary limitation.
 Current language models are largely feed-forward, but there are recurrent language models and
 there's a lot of people working on advancing them. There's also questions about where the
 consciousness requires something like a global workspace and whether those can be found in
 language models. People have at least built language models. Yoshua Bengio, Ryota Kanai,
 and others have built language models that have at least a limited kind of global workspace.
 But, you know, it's a reasonable question as to where the language models have this
 and whether this is required for consciousness. And then maybe there's also slightly mushier
 worries about whether language models have the kind of agency that we want for consciousness.
 So they really have their own desires, their own motivations, unified action,
 or are they just doing something like acting, improvising, which is not really what we think
 of as unified agency. Anyway, there's a lot to be said about all of these. I don't have time really
 to go into them, but I would say that to take an overview, arguably, AI systems don't need biology
 to be conscious. Certainly, I've argued for this in my work in many places. I would say they already
 have, you know, some language models already have sensory systems. Some of them have a form of
 embodiment. And I think many of them have world models. They may not yet standardly have full
 scale recurrent processing or global workspaces, but I don't see a principled obstacle to having
 large language models with full scale recurrent processes, or with global workspaces. Those may
 be coming before too long. I think the whole question of agency and unified agency is a
 tricky one. It requires really spelling out what the conditions are for agency. But I do know that
 there's at least a research program of developing much more agent-like language models than current
 systems, and I expect work in that area to develop fast in coming years. So I guess my view is,
 which I argued for in this paper, is it's at least possible that current systems could already
 have some element of consciousness, perhaps not human-like consciousness, but, you know, arguably,
 insects and fish have some kind of consciousness that falls well short of human-like consciousness.
 I say that's possible, but I don't say it's likely. I don't think we can rule it out that current
 systems have some element of consciousness. But it's unlikely that they have human-like consciousness
 and understanding right now, especially in the domain of cognitive consciousness, introspective
 consciousness. I think various reasons for thinking that's unlikely right now. That said,
 I think it's quite possible that within 10 years or so we'll have AI systems with consciousness
 that's human-like in at least many respects. In the article I wrote about a year ago on this,
 I tried to do kind of a calculation given that, you know, what are the biggest obstacles
 and what are the chances that we'll overcome them within 10 years to make the case that, yeah, maybe
 somewhere 25% to a higher chance that we'll have AI systems with somewhat human-like consciousness
 within the next 10 years. That's not to say it's certain, but it is to say it's a possibility,
 I think, that we really need to take seriously. Why do we need to take it seriously? Well, why
 does AI consciousness matter? And I'll close on this. Well, I think, you know, it's a reason
 which our moderator, Stephen, has articulated in his work on animal sentience. Why is sentience
 or consciousness important? Because sentient beings or conscious beings, they matter morally.
 If it turns out that fish have subjective experience, you know, especially if they can
 feel pain or if they suffer, then it matters how we treat them. At least they should enter our
 moral calculations. If it turns out that AI systems are conscious, then the same is true.
 It matters how we treat them if they have human-like consciousness. If AI systems come
 to have human-like consciousness, which I take it goes well beyond, say, fish consciousness,
 then they will arguably at least have human-like moral status no matter a great deal. And at that
 point, you can no longer treat your AI systems like tools. If we treat AI systems with human-like
 consciousness as mere tools, that could in principle lead to, you know, a moral catastrophe that
 was in many ways reminiscent of moral catastrophes we've seen in the past when human beings were not
 treated as with full moral respect. This may make us question, I think, whether we should be trying
 to develop human-like AI at all. And I do take that to be a very serious question. I mean, if at some
 point it becomes possible to develop useful AI that's not conscious, then there's a lot to be
 said for going in that direction because we have the potential of avoiding moral catastrophe that
 way. But it's not obvious that will be possible. Here, there's a couple of massive problems that
 we need to overcome to understand the question of whether language models are genuinely conscious
 or may have that capacity. Problem one is we don't understand consciousness. To respond to
 that challenge, we need better scientific and philosophical theories of consciousness. Something
 people have advanced on a lot in recent decades, but there's still a long way to go. Second,
 just as big a problem, we don't really understand what's going on in language models. Notoriously,
 it's extremely opaque what's going on in language models. Interpretability is methods are just
 starting to be developed in sophisticated ways, but we have a very long way to go there. And there
 are the challenge, I think, to really get at this question of the mental states of language models.
 We're going to need much better interpretability as well as a better understanding of consciousness.
 So I see really there as being a massive interdisciplinary project, which is required
 to make it further on these questions. First, improve our theories of consciousness and other
 mental states, relevant mental states like understanding, reasoning, belief, and agency.
 Second, improve our understanding of AI systems to come to know better, to be in a better position
 to know whether they have these states whose criteria we'll hopefully understand better.
 Third, improve our understanding of the relevant moral issues, like moral status, to know exactly
 just which mental states confer moral status and why. And then this is already a massive
 interaction between philosophers, AI researchers, neuroscientists, psychologists, and others,
 then put all these pieces together to jointly determine what it takes for an AI system to have
 moral status, and whether AI systems have or can have moral status, at least in principle,
 and eventually in practice and in fact. Okay, so just to sum up, current language models
 may not yet have human-level reasoning, understanding, and consciousness, but I think
 they're well on their way, and it's entirely possible that in the next decade or so we'll get
 to a point where we do have language models with human-level reasoning, understanding,
 and possibly consciousness to be ready. When this happens, it will be, I think, a momentous
 development. To be ready for this, I think we really seriously need good philosophical thinking.
 Thanks. Thank you very much, Dave, and now we have Holger, who I think should be
 relaxed about what he was planning to say, because in the end you said all the stuff that he had to
 comment on. That's just my prediction. Yes, well, I'm not so sure about that, but I'm not really
 relaxed here. My apologies, Holger saw nothing in advance. No, no, totally my fault. I should have
 contacted you first, but anyway, I typed in a couple of things, so maybe it's possible
 that I share my screen, and just see. Or maybe, how does that work? Sorry for the delay.
 Do you see it yet? No, what you have to do is look at what you have on your screen,
 on your desktop, and pick the screen that contains the PowerPoint that you want,
 or whatever it is that you want to show. Yes, or something like that. Okay, so all right.
 Yeah, I'm mostly in agreement with many of the things that Dave says. So naturally, I think I'm
 in agreement with many of the things that you said, Dave. By the way, first of all,
 many thanks for the great talk. Great as usual, although not the one that I expected.
 But there may be certain intersections to what I did expect. So I typed all of this in while you
 were talking. So I think I don't have that much to say about the Turing test. I think I basically
 agree with what you said about that. The part two on stochastic parrots. I very much like
 what you said about the glass is half full rather than half empty thing, and also about your
 way of looking at reasoning and understanding of possibly weakly emergent properties. So
 we should not be too astonished about it. Well, we may be astonished about it, but there may be such
 emergent properties. And so then the stochastic parrot remark is a stark remark as it seems to
 forget the possibility of weakly emergent properties. Also like the idea of LLM world
 models as structural models, and that truly optimizing prediction in LLMs would produce
 reasoning. So then there are still these open questions about reason understanding consciousness.
 One remark I'd like to make is that I think that prediction, and that would be also my remark
 concerning the stochastic parrot's argument, that prediction and truly optimize prediction,
 as you put it, isn't trivial at all. Doesn't it require already if prediction comes out that
 optimal and that good, as we find it in LLMs, doesn't this require the development of some
 sort of world models, probably then structural world models that don't yet have a grounding in
 terms of what we might call intrinsic properties, but that picks out the relationships between
 what's going on in the world. Why? Because these are the relationships that we also found in the
 data set. So there is a connection here to the grounding question and whether one can have
 purely structural grounding or whether grounding is in need of something more than structural models.
 I don't think it is. And yeah, so that would be one question in this respect. And your part about
 LLMs understanding, you said that explanatory and use understanding are maybe not so central.
 At least it seems that we... Sorry, I said explanatory understanding is not so central.
 Understanding I think is central. Use is central. But even with explanatory understanding,
 we typically ask children in school and then we ask our children to be able to explain what they
 do, not only to show performances, but then also to give explanations. I think that's part of what
 we do. And this is why many people would like to see LLMs to give self-explanations. And actually,
 my experience with LLMs is that they are actually rather good at that, sometimes even better than
 we humans are, because sometimes we humans are rather lousy about our own capabilities.
 But then, of course, you said that phenomenal understanding is indeed the central question.
 And here, I'm no longer sure that I would follow you. I already struggle by the question of
 whether there is something like a feel or a conscious experience of understanding. What
 exactly is that? I know, of course, I mean, as a scientist, I know that we have these aha
 experiences and heurika experiences. Here and then we at least we do. And so I wonder what this would
 mean for, in the other paper, you talk about pure thinkers, or you may talk about systems that
 are a bit like commander data and Star Trek. So don't you think that they have an understanding
 that is like, that is, actually, it's a question for you, what kind of understanding do you think
 these systems have? And is it really the lack of a feel about understanding that I solved a certain
 problem that makes the understanding a genuine understanding? Because this is what, how I
 understood you in this part three of the paper. And from there, you slightly went over to the
 consciousness issue, called it the big Kahuna. Yes, there are, if there are reasons to deny LLM
 consciousness, then what is it that LLMs lack? And I think that's a list one could come up with.
 Personally, I would think that, for instance, lacking world models itself is not sufficient
 for consciousness, but lacking world models would be something that is important for understanding
 and being semantically grounded to the world. You said that it's possible that current systems
 could have some element of consciousness, but isn't it even more so possible that current
 systems already have some element of semantic grounding there? Which is to say, unlike you,
 I seem to have, I guess I have the idea that we can make a rather strong divide between reasoning
 and understanding and semantics on the one hand side and consciousness, phenomenal experience on
 the other hand side. So you can have one without the other. I actually even think that they are
 doubly dissociable. Whereas you seem to think that, at least in this talk, you seem to think that
 consciousness is even constitutive for semantics. And I wonder whether you really want to say that
 at the end of the day, which is to say that you would think that a figure like Commander Data and
 Star Trek really is a fiction. Is it even inconceivable to have a being like Commander Data?
 Is it inconceivable that LLMs are a bit like that? What I do like, and I have no time to print it,
 to type it in. Maybe I can show the slides that I put it in before.
 In the other paper, you make the remark that
 there is that textual input that the LLMs are fed with is a sort of sensory input that you
 seem that you don't see a categorical difference between textual input and sensory input.
 And I do wholeheartedly agree with that. But if that is true, then where does the divide between
 phenomenal experience and... Where does phenomenal experience come in? If textual input is just one
 form of input, and when we can have that without phenomenality at all, then why should we expect
 phenomenality as being a sort of precondition for understanding, semantics, grounding, reasoning,
 all these sort of categories and capabilities and capacities that I would think
 are possible for systems without being conscious? Okay, I think that's it for a moment.
 Okay, thank you very much. I'm going to pass it over to Dave. But first, I can't resist
 saying something to you. Yes. You understood what I just said.
 Now I assume I'm right that you didn't understand. The difference between those two examples was not
 acoustic. Yes. Okay. But what do you want to say with that? By the way, you're German is great.
 That's... Well, Hungarians have to learn all other languages. That's what's meant by sentience.
 Go ahead. Dave, go ahead. Okay. Well, thank you, Holger, for the absolutely heroic comments.
 No advance notice compiled on the fly during the talk. Yeah, they're great comments. I'll have to
 think about all the issues you were talking about. Yeah, on the structure of the New York
 subway system, I guess I do think this is the case where a structural understanding is already going
 to be good enough to certainly to respond to the questions very well, even in the absence of sensory
 grounding. Will it be a full understanding of the New York subway system? Well, many people would
 say for it to be a full understanding of the New York subway system, you have to understand something
 about trains and lines and New York City and speed and grime and people and so on. And many
 would argue that's the point where grounding may need to come in for like a full-scale beyond
 structural understanding. Of course, a multimodal language model could in principle have many of
 these things. But I'm also moved by the work of Eli Pavlik, who's going to be speaking here later
 this week on just how much can be done by pure language models without the multimodal aspect and
 even a structural characterization of many of those things gives you at least a strong partial
 understanding. But yeah, for those, that and some of the other remarks get into
 some of the things I was talking about in my article. By the way, for those who don't know
 the article on sensory grounding, it's a little hard to find, but it's online. The article I wrote
 called "Does thought require sensory grounding from pure thinkers to language models?"
 Oh no, I don't know that. It's in the same archive as this will be in, so you can find it easily.
 I thought you said you based your remarks on an article of mine, your initial comments?
 I thought it was that paper. Yes, the APA, but wait a second, the APA paper, right?
 Yeah, it was my presidential address to the American Philosophicals, came out.
 Yes, does thought require a sensory, I'm sorry, oh yeah, that's actually the one.
 That's got my thoughts on sensory grounding towards the end. But yeah, but then you started,
 towards the end you said, why is phenomenal understanding important? Well, part of it,
 I said, was just for the critics, for the stochastic parrot crowd, who think that
 language models do not understand. I at least suspect strongly that for many people
 on that side, the crucial aspect is the experience of understanding. I suspect, for example, that
 Stephen would put a lot of weight on the first person experience of understanding,
 and John Soule certainly would, and the likes of Emily Bender appear to. So it's at least central
 from their perspective as the potential obstacle, which makes it dialectically relevant to try to
 make the case that language models have this capacity. But then if you ask me what is actually
 the relationship between consciousness and understanding, well, this is of course, is very
 complicated. I'd say understanding divides into all these kinds. Sensory, I mean, it's also worth
 remembering here, the consciousness here isn't just sensory consciousness. Sensory consciousness
 is relevant, but there's also cognitive consciousness. And much of what's involved
 in the experience of understanding is arguably on the cognitive side. I guess I'm inclined to
 think that consciousness brings with it certain cognitive capacities, you know, to grasp a
 proposition, to have some acquaintance with the world. Even perceptual consciousness brings with
 you a certain kind of perceptual acquaintance with the external world. I'm inclined to think that
 cognitive consciousness does something like that for the propositional contents of thought. It
 gives you a kind of cognition that you couldn't have without it. But there's of course always the
 question, what is its cash value? For example, could a philosophical zombie do all this without
 cognitive consciousness or sensory consciousness? And I'm committed to there at least being a
 metaphysical possibility, if not a possibility in this world. So fundamentally, we don't understand
 the function of consciousness. But I guess this is one reason, you know, one reason to care about
 consciousness might be if we knew something great that it could do for you. But, you know,
 when I was saying why does consciousness matter here, I deliberately stayed away from claims of
 that sort. For another kind of claim, which I think is much clearer to us, we care about a being much,
 much more if it's a conscious being. If language models have both sensory and cognitive consciousness,
 then that brings itself I think very squarely into the realm of beings that we care about.
 So I guess I'd be inclined to put the importance of consciousness in the first instance right there.
 And then there's a whole further set of questions as to whether it turns out to bring with it all
 kinds of remarkable cognitive and behavioral capacities that so far we don't understand very
 well. May I just add one remark or question of clarification then? On the distinction between,
 or on your remark in the Thus Thought Requires Sensory Grounding paper, on your remark that
 textual input is a sort of sensory input. What exactly does that mean? For me, it sounds as if
 there's just a gradual continuum between pure textual input as we find it in LLMs and sensory
 input as we used to think about it. And I applaud to this. I think that's actually true,
 but which then would mean that there is no categorical distinction between
 sensory as being equipped with the phenomenal and textual as being equipped with the non-phenomenal.
 Why isn't it? I agree with you that text models, they really problematize some standard
 distinctions between the sensory and the cognitive. And I think actually the last paragraph of
 that article of mine on sensory grounding says something like this. Yeah, my colleague Ned Block
 wrote a whole book called The Border Between Seeing and Thinking. And it all looks like in human
 beings, there's a very clear distinction to be drawn between the sensory and the cognitive.
 We can argue about edge cases, but yeah, once we get to language models that treat text very much
 in the way that we treat the sensory, and you can go back and forth between a multimodal model that
 just gets a visual, a file representing a visual input, and a language model that has a description
 of it, those two things might be trivially intertranslatable for a language model, not for
 a human being. And therefore, the distinction between the categories of the purely sensory
 and the cognitive may actually break down in a language model. So yeah, that was where I left it
 in that paper. I do think that language models problematize the distinction. Now the questions
 are open three ways, and I want to ask you first, Dave, are you savvy enough to find the Q&A? Yes,
 I have it in front of me. Okay, do you want to start with that? Anybody can interrupt by raising
 their hands and they'll get the next... Okay, come down here while he reads the first thing that
 catches his mind. Go ahead. Okay, David, can you speak a bit about your thoughts on the connections
 between language models and their potential ability to reason and understand with the hard
 problems of consciousness? Can you comment on how language models and AI relate to a dualist
 understanding of consciousness? Yeah, in what I said today about consciousness, I didn't want to
 presuppose too much of my own distinctive views about the hard problem, about materialism or
 dualism. I think I did, in effect, want to presuppose some kind of realism about consciousness,
 that consciousness is real and not an illusion. But beyond that, I wanted
 what I said to be very much open to people with a materialist perspective. That said,
 I do think there is a big gap between our understanding of physical processes and our
 understanding of consciousness that makes these issues about consciousness much more difficult
 to assess. I mean, the problem of other minds is not exactly the same as the problem of explaining
 consciousness, but they are very closely connected. The fact that there is this gap
 between understanding physical processing and understanding consciousness makes the problem of
 which systems have consciousness much more difficult. That said, I think we've got a science
 of consciousness. I've always wanted to say that the existence of the hard problem is totally
 consistent with having a science, a well-developed science of consciousness, which may be in the
 first instance a science of the correlations between neurophysiological properties, computational
 properties, and consciousness. And that's really the science of consciousness I think we've got
 over the last 30 or 40 years. So what I was really interested in doing today was
 really much more length in the longer paper I wrote, was taking the kind of insights which
 have come from our correlative science of consciousness and applying those to
 language models. There are going to be further questions about the underlying
 metaphysics, but I think of all this as being very much, yeah, it is absolutely true that one
 of the things that makes questions about the distribution so difficult is the gap,
 which is the hard problem. I had a question. I'm Sayara. I'm an undergrad in cognitive science at
 McGill. I read Professor Chalmers' transcript of his talk at Euro-ITS about if LLMs could be
 conscious. Now, I thought of a research project where I could get the idea of the answer proposed
 about intentionality, or why I cannot have this intentionality. I thought this was similar to the
 idea that you've discussed that there's a cognitive aspect of consciousness, which is the ability,
 I guess, to represent things in LLM, representing real things in itself, kind of like how we have
 mental representation of physical objects in the world. Is that a question or a comment? Yeah,
 that's a comment. Sorry, there wasn't a question? No, like I added this comment now. What do you
 think? Like, would you make it draw similarities between John Searle's idea of intentionality and
 your idea of mental representation in a conscious being? Yeah, I want this to be very much
 consistent with, I mean, Searle has important work on perceptual and cognitive consciousness as a
 fundamental locus of intentionality. He thinks intentionality is ultimately grounded in the
 experience of perception and the experience of thinking. I mean, I'm a pluralist about
 intentionality. I think there are notions of intentionality that can be understood in
 computational and behavioral terms. But I do think, yeah, a lot of what people talk about when they
 talk about human-like intentionality does involve something that connects very strongly to the
 experience of both of cognition and of perception. Okay, Richard, you're on.
 Hey, thanks for a great talk and a discussion. So I'm wondering, do you all see any role for
 these sort of information processing ideas like integrated information theory
 in understanding large language models in their consciousness?
 Yeah, well, integrated information theory is extremely controversial. As people here will
 know, there was an open letter last year that described it as pseudoscience. I'm very interested
 in information theoretic frameworks for understanding consciousness. And way back in
 in my first book, The Conscious Mind, there was a whole chapter saying the key to understanding
 consciousness may be thinking in terms of at least to understanding physical correlates of
 consciousness may be tied to certain information structure. I look at the structure of a visual
 field or auditory experience. It's just very natural for me to understand that structure in
 some kind of mathematical or informational terms. So I'm very interested in frameworks that map
 physically embodied information to experientially bodied information. The question then is just
 whether there's some further conditions on that which information structures get to correspond
 to consciousness and up, you know, for better or for worse, to know these integrated information
 theory really does give you some some mathematical conditions here. I mean, it's a large despite its
 mathematical complexity, it's a largely a priori theory, I think of it as a philosophical theory,
 and I think there's not a whole lot of direct empirical support for its conditions. Nonetheless,
 I like it as an example of a mathematically precise theory of consciousness and its structure.
 And I think it kind of serves at least as a starting point that maybe other theories could
 could improve on. Cool. Riedemann, your turn. Yeah, thank you very much for an excellent
 talk. Can you hear me? Thanks, I can. Yeah, thanks. Great. And we have learned during the
 during the conference that that large language models in particular GPT does is nothing like
 one of the realizations of a Markovian process, whereby a string of k elements is used to calculate
 the probability of the k plus first element. This is possible with, as Chomsky and Miller showed
 in the 1950s. It's absolutely not, it's to a limited degree possible to model language with that,
 but it has its limitations. Now, the new thing is, and that might actually not be so new,
 that if we extend the k to an incredibly large number, to 50 originally and now to up to 40,000,
 then we are able to make very good predictions on the next word. But this, in a sense, is trivial.
 So how can we possibly consider to call such a machine conscious if we know about this?
 Yeah, I mean, I think the viewpoint you're articulating was everyone's viewpoint before
 these large language models came along. These machines are going to be at best predictions of
 the next bit of text given a whole lot of existing text. And maybe, just maybe, I mean, the original
 hope was they might be useful for understanding certain properties of language. Very few people,
 I think, thought they were going to be useful for understanding cognition and reasoning and so on.
 But by the same token, very few people expected that they would display the capacities that they've
 ended up displaying for whether it's mathematical tasks or game playing tasks or coding or
 explanation. I take it, so it's precisely given the very surprising fact that these very simple
 models, mathematically very simple models, display these totally unexpected capacities.
 That's what raises the question of whether they also have unexpected mental capacities,
 like reasoning, understanding, and consciousness. I don't think it's obvious that they do, but I
 certainly don't think it's obvious that they don't either. Okay, thank you. We run out of time, but
 Dave, are you going to stay for the panel? Sure. Okay, well then the people who have
 asked written questions will get their chance. Jean-Louis and Nora also will have their chance
 in the panel session. In the meanwhile, we're going to cut this out.
