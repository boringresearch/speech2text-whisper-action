 The first question is, like for example, every day we have a process that looks at a large
 collection of LLMs, pretty much all of the standard LLMs, and we are assessing how well
 does it do in generating Wolfram language code.
 We have a nice table, actually, we're about to start publishing these tables, it's kind
 of interesting actually.
 So there are two levels, first is it syntactically correct, second does it, so what we did was
 I wrote a book some years ago, Introduction to Wolfram Language, which has exercises,
 and the exercises have an English language description, and then they are, now you're
 supposed to write code.
 We have a pretty good and now tested, you know, gazillions of times, you know, testing
 system that says is the code that got produced, that got written by the student, so to speak,
 quotes correct code, okay?
 So we have good test for that.
 So now we ask, for these LLMs, do they produce syntactically correct code, do they produce
 essentially semantically correct code in the sense that it does the thing that was supposed
 to be done?
 So you can just look at those numbers, I mean, they get different percentages of success.
 But I think, you know, to this question about does it parse, does it turn into, you know,
 does it turn into Wolfram language code, that's a lower bar than you need, because it could
 turn into code, but the code just does something totally crazy.
 The question is, you know, when you ask the question, does the code do what the code should
 have done, the code does not in and of itself, the code will just do what it does.
 It is the description of what it should do, is that code.
 So to ask from the outside, did it do what it was supposed to do, is a different question.
 And you could, for example, you could say, I mean, if I take random cellular automata,
 and if my code defines a random cellular automaton, and I run it, and then I say, did that do
 what it was supposed to do?
 Well, that's not in and of itself a meaningful question, because it did what it was, you
 know, it did what the code said it should do.
 And now the issue is, well, was it the thing we wanted it to do?
 I mean, it reminds me in the world, a very different world, in the world of cryptocurrencies,
 there was this situation that you're probably aware of, this DAO thing that happened eight
 years ago, or something now, on Ethereum, somebody set up this thing, there was a thing,
 the Distributed Economist Organization, and its big banner was, this thing doesn't have
 humans in the loop, it just operates according to code.
 And it just, you know, its laws are code.
 And then somebody transferred $50 million to their account.
 And people said, but you can't do that.
 And the person said, but I just followed the code.
 But people then said, but you're not supposed to do that.
 But this thing was defined by its code, so to speak.
 So it's kind of a, you know, I think this is a question of did it do what you wanted
 it to do, is a complicated question.
 And I mean, in the end, what it wants it, you know, in the end, it becomes almost a,
 you know, it's very analogous to sort of an ethics question.
 Did it do the thing that you thought it should have done?
 And that becomes this huge network of, you know, how does it connect to every part of,
 you know, in the case of mathematics, good example of this, okay?
 Ramanujan, famous mathematician of the early part of the 20th century, discovered this
 result.
 E to the pi square root of 163, I think, is very close to an integer, okay?
 To most people, that would have been a random fact of mathematics, of no significance whatsoever.
 In fact, you know, I've even gotten postcards from people with similar near integer statements,
 which I don't think have any significance.
 But to Ramanujan, that thing became something of great significance.
 That became actually one of the standard ways to compute pi, among other things, and a bunch
 of other number theory.
 So this question of what took it from the random fact to this thing that is, quote,
 interesting is interesting to consider, because it was basically, I suppose in that case,
 it was the success of building kind of a tower on top of that result, I suppose.
 But I think that's a, you know, I mean, for language, I'd be curious whether, you know,
 is there a rating, if you said, I've got these sentences, and, you know, one of them is from
 the Jabberwocky poem, and another one is from whatever else.
 Is there some sense, can one imagine a rating of what is it?
 Is it interestingness?
 Perhaps it isn't interestingness.
 Perhaps it's, what is the thing that is, that goes from, you know, you know, I don't know,
 happy electrons, it's hard to make a nonsense sentence.
 Happy electrons kind of eat turtles or something.
 But, you know, that appears to be a nonsense sentence without further context.
 You know, what's the, I mean, okay, in this project that we're doing, the Symbolic Discourse
 Language project that we're doing, we will have a very precise sense in which sentences
 are meaningful or not, because they're basically being turned into something that is computational.
 Yeah, kind of following up on that, I was curious about the semantic grammar point that you
 mentioned in this kind of question about what is it that linguists may have missed and that
 language models seem to have picked up on.
 I think I would have thought of it as kind of the success of the distributional hypothesis,
 so going back to Paris and like Firth and this idea that, you know, you shall know a
 word by the company that it keeps, that that idea with language models seems to have turned
 out to be much more successful than I think people initially thought or predicted.
 So things like Adele Goldberg, the construction grammarian, has all these nice examples of
 to know the meaning of a word like red, you need to know that, you know, a red grapefruit
 is red on the inside, but a red apple is red on the outside, and red square isn't really
 red at all, and red hair is not really red either, but we kind of just call it red.
 So is that the kind of, that to me feels like the kind of knowledge that you're getting
 just from language usage and keeping track of these statistics?
 And it's the kind of thing language models seem good at.
 Is that part of what you would mean by the semantic grammar or is that something else?
 I mean something more precise.
 I mean, that's an interesting thing in its own right, but, you know, if you imagine,
 so for example, logic is an abstraction of language that tells you about sentences that
 make sense. In other words, I could make a sentence, think syllogistic logic, for example.
 I could make a sentence, you know, all men are mortal, Socrates is a man, therefore Socrates
 is a turtle. That doesn't, you know, that isn't a logic, I could have a better version
 of that, but so logic is a kind of an overlay on language that tells you which sentences,
 you know, potentially make sense versus which sentences just are syntactically correct,
 but don't, but aren't saying something that's true or right or whatever else.
 So my point is that I think there are a huge number of other kinds of constructions and,
 you know, there are pieces of, I don't, you know, I try to do my homework, but I certainly
 wouldn't claim to know every corner of the world of linguistics. But, you know, there
 are things, whether it's frames for verbs and things like this, where people have tried
 to sort of make the beginnings of a construction kit for what goes with what. But, you know,
 if you say, here's a motion word, and you say, here are place words, just like we can
 say here are nouns and verbs and so on, we could say, you know, a place verb, you know,
 you go place word, motion word, place word, that's a thing that can make sense. But, you
 know, place word, motion word, emotion word is probably not going to make sense. You know,
 or, you know, some other. So that's the kind of thing I mean. And I'd be curious, I mean,
 in, you know, there have been various attempts, I think, in the linguistic history to sort
 of set things like this up. And, you know, some of them have been very, you know, there
 was the semantic primes effort that was from the 1960s, I guess, that kind of got, you
 know, got mired in, which I thought was an interesting effort. I mean, there was also
 efforts in constructed languages, of which my kind of two favorite examples, which are
 the two extremes are Ithquil and Toki Pona. You know, Ithquil being the maximal constructed
 language and Toki Pona being a minimal one. I was very sad to realize with Toki Pona that
 if you didn't use languages like Finnish as the word roots, and used English instead,
 it sounds like baby talk, which is sort of interesting. And it's not totally surprising
 because it's got very simple, you know, structure and it's like, you know, and you're saying
 things like, you know, food want now, I don't know what, you know, you're simplifying the
 structure and it sounds a lot better if you use it's weird, you know, it's actual weird
 morphology and so on. Yeah, that's it's interesting that it feels to me like there've been a lot
 of these efforts, as you're saying over the years to translate natural language into these
 kinds of representations. And it turns out to be really hard in part because so much
 of what we say ends up not having these kinds of logical forms and even words that seem
 logical like or and and turn out in practice, you know, or sometimes is logical or sometimes
 it's very different or like, will is kind of like, you know, at future time to a lot
 of the time, it's more like a prediction or an assertion or different kinds of things.
 And so this kind of relating to what Rony and I were talking about in our session earlier
 is that kind of one of the things that I take away from the success of language models was
 the idea that, well, maybe if you can just avoid having to do that, like the actual bit
 of processing language gets easier because you got these fuzzy representations without
 having to turn them into these kinds of things. But then you run into this problem, then how
 do you turn them into parses to do, you know, what's the quote from Alpha? Yeah, right.
 It depends what your goal is, right? I mean, you know, one one point about sort of the
 failures of the philosophical languages, as they were called in 1600s, you know, my main
 conclusion about this is just because people haven't succeeded in doing it doesn't mean
 it's that hard. I mean, you know, and you know, like for Wolfram Alpha, for example,
 we were building a general question answering system and there was 50 years of failures
 in doing that. And so the question is, why were we able to succeed even given all those
 failures? And I only realized in retrospect what the answer to that question is, other
 than, you know, just having engineering capabilities and all that kind of thing. The real answer
 was because we had this computational language that we could hang things on. We were not
 trying to do abstract natural language understanding. And we were not trying to do we were we were
 trying to actually take something where we had a target. And by the way, an important
 point about that in the symbolic discourse language effort that we're doing. So here's
 the point, you know, people are working on this, I keep on telling them, forget the words
 in English and what they and the particular meanings they happen to have, like the word
 red, for example. You know, there is take take what we've done in Wolfram language. For example,
 we have the word plus the word plus in our language has a very definite meaning. It means
 you're adding things together. They're a bit like numbers and it has all these properties
 that X plus wise, Y plus X, all these kinds of things. We've we've hijacked the English
 word plus and we're using it for our precise meaning of a plus thing. It's useful to have
 that English word because when people are reading our language, it gives them some kind of cognitive
 hook. It isn't just, you know, function seven fifty two, which they'd never understand what
 it was. But then we're taking that word, we're hijacking it and we're turning it into some
 very precise computational thing. So kind of what we're trying to do in the symbolic
 discourse language is to do that same kind of thing across other domains of understanding
 of taking. And, you know, we've done this now for a huge number of domains and the things
 we haven't done. So so a typical example, you know, I want to eat a peach, which I was going
 to do actually. The you know, we have a lovely representation in Wolfram language of a peach.
 We know it's species. We know it's nutrition content. We know lots of things about the
 peach. We don't have a representation of I want to eat. And so the challenge is to have
 a representation of those kinds of everyday concepts. Now, one thing we'll do in turning
 things into the symbolic discourse language. If there was any poetry in what you said,
 it will be crushed out because it's going to be just this is the raw computational compatible
 meaning. And, you know, I think that's a this is let's be clear. This is not, quote, solving
 the problem of language. This is making something that is an engineering technological solution,
 which seems to have a bunch of applications. You know, it allows you let's say you write
 a contract. You can write a contract in computational language then. And that contract is automatically
 executable and so on. And that's, you know, that's potentially useful. But it isn't solving
 the problem of answering, you know, the question of, for example, how do humans generate language?
 You know, that's a different question. And I think I think this the the but the fact
 is I my impression, my guess is that there is this kind of there's this framework that
 is this kind of semantic grammar framework that neural nets have with huge effort. They've
 kind of adapted themselves to this. I mean, you know, one of the other lessons of neural
 net says, which people are learning now is takes a lot of effort to train it. But once
 you've trained it, you can start, you know, instead of using a precise real number weight,
 you can reduce it to a two bit weight or something. And the thing will probably still work. It
 takes a lot of effort to navigate through to this trained state. But once you're there,
 you can strip away a lot of lot of kinds of things. I mean, I think it's it's yeah, seems
 like Ava has something to say here. Go ahead. I was just going to comment on the fact that
 LLM is one of the big differences that they have compared to models just from a couple
 years ago is that they're trading on large amounts of code. And you made the interesting
 point that when we write code for it to be more human readable, we do tend to sort of
 name things with names that are similar to natural language in some way, even though
 they may be defined as like specific functions and therefore have very, very clear definite
 definitions. And I wonder what, whether or not the fact that we have now trained these
 LLMs on so much code and that that at least correlates with them doing much better, whether
 or not there may be a relationship there, if the fact that they have been suddenly trained
 on all of this defined and sometimes even grounded in the form of assertions and code
 has somehow helped them, then generalized to like their usages in natural language as well.
 Here's my guess. You know, people are really surprised that LLMs could do logic. My statement
 was LLMs learn logic the same way Aristotle did. That is, you know, imagine you were inventing
 logic from from nothing. You would say, I'm going to look at all these sentences, I'm
 going to see the ones that seem to make sense. Then I'm going to abstract from those. What's
 the structure of the ones that make sense? And I think that's what the LLM did too. I
 mean, the LLM is very poor at, you know, when you have a tower of things that you have to
 figure out, it's pretty poor at following that tower, not what it's good at. It's good
 at, you know, you ask it to do parenthesis matching, it will fail after a certain number
 of parentheses. And, you know, the deeper, the longer the look back in the transformer
 and the deeper the stack and so on, you can get it to go a bit further. But fundamentally,
 it's going to fail at a task like that. But, yeah, no, I think it's an interesting question.
 I mean, one thing we've been interested in is, you know, often language is the unique
 example of a language where it is a computational language that is intended, well, first of
 all, that attempts to actually represent a large chunk of the world, not just the particulars
 of how a computer works inside. And point one and point two, it was built from the beginning
 to be a language that could be read by humans as well as written by humans. And so that's
 a sort of special case. But, you know, we're definitely, we've been pretty interested in
 whether there is transferability from, oh, it read a lot of C++ code. Does that help
 it in writing Wolfram language code? We don't know the answer. I mean, we know that, I mean,
 an interesting thing is it writes pretty good Wolfram language code, which I find interesting
 because there's plenty of examples of bad Wolfram language code out there. And also
 the obvious translations from procedural programming languages, for example, would lead to bad
 code. But typically LLMs don't write bad code. They typically write pretty nice, compact
 functional code. So, I mean, that's a, but it's an interesting hypothesis that by seeing
 sort of the structure of, of code, that it's kind of like a version of, of seeing something
 like syllogistic logic of seeing these are things that fit together. I mean, look, one
 thing, okay, it's been a long time question of mine. Okay. Have to ask it with linguists
 and so on. I want to get a spoken version of Wolfram language and I can't figure out
 how to do it. Even though it is a tree structured language, it's context free language. It has
 the same, you know, you can look at its parse trees. They look just like human language
 except they're more structured, but yet humans have a really hard, I mean, I just don't know
 how to do it. I think there are probably markers and, you know, it will be a lovely thing to
 solve because there are a bunch of use cases, particularly in education, where you really
 want to be able to speak that code, but you want to be able to speak it in a way that
 people can do transcription trivially, so to speak. So I'd say, but it's sort of a,
 that maybe speaks to this question of transferability of, because, you know, regular natural language
 is speakable, one assumes. And, but yet, well, code generically has not been speakable at
 all. Wolf language code has the best chance to be speakable because it has actual words
 in it and things that you can use as kind of cognitive hooks. But I just don't know
 how to do, you know, F of, I can say F of X, right? You understand that. That's easy
 enough. F of G of X, H of whatever, it loses it very quickly. But I'm sure there's got
 to be a way of having, you know, just like in natural language, there are all these tricks
 for, you know, these markers of where you go in a sentence. There's got to be a way
 to do that. There's got to be a way of filling in fluff that represents those commas and things
 like that.
 Let me address the question to Caillou. Okay. You, you go in auto formalization, you go
 from English to, uh, to formal code. What, what Stephen is talking about is going from
 formal code back to something like English. Is that?
 Not really, not really. That's not, that's not what I'm talking about. I mean, that's
 a, that's a different problem. Well, now it's going from natural language to formalized
 language. Just like, I mean, it's the same problem, but a different use case from, from,
 you know, the, the quotes auto formalization. I mean, there's a different question, which
 is given a piece of code, have the LLM write a description of what that code does, which
 is reasonably good at, although, you know, people say, well, then that's a way to understand
 the code. But the truth is if you have a well-designed computational language, the code is probably
 the best way to understand the code. But, but I'm, I'm curious. The, do you, I mean,
 if you say, I mean, actually, so there's another case we've certainly looked at extensively
 is explain the proof basically. So we have in, in wolf mouth, uh, we have the show steps
 feature, which happens to be super popular among students and things. Um, although we've
 never done the kind of educational piece of terrorism of randomizing the steps. Um, we've
 always had to be the case that, you know, if you're a teacher, you can actually tell
 it's a wolf mouth for generated show steps. Um, LLMs do randomize the steps, although
 they often get the steps totally wrong. Um, but that's, that's a case where, you know,
 we've been interested in how do you take this computation and break it down for human explainability?
 And there'd been lots of people, I don't know, you, do you work at matter that where you
 work? Yeah. There are people that who are, who are interested in, um, maybe it's even
 your group. I don't know, but, but, um, uh, I think who've been, who've been, um, I mean,
 but a bunch of people have been using our tech to kind of explore these kinds of questions.
 I suppose what I've been doing is try, um, try to give the language model a form of proof
 and ask it to explain it step-by-step. Um, I think, um, mostly it's, um, I mean, maybe
 I would say maybe 50 or 60% of the case it's, um, it explains the step fairly well, but
 still, um, uh, it often gets a high level picture, right? But for a specific term in
 the proof, a specific operation, uh, I feel it's not, um, that accurate. And, um, uh,
 I feel like, um, I guess maybe it's that this, this problem also exists when you talk about
 how to get a spoken Warframe language. Uh, I feel like the programs are very compositional.
 You have all these parentheses, you have terms, compose them together. You have a very deep
 tree. Uh, although natural language such as English can also be compositional, you can
 have clauses, but usually they are never compositional to that level. Um, at least in the language
 commonly we commonly speak. Um, so I'm wondering what's your thought on, um, I mean, if you
 want to really do a spoken programming language, how do you deal with this?
 Well, I think it's, that's why you define sub functions, so to speak. That's why you
 define little functions and you put the little functions together. I mean, once you've, and
 typically that little function is going to say, you know, you're going to have a name
 for that function. Just like if you're saying, I will call this, you know, a, a plague, and
 then I'll go and talk about that. Um, I mean, I think, uh, you know, it's interesting to
 see there are things like pronouns, for example, which exist in natural language. We have a
 direct analog of that and most language we have in anonymous function and in pure functions,
 we have these hash signs that represent a thing that can get put into that slot. And
 so, for example, in natural language, we have some tricks with pronouns like gender and,
 and, and, you know, number and so on, which allow one to sort of multiply instead of just
 saying it's a this, you say it's a he, it's a she, it's an it, it's a, it's a plural, it's
 a they, et cetera, et cetera, et cetera. Um, and so, you know, it's interesting to try
 and see to what extent one can, you know, yeah, I mean, there are structures like that
 that we don't quite have complete analogs for in computational language, but it's sort
 of interesting the extent to which those are used as tricks for helping us more compactly
 understand natural language in a heuristic way, because if you say, you know, uh, I don't
 know, the cat chased the dog and it ran fast or something, you know, you don't know what
 that it refers to. If you say, you know, uh, Oliver chased the cat and he ran fast, then
 assuming we're in a, in a gendered world where Oliver is a he, we kind of know, you know,
 know what that refers to.
 Well, I'm curious and go ahead.
 That's pretty garden variety, um, question about anaphora and the way that it gets solved
 in language. Uh, I'm wondering about another aspect for, for, for Wolfram language. Positi
 was, was, uh, was, uh, created or evolved in order to handle spoken versions of things
 that if they were simply written down without punctuation, wouldn't be understandable in
 English either can, can, uh, can turning Wolfram language into spoken form, make use of prosodic
 tricks?
 Yeah, it's an interesting question. You know, I had a, there was an issue like this a number
 of years ago in Wolfram Alpha, when she's been intelligent assistants, it would often
 generate, you know, we'd, we generate a table of data like, you know, this is the population
 of this place, this is the population of this place, blah, blah, blah. If you read that
 table just straight, it's hard to follow and really boring. So I had this idea of what
 if you sang the table, so to speak, or what if you had kind of musical note, you know,
 musical indicators in the table and you made, you know, funny beeps and things at different
 times. We did some experiments, it never really caught on, but I think that's an interesting
 as a way to, at least on the output side, I have thought about that of, of, um, you know,
 to what extent you can, um, but actually we even did that. We, we did that and some spoken
 generated spoken results. Yeah. And in fact, that was even deployed in, uh, one of the
 intelligent assistants, maybe Syria, I'm not sure. Um, that, um, we actually had some,
 I mean, the problem was whether we could control the final deployment of the voice well enough
 by marking up what the results were. Um, but yeah, no, it's, it's a, it's an interesting,
 interesting question. And I, you know, I don't know. Uh, I mean, it, it, um, one isn't used
 to in, in, you know, the translation to prosody from written language. I mean, I don't know,
 you can presumably have little scan marks on top of the, on top of the text, but it's
 not, it's, uh, it's kind of a meta layer of, um, uh, of human presentation of those things,
 I think. Can you think out loud for a few seconds? So we know how this is striking you.
 You haven't said anything. Thinking out loud. Um, I'm tempted to say I'm a good large language
 model. I only speak when spoken to. So thank you for speaking to me. I'm thinking, um, again
 about cross-cutting themes. I'm coming back to something that was recurrent, uh, in Stephen's
 talk, which is finding those things that we care about. And I think that theme re-emerged
 in discussion, uh, again, Steve asking the question, you know, how would we know that
 this is sensible? Does it make sense to me? And by how would you quantify that and how
 would, uh, and how would you, um, formalize that if you, if you wanted to? Um, and it
 strikes me that that, that's a really important question and not only important, but very
 prescient for language. Um, and indeed relevant to the extraordinary coupling and synchrony
 we see, we see between users and large, large language models. So I'm, I was just wondering
 how I would answer that, um, and put it forward as a suggestion and invite comments upon it.
 It seems to me that there has to be some, um, again, coming back to a lot of the content
 or the discussion of the rouliade, there has to be a relational and observational aspect
 to how you score how much this thing makes sense to me and specifically this thing is
 something like me. I share some frame of reference, some common ground with this, uh, with this
 thing that's generating something that may or may not make sense to me. It strikes me
 one very heuristic way of looking at, at that is that if this sentence or if this code or
 if this message, um, affords a solution of the hermeneutics problem, then that will be
 one way of scoring, whether it makes sense to me and by the hermeneutics problem, I simply
 mean, can I infer what is meant by that message, that language, that sentence, that, uh, piece
 of code? Um, and I'm just wondering whether one could articulate that in terms of the
 information gain and the degree to which it resolves uncertainty in me about you. So to
 put that sort of interactivity, uh, in a game. So I'm, so I'm just putting this out there,
 you know, and I'm sure that that question could be answered from a number of different
 perspectives, but the, the, is there any way of being able to quantify and formalize, um,
 the exchange of information between two things, um, that rests upon a common commitment that
 may underwrite the information? I can give you a criterion. I'm not sure it's a very
 good one. Imagine you're doing mathematics and you're, you know, and one side is saying,
 here are some theorems that are true. And the other side is saying, well, what can I
 conclude from that? You could say that before you have those theorems, two things are not,
 you know, you don't know whether they're the same or not. As soon as you, you know, as
 soon as you get one of those theorems that shows equivalence, you've just connected those
 two pieces. So one criterion you could imagine is you've got a stream of, you know, one interlocutor,
 so to speak, is speaking a stream of theorems. And you can ask the question, to what extent
 does that, how, how much, you know, previously to the other, you know, to the other party,
 there were just a bunch of scattered theorems all over the place. They didn't connect together,
 but suddenly they're told the connection and then they start to join up. So you could imagine
 a situation where you could say, you could ask the question, given the stream of things
 that are said, how much joining up is going to happen? I mean, that would be an example
 of a, a, a criterion, you know, a meaning, a little bit of a meaning based criterion
 for what did you learn, so to speak. I mean, it's, it's, you know, the joining up of theorems,
 the knowing what's equivalent to what might not be sort of the whole story, but it's,
 I mean, you could imagine another, another thing you could imagine. I mean, you could
 imagine any kind of system like, like a Boolean expression with many variables. And you know,
 person A suddenly says, you know, P seven is true. And then that reduces the, you know,
 then, then the set of expressions that, you know, that then you have a simpler expression.
 I don't know. It's a, I mean, it's, I'm trying to think what else could you do in terms of
 understanding what, you know, you know, in a sense, what you're saying, the reduction
 of uncertainty is taking what were arbitrary values and, and instancing them. Like for
 Boolean expressions, it might be, you know, P could be either true or false, but now we
 know it's true. And I suppose that yeah, I mean, what one could just imagine some, yeah,
 I mean, even in that case, one could just imagine a space of possibilities. There are
 N variables, there's two to the N possibilities. We start supplying values of variables that
 will reduce the size of that space. You know, it, it isn't two to the N anymore. It's two
 to the N minus Q or something. I'm not sure how, yeah.
 Well, no, I mean, that fits very comfortably with the sort of information theoretic treatments.
 That winnowing down, you know, namely finding those, that sort of coarse-grained structure
 and which you could now indeed start to relate back to Komarov complexity and the like, or
 you could just articulate it in terms of, you know, good old fashioned information theory
 that you're reducing the entropy. You're maximizing information by reducing the entropy from a
 probabilistic point of view. That would be one way you could articulate it.
 Right. Although I think that, you know, in normal sort of Shannon information type stuff,
 the typical use case there is just, you know, block compression, so to speak. You're not,
 you're not asking sort of how did the program, you know, what could you run from a program?
 You're just asking, you know, after every Q, there's typically a U type thing. You can
 say there's a, there's compression at the level of, of, you know, picking only particular
 blocks out of all the possibilities. So I suppose the analogous thing will be the statement
 that, you know, if, if entity A says, by the way, after every queue, there's always a U
 that will immediately reduce the, you know, the number of possible possible sequences
 of characters that, that one might have to consider.
 Kyle.
 Yeah. I was just going to mention that I think there's a relevant in the linguistics
 and cognitive science literature, some relevant work thinking about the way that like semantic
 systems of particular spaces. So things like color words can be like, you can use an information
 bottleneck approach to look at the way that the kind of space of words used to describe
 a color grid can be thought of as exactly trying to maximize the informativity will
 minimize the complexity. And so there's languages that, you know, have only two or three color
 words, which is great and efficient. And they don't have to, you don't have to learn as
 many words, but you're right. It comes at this cost. And you can, there's a paper I
 was involved with, and there's been a bunch of literature on this that you can actually
 in information theoretic terms quantify for a given color system, you know, how, how is
 the informativity of the space of color words about, you know, the same color grid across
 languages treating off. And you've got these kinds of nice curves showing that, you know,
 languages which have more complexity in the system, use it kind of optimally to maximize
 the amount of information being conveyed. So that's what I'm curious about that. So,
 so if we, if we take, you know, a color space and obviously the, you know, the perceptual
 color space, you know, for us is like XYZ color space or something. It's not RGB. So
 we take, you know, our perceptual color space based on alpha receptors, and then we start
 saying, okay, this is red. And that's some region in color space that represents that.
 So is what, so one question would be, and it relates also to what one cares about because
 one might say these two shades of red are, you know, pink versus red is profoundly and
 distinctly different. And so we really care about making a distinction between these.
 Or you might say, you know, like in ancient times, you know, green and blue, you know,
 I don't know, in ancient Greek people say green and blue didn't have separate words.
 How does your, I'm really curious actually, how did, you know, so, so presumably you can
 ask native speakers of these languages what they consider these, you know, which, what
 word they use to describe these colors. And when you say there's optimality and what went
 on, what do you mean by that? So optimality across languages. The experiment is basically
 exactly that you show people a color grid. It's called the Munsell grid, which the colors
 are perceptually equally distinct in space. But that's a particular, I mean, it's a, it's
 a well-used, it's like Pantone colors and things, a well-used sort of graphic design
 color space. I'm not sure that it has physiological basis. I mean, it has, it has Western graphic
 designer basis. Or perceptual. I think it's based on, I don't know if it's physiological.
 Supposedly based on perceptual Western graphic designers. Yeah, possibly. Definitely. I mean,
 that's, I'm just making that point. It's a, you know, I know your, your job is hard. Your
 experiments are difficult to do. I'm just, I'm just adding an additional piece of complexity
 to it. Yeah. It's not just, it's not just designer artifact. And it's not the, the,
 the Berlin and K international color chart comparison. So that for all cultures, cultures,
 even if you look at the, at the chromatic spectrum, indifferently, some parts of it
 produce a bigger difference. The rainbow is the best way. If you look at a rainbow, you
 see these bands, you don't see a continuum. And that's not because of some designer. Yeah.
 And you see a lot of, on these grids, you see a lot of blue green. It turns out the blue
 green space is pretty big. Whereas red, like what we see as red picks out a pretty small
 perceptual space. But yeah, that's basically the experiment is to then ask people, you know,
 what color is this chip? What color is this chip? And then you can measure basically using
 like kind of simple information theoretic terms, the entropy over chip colors. And then you can
 measure basically given a set of color words in a language and the way that they're used,
 if I say a word, how much information am I getting about what chip is being discussed?
 So you have a collection of chips, which are essentially uniformly distributed in
 Munsell color space. So let's say you have a hundred chips and they're,
 you know, equally spaced somehow in this, in this color space. And now you're asking for,
 what would it mean? So, and then you're trying to define how close can you get to saying a
 particular chip? I don't understand how this works quite. I mean, if we were in a language where,
 you know, there was exactly one chip, which was called red and everyone agreed that chip was red
 and no one called any other chips red, that would be highly informative for that particular chip.
 But we have to average it for all the chips. And if all the chips were like that, we'd have a system,
 which was very informative, but also very complex because we have, you know, a huge number of color
 words to learn. So the question you're trying to, the goal, the task is effectively home in as
 close as possible on chip number 13 or something. Can you get, you know, if all the chips are
 numbered, so to speak, with these color words that you have, how much can you home in on the
 fact you have chip 13 or something? Is that, is that right? Yeah, that's right. And so then what's,
 what's the conclusion of the study? The conclusion is that you get an optimal trade off across
 languages. So languages which have more color terms and more complexity in the system that they've
 learned get the benefit, right? You could imagine learning more color terms, but not actually having
 the space or the color system more efficient. And if you plot informativity versus complexity,
 the real languages follow a kind of pre-optimal frontier and like a bunch of simulated systems
 you could make up just by, you know, simulating, let's assume these are the color words and this
 is how they're distributed, all look much worse than what real languages look like.
 Why don't you just take that space and, and, you know, chop it up into equal boxes? What does that,
 what does that do? Is that not, does that fail for some reason? If you chop it up into equal boxes,
 I mean, uh, that's a good question. I think. So in other words, each one would have the same number
 of chips in it. Each, each bucket would have the same number of chips. Yeah. It might not be
 physiologically possible to distinguish those chips correctly. The question in terms of the
 rainbow. You couldn't take the rainbow and chop it up into, into, into pieces on the basis of the
 physical parameters or on the basis of the psychophysical ones. It's not the same pieces.
 That's the point. It turns out also that there's an asymmetry between warm and cool colors,
 um, which is something. So warm colors, it turns out, um, we have a hypothesis in this
 paper that they tend to be more in the foreground. Um, so if you intend to be like objects and things
 we care about are often warm colors and blues and greens are, you know, sky and grass and background
 kinds of things. And so it's actually the kind of space picked out by red and yellow and orange is
 very small compared to the blue green space. Um, the question there, I don't remember what the,
 what the, um, uh, except the curves are, what the absorption curves are for the three kinds of
 photoreceptors, but that's, there are two, they're very close to each other, as I recall them one.
 That's much further away, which one, I mean, in this claim that reds, we care more about,
 would we not expect them that we would evolve a photoreceptor? Well, actually two questions,
 one, why wouldn't we evolve a photoreceptor that does better at reds if it's not chemically
 impossible? Question one, question two, I wonder what would happen to your experiment if you used
 a mantis shrimp. So mantis shrimps have like 15 color receptors or something. I don't know how
 they're distributed in the space of, in color space, but that will be a question of, if you
 look at the mantis shrimps color receptors, you know, any color receptor a mantis shrimp has,
 we presumably could have had in a first approximation. So if it has heavily distinguished,
 you know, it has a lot of fine distinction in the red color space. I mean, it also raises the
 question for us, an interesting corollary of what you're saying is, you know, we have three color
 receptors, cats and dogs have two mantis shrimps have 15, you know, various kinds of fishes have
 more in your theory. Can you explain why evolution has picked different numbers of color receptors
 for different critters? I mean, I think the general theory would be kind of neat, right? So
 there's some term here, which is like a neat distribution over the colors we want to talk
 about, which I guess for mantis shrimp, I guess it would be talked about, but use and distinguish
 in their mantis shrimp lives. And so probably that term would figure into it. I don't remember.
 It's a good question about, we have a, had color vision collaborators on this paper who would know
 the answers about like the actual cones. There's an exact analog of this in phonology also.
 What is it? Phonemes. Yeah. You mean, you mean what, but phonemes, I mean, we take a spectrogram,
 for example, and we can have lots of different forms in a spectrogram. And we, I mean, people
 have chosen to pick out, you know, a hundred different things that they call. Well, okay. So
 your point is imagine that we can physically with our throats produce all kinds of different
 spectrogram patterns. One question I don't know the answer to, maybe you guys do. I'd be curious
 to know if you imagine, well, it's a little complicated because by the time you've got an,
 you know, an S it's a big kind of mess in the spectrogram. And if, you know, if you've got some
 definite vowel that it's, it's much clearer. My question would be, you know, to what extent are
 you filling the space of possible spectrograms with what, you know, with the actual phonemes
 that get used and to what extent, for example, well, I mean, another question like this is when
 you look at letters of the alphabet, you know, and you, you put them in a feature space plot,
 you know, B's and P's are very close together in typical feature space plots. How come we ended up
 using letters which are not readily distinguishable? Presumably, you know, for phonemes, we want
 phonemes which are readily distinguishable in some way to our perceptual system.
 I'm curious, what, what, what do you guys know about this? I mean, for example, in the case of
 letter forms, that's a, that's a case where you can clearly see, I mean, you know, it's surprising
 that so many confusingly similar letter forms survive. It's surprising to me at least.
 Now you switch to letters. Let's go back to phonemes. There's an answer to the question,
 okay? Yeah, please. The, and in fact, Judith spoke about it in her talk. She said that,
 that babies are born with a certain number of selective feature detectors. They take
 phonel, let's say phone, phone space. And they, and they, and some of their select, they have
 selective detectors for some phony phones. But these selective detectors depend on whether you
 end up speaking a language that uses them. If you don't use them to make contrasts, and that's where
 these minimal pairs come in, then you lose them or they become weaker, especially in second and third
 languages. So my kids were young. There was lots of sort of things saying, you know, play the kids,
 the phonemes from all the languages of the world so that they will be used to these phonemes.
 I did try that. I don't think it had any effect. They have to save them. Yeah. Yeah. No, I realized
 that I knew that even at that time. And that was, um, but it's hard to even know whether they are
 saying them. If you're just a human who is not used to those phonemes, it's hard to tell if you're
 saying them. And in modern times, you can obviously get, you know, a computer to look at the spectrogram
 and say, are you saying that thing that I can't tell whether, you know, that I as a human can't
 tell you're saying, but, but to this question about, about, so you're saying we are born with
 certain phonetic feature detectors and evolved ones, probably because we have articulatory systems
 that are capable of producing some sounds and not others. Birds are better. Right. But I mean,
 it's like the, the Hubel Weasel experiment of the cats raised in an environment of vertical stripes
 that if you don't use the horizontal stripes detectors, then those ones, you know, then you
 lose those horizontal stripe detectors. And you're saying the same thing is true. The same kind of
 thing is true for phonetic detectors. So how many distinct, I mean, in the case of, of the primary
 visual cortex, we know something about what, at least of the things that we've recognized to look
 at, we know something about what feature detectors exist in phonetic space. Is there, is there
 something known about the diversity of detectors that exist? You can infer them. I, I, being a
 vegan, I don't recommend chopping up animals to figure out what their sound detectors are,
 but you can infer what they are from their performance. That's why I said it was psychophysical
 when they, if they can tell certain distinctions apart better, then they probably have a feature
 detector. What about other animals? What about a cat or a dog? It varies. It varies from species
 to species are different too. Let's say it again. What butterflies are different with colors too.
 So, right. But I mean, for example, for a dog, if a dog, you know, started in some, you know, country
 in Africa with some really funky collection of phonemes and it learned commands in that language
 and so on, it was used to that language. Then the dog is trans, you know, is, is, is moved to an
 English speaking country, for example, and has a completely different set of phonemes. Does the dog
 manage to, does the dog have the same issue that a human has of not being able to recognize
 distinctions between phonemes? In other words, would you guess that a dog has the same intrinsic
 phoneme detectors that humans do? And maybe, maybe it's born with the same ones, but there's
 another side of this coin. There's your innate feature detectors and your learned feature detectors
 and the dogs that move. Okay. This, this, this story is sort of nice and a special interest to me,
 but it's not, let's get back to the wider question here. And this is addressed to
 Rony, who has not been engaged so much in this yet. Tell me if it's a hallucination or confabulation
 on my part, that Chomsky had one interpretation of the function, if you like, of universal grammar,
 which was that it distinguishes the kinds of utterances that express thinkable thoughts,
 thoughts that you can think and where you can translate this into something that makes sense
 from those that are combinations, but that are not thinkable. Those that are UG compliant
 are thinkable. Those that are not UG compliant are not thinkable. And we're talking about a
 microcosm of that in the special case of, for example, mathematics, but tell me if it's apocryphal
 or true. That's not my understanding of anything that Chomsky said about universal grammar. My
 understanding of it and the details have changed, but the basic conception I think has always stayed
 the same is that universal grammar is simply the name for the innate programming language that we
 have. So we have a computational ability as part of our linguistic ability and part of our knowledge
 ends up being acquired and it looks like what we can acquire doesn't change at all between
 humans. So we're all born with basically the same ability. And so that ability, that is something
 that we can talk of as a shared programming language, which we're born with. And then we can,
 of course, argue about the details. And that's the point where different versions of the theory of
 Chomsky's and others. So different versions can look very different, but the basic idea is always
 the same. And that is that there is something that we're born with some programming language.
 Into that we acquire a language. And of course, there can be different ideas about how we acquire
 it. And that is UG, the universal here is because we all seem to be born with the same programming
 language, whatever it may be. And that's it. And calling it a grammar is a bit of a misnomer. And
 it's really a programming language. The grammars are the programs that we acquire. Kyle, do you
 agree? Okay, there's an operational version of what you're saying, which I think is kind of
 interesting, which is imagine you have LLMs with different, you know, very different architectures.
 Is it the case that, you know, in other words, can this statement about there is a universal grammar,
 there is something universal between humans, and you can learn this language, you can learn that
 language. The question would be if you, for LLMs, different LLMs can learn the same things. There's
 a question of are there things that an LLM cannot learn? Are there things that are, likewise, you
 know, for humans, for example, humans can't run code in their brains. Humans basically, I don't
 think anybody can run a non-trivial program in their brain. Maybe there's some savant somewhere
 who's capable of doing that, but it is certainly at best an unbelievably rare trait. It's not
 something that brains normally do. And, I mean, I guess, well, I mean, this question... I'm also
 sure LLMs are also not good at running code. Indeed. Compared to generating code. Absolutely.
 No, I mean, one wouldn't expect them to be good at running code. If one could make an LLM that had a
 recurrent stuff inside that one could train, they might be better at running code. But as it is,
 a feed-forward network, which just has an outer loop, is really not... It's great for cached things.
 It's bad for the computed things. I mean, this question about thinkable thoughts, you know,
 I suppose the linguistic philosophers would assert that, you know, for it to be thinkable,
 it has to be linguistifiable, so to speak. And I wonder to what extent... I'm curious to what extent
 you guys think that actually. That as, you know, in other words, are there... And it relates to these
 questions about sort of mathematics that we haven't cared about and all these kinds of things.
 Because are there thinkable thoughts for which we do not yet have words?
 Right? Because that will be... For example, there's a thing that's out there in the computational
 universe. It is in principle thinkable. It has a set of, you know, rules by which you can go from
 one thing to the next and so on. But yet, it is not something that we have attached human concepts
 and words to. So, I mean, my guess is that like the LLMs, for the LLMs, having a word is an important
 kind of hook that lets them have some grist, that lets them make progress. Without the word,
 it's like, as you say, they can't run code in their brains. They can't go multiple steps
 without having this anchor of now there's a word, which kind of would encourage the idea that
 the only sort of easily thinkable thoughts are ones that wrap around human language with words
 and so on. But I wonder what your view of that is. I'm thinking about a related problem. And also,
 I noticed that in Stephen's talk, when he talked about irreducibility. So, if you talk to many
 people who work on theoretical deep learning, they will say, "Oh, I work on how can we understand
 deep learning, or how can we understand language models?" But I'm always wondering, what do they
 mean by understanding a neural network? It sounds like the neural network has a lot of... It computes
 a certain function with a lot of neurons, millions of parameters. But it looks like in order to reach
 a human understanding, it's like we have some simple language that can describe what they're
 doing, which might look implausible for me. I'm not sure if that's... Well, what you're asking is,
 is there a natural science of LLMs? I mean, there are lots of things in the world that happen in the
 world. And the achievement of natural science is to reduce the things that happen in the world,
 the things about which we can have narratives that we can capture in our brains.
 Then you kind of assume the natural world is compressible. It has this low dimensional
 structure. But what if a network is just random? I profoundly think that the natural world is not
 compressible in that sense. I mean, this is one of the big things that's come out of a bunch of
 science we've done recently. The very fact that we humans sample only those parts of this thing we
 call the rouillard, this kind of limitable possible computations, the fact that we sample only parts,
 we have this sampling that is based on us being computationally bounded, that fact is what gives
 us the laws of physics that we have. If we were built differently, and we did not have that kind
 of limitation, we would conclude different things about the laws of physics. Paul? Go ahead. So yeah,
 I mean, I think the point I was worth making there is that it's some...
 Yeah, okay. Go ahead. I have to be a brute like that to keep things going. Carl, go ahead.
 I was saying that it's still interesting that the way we do scientific discovery is
 fundamentally shaped by some computational limit of our humans.
 Yeah, I mean, I think that what would it be... So this is a big question for neuroscience or for
 linguistics. Can there be a theory of neuroscience? Can there be a theory of machine learning? Can
 there be a theory of linguistics? In other words, is it the case that we can have, when we think
 about neuroscience, for example, is it the case that we can have a level of description
 that is intermediate between, oh, here's the electrochemistry of a neuron, and this is the
 word we say? Is there an intermeeting description? In fact, those pictures I was showing that I just
 generated a couple of days ago about machine learning kind of do not encourage that idea.
 That is, those pictures tend to suggest that the things that come out in machine learning
 are things where it just sort of happens to work. It isn't something where there is a narrative
 set of principles that we can kind of compress and say how it works. It's just, it happens to
 work. It's kind of a... And I think it's really not obvious whether there's a... You mentioned
 Chomsky. I had a discussion with him a few years ago in which this whole question about... He was
 complaining that the things I was trying to do were not answering the questions of the science of
 linguistics. And I guess my statement was, "I'm not trying to do science of linguistics. I'm just
 trying to build technology." And I then put back the challenge, do you believe there actually can
 be a science of linguistics, or is it so mired in computational irreducibility that there can be no
 science of linguistics? But anyway, it seems like there are various hands going up here.
 It'll be, Ronnie, in a second. But actually, before, when I said Carl, I was saying K-A-R-L,
 although I'm very happy to hear Caillou. Carl, say what you were going to say, and then we'll
 go to Ronnie. Yeah, it must be very difficult with Carl's K's and Kyle's to manage. And I was just
 going to submit, you know, is the natural science of large language models itself neuroscience?
 It strikes me that the questions that people are posing about large language models are exactly
 what I pose in my day job, trying to use things like brain imaging and the like.
 And if that is the case, then to understand a large language model is just the same as me
 understanding you. And how do I do that? I just do that by talking to you. There is no other way,
 I think. You're denying the possibility of a sort of true sort of underlying science of neuroscience.
 I don't know. I mean, I think—but you're right. For example, I've done experiments. I'm surprised
 more people haven't done these. I've done kind of lesion experiments on image generation systems,
 for example. It's kind of cool. You zap pieces of the neural net and you see what happens to the
 images it produces. It's kind of like, what is the alien mind of the neural net that started off as a
 humanized mind, but then you make it alien by zapping pieces of it. And it's really quite
 remarkable what you get. I mean, you get these bizarrely non-human images. And, you know,
 I don't know what—and actually, there's really cool—I mean, you can get, like, you know,
 whatever it's called, hemispherical neglect, the thing that happens in, you know, stroke patients
 and so on. You get analogs of that in these image generation networks. It's fortuitous you say that,
 because that's a nice entree into a question for Kyle, because I read Kyle's presentation
 as identifying some of the very basic principles that we use in psychology and psychophysics,
 you know, the principle of pure insertion, for example, donder subtraction methods
 to alternative forms. What are those? Just for my education, what are those?
 I'm not sure Kyle can answer that, but he'll know what I'm talking about.
 And just standard tools in neuroscience, or cognitive neuroscience, at least, or cognitive
 science. And I was just wondering, has anybody, and Kyle, do you want to then take it to the next
 level, which would be either doing brain imaging on large language models, or neuropsychology,
 which is exactly what Stephen was talking about, selectively lesioning an attention head,
 or three attention heads, or four attention heads, and seeing and using the lesion deficit model
 to start to get at the functional architecture in exactly the same way that we do in neuropsychology.
 So, it's fortuitous that Stephen actually was saying, you know, he's been doing that.
 That's exactly what we do, you know, in cognitive neuroscience when talking to
 neuropsychologists, after you've grounded things in the psychophysics and the psychology.
 And then you move on to brain imaging, which suggests that, you know, and of course,
 the wonderful opportunity with large language models, you can do invasive electrophysiology.
 You can stick your needles in to different parts of the architecture and start to explore the
 principles of functional specialization and integration, and probably get to the same level
 of understanding that people in cognitive neuroscience are, you know, wrestling with at
 the present time, but now in a synthetic or, you know, a silicon context, and using the power and
 the beauty of large language models to endorse that test bed, that sort of, you know, digital
 twins of us with a kind of validity that makes that exercise, I think, you know, a very useful
 one to consider. That's a picture, by the way, if you're interested. That's an image generation
 network, and that's showing what you get as a cat in a party hat if you zero, I think you're zeroing
 out here, particular parts of the network. So you see there, I mean, I'm curious from a neuroscientist
 point of view, what do you make of something like this? I mean, in other words, you can, you know,
 this layer, you know, how would you describe this? What on earth is going on? I mean, this layer here,
 you change that, you get a bunch of random pixels. Here you're getting, you know, I don't know what
 you're getting here. I mean, how would you, is this, this seems like it's relevant to what you're
 talking about? You know, absolutely. And you know, one example would be, are we looking at a disruption
 of the phonology? Are we looking at a disruption of the lexical representation, the semantic
 representation, the narratives? So at different levels, can we associate now the scale of the
 disruption induced by a particular lesion with the spatial location in the network itself? And, you
 know, that would be one way of, if you like, taking the principle of functional specialization
 in the neurosciences and applying it now to these kinds of networks. But it would depend upon you
 having some sort of coordinate system to make sense of this part of the network has a unique
 relation to the other part of the network. But of course it will, because the whole point of these
 networks, it's all about the relationships. It's all about the, you know, as you've emphasized,
 Stephen, it's all about the graph. So there is an architecture there. And then you're just looking
 for the morphisms between where you are in that architecture and the, say, the representational
 aspects that you're disrupting when you apply your lesions. By the way, I think this is a picture,
 if I remember correctly, these are sequences where you're progressively reducing the weights,
 all weights, just reducing the values. It's kind of like you're drugging the neural net.
 And I don't know what you're giving it. I don't know what drug does that to humans. I'm sure you
 guys know what does that. But psychedelics, 5-HT, 2-A agonists. You think that will reduce the
 weights. Because the other thing you can do is increase the weights. And I think this one,
 it kind of blows its mind. It kind of increases, this is increasing the weights. And it kind of
 blows its mind, more or less. It's kind of, I don't know how to interpret these things,
 but this is what, I think the image case I think is actually easier to handle than the linguistic
 case. Because I think it's more obvious, when you see a weird image like this, if I gave you a pile
 of random words, you would make nothing of it. But these images, we kind of, I think, are a little
 bit more able, with our perception, we're a little bit more able to say something about them,
 more so than we could a pile of random words. Okay, now K-A-R-L addressed a question to K-Y-L-E,
 whereas K-A-T-Z-R-R had raised his hand. Do you wish to continue the question to K-Y-L-E, K-A-R-L?
 If you'd like to answer it, yeah, I think, Kyle, you got the question. Are you going to use...
 Yeah, sure. Yeah, I think the brief answer is yes, I'm really excited about methods like that.
 This conversation just reminded me of the AI company Anthropic, a week or two ago, released
 a model that identified this part of the network in their model, Claude, that seemed to be
 responsible for the Golden Gate Bridge. And they kind of turned it up and released this chatbot,
 which no matter what you ask it about would kind of respond to you, but also just kind of start
 talking about the Golden Gate Bridge and relay everything it was saying to the Golden Gate Bridge.
 And it was a nice demonstration of exactly this kind of intervention. But yeah, for the kinds of
 linguistic questions I was looking at today, it was very much a behavioral intervention method. So,
 you know, mess with the training corpus, train a model, see what it does. I think you're exactly
 right. The next step with that is can we actually look mechanistically within the network? And there's
 all kinds of work like within linguistics in this space. So Chris Potts and his students at Stanford
 with Atticus Geiger are doing really interesting work on exactly that kind of causal abstraction
 work. And there's a bunch of other interesting work in that space. So yeah, I think there's
 going to be a lot of interesting stuff there. What is your intuition of how far you can go with those
 kinds of things? I mean, you know, neuroscience has proved really difficult to make these high
 level statements. We can now make more accurate probes in neural nets. Does that, does that feel
 to you like, oh, well, first of all, do you think there's going to be a grand theory? Question one.
 And how much does it help you to have more data, more internal data rather than just behavioral
 data, so to speak? Oh, yeah, I guess I'm dubious there would be a grand theory, but I think we'll
 figure a lot out. And I suspect we're going to keep finding a lot of these like abstractions
 and representations that we know and love from their work and linguistic theory and psycholinguistics.
 Like, I suspect we're going to find a lot of that there and it's not going to look neat and tidy and
 exactly the same as it was in our theories. But like those kinds of things are going to be there
 kind of because we think they have to be there. So things like hierarchical structure and subject
 verb agreement, like we don't, something that looks kind of like that, you know, you can find
 representations of. And it's, I don't think we're going to like, some perfect trees are going to pop
 up, but I think something. If it's short, say it. If not, Ronnie has been waiting a long time.
 Go ahead, Eva. Yeah, it was short. I was going to say to that point that I don't think it's that easy
 all the time. The Golden Gate Bridge example is a really nice one of a case where you can sort of
 find a part of the network, but because these networks are so interconnected, a lot of information
 is often kind of just so dispersed across the network that it's very difficult to actually
 identify something through these types of like model ablations. And I think that's also going to
 be true for a lot of, you know, these types of abstractions that we think are interesting and
 may be very difficult for us to identify them that way, which is why most work tends to be this other
 sort of behavioral approach, because it's so scary to try to figure out what, you know, all of these
 connections could be. The point made, Ronnie. So I had one general methodological question to
 Stephen, and that's regarding what I understood as the general pessimism about the
 computational reducibility of things. And I wanted to ask you what looked like
 scientific discoveries over the millennia fit into that view? So if things are irreducible, so why does
 it look like there is so much reducibility with probably plenty of stuff that we can't understand
 and can't have shortcuts for, but there are those chunks that were discovered. And so how do they fit
 in? Right. Good question. Then I had another question for, but. Okay. So to address that,
 because it's really interesting. I mean, the answer I think is interesting. One of the things you can
 theoretically establish is that within any computationally irreducible system, there will
 be an infinite number of pockets of reducibility. In other words, when there is this thing, which in
 general we say you can't predict everything, there will always be these particular things that you
 can predict. You can say of the rule 30 pattern, it will always have, you know, the edge will always
 look like this. You can't say everything about it, but you can say some things about it. So then
 science has consisted of finding these pockets of reducibility. In fact, our very existence, you know,
 it could be the case that nothing about the world was in any way predictable. It would be hard for us
 to be organisms of the kind we are if nothing was predictable. I think if I've understood Carl's
 theory of things, you know, Carl has sort of made a point of the fact that, you know, we, we set
 things up and we're built so that we can live in these predictable parts of, of, of the world,
 so to speak. And so what science I think is, is the finding of more and more pockets of reducibility.
 And the fact that there are an infinite number of pockets of reducibility tells us
 that science will never be over. Every invention won't have been made, et cetera. The question of
 whether the pockets of reducibility that we find are ones we care about is a different question.
 And that relates to Caillou's point about, you know, when a theorem's interesting and so on. Yes,
 there is a theorem that jumps from here to there, but we may not care about that. It's a, it's a
 piece of reducibility we may not care about. But yes, the, the existence of, in a sense,
 computational irreducibility is the proof that science will never end.
 Because there are these... Go ahead. Just a quick follow-up then. In that case,
 is there a reason to be more pessimistic about there being interesting pockets of reducibility
 in cognitive science than there are in the structure of materials or anything else?
 Well, that's a super good question. We don't, I think we have no idea. I mean, the fact is, okay,
 so in natural science, the pockets of reducibility are kind of where we live and where we build
 technology. There are certain aspects of natural science. Like, for example, let's take fluid flow.
 You know, smooth fluid flow, we have pretty good theories of. We use those theories in designing
 lots of kinds of things. Turbulent fluid flow, on the other hand, we don't have such good theories
 of, and we don't tend to use it in technology. We kind of have to avoid it. We have to do things
 around it. So in other words, these, in, in, in terms of our experience of the world,
 we are, when we do technology, we're doing that within pockets of reducibility that we found.
 And so, like, for example, here's an example. What is physics? You know, physics tends to be
 the study of things we can say about certain aspects of what the natural world does. There
 are areas like even fluid turbulence that are rarely talked about in physics because there's
 very little that the methods of physics can say about it. So, you know, I think what, what, um,
 you know, so now you ask the question, is there, let's compare physics and cognitive science.
 Which one has more pockets of reducibility? And one of the things I suppose about physics is there
 are parts of physics that will not have computational reducibility, but which we choose not to interact
 with in our life, in the world, in technology, and so on. In cognitive science, are we having
 our noses rubbed in the computational reducibility? Because the things we really, we need to know about
 are ones that have computational irreducibility. I mean, another good example of that is in economics,
 but it's the same question. There may be things that we can establish about economic systems,
 or we can have a general law, like general relativity or something, about economic systems,
 but it may be about an aspect of economic systems that we don't, as humans right now, care about.
 Some weird thing about the details of velocity of money in different situations that isn't telling
 us, you know, should we buy or sell the stocked kind of thing. So it could be that in cognitive
 science as well, that there may be theories, there may be grand theories that can be had,
 but they're about things like, oh, I don't know, to make it up, you know, some weird correlation
 between neurons in this place and that place and so on. And it's like, that's amusing, but we don't
 care. Oh, for example, a good example is EEG. It might be the case that, you know, we can have some
 theory that tells us about why, you know, brain rhythms are the way they are. But so far as I know,
 there's not a lot of connection between those brain rhythms and, well, there's some connections,
 but there's not detailed connections between those brain rhythms and a bunch of detailed cognitive
 behavior. So, I mean, in other words, we could have a theory of EEG. We could be very proud of that.
 That's a slice of reducibility, but it doesn't tell us about the things we're really interested
 in in cognitive science. And so I think that's some, but it's a very interesting question. What,
 you know, are there, you know, what's the density of pockets of reducibility? And are those pockets
 aligned with things that we care about in that field?
 Thank you. That makes a lot of sense. Thanks.
 Sounds like you had some other comment or something here.
 Yes, but I don't know how much time we still have.
 Three minutes. Okay. Yeah. So maybe not enough for what I want to do.
 Yeah. So maybe just a quick follow-up on what you just said. So I think the view from within
 linguistics has been starting from Tomski's early work, but continuing that there is or there are
 some interesting pockets of reducibility not stated in these terms exactly, but I think something that
 can be understood like that around certain things like certain properties of what can be seen as a
 programming language that we're born with. So, and certain other things, and then there are lots of
 other things for which maybe there is no good theory and that are left without analysis. And
 so I think the way that traditional or that theoretical linguistics traditionally
 partitions the things into what is considered interesting and what we would have perhaps wanted
 to like to understand, but we can't. Maybe there is no theory of that and it's treated as some kind
 of noise, even if it's not really. And I think that fits the kind of way that you were
 talking about pockets of reducibility, I think very nicely. So yeah, I like that perspective.
 No, I mean, look for linguistics, it is really, I mean, I'm very curious what pockets of reducibility
 there are. And I know there's been lots of controversy in linguistics about what's universal,
 what's not and so on. And by the way, with respect to LLMs, it is kind of disappointing
 that all you can do right now typically with them is ask them questions about what they think about
 things. In other words, it's disappointing. I mean, for example, I'll give you an example of one
 might not be obvious. We build software that in some cases now uses LLMs and we need to do software
 testing. And the question is, we're trying to do regression testing. Are the results that we get
 now the same as the results we were getting before? So how do you do that with LLMs? The only way we
 found to do it is to ask the LLM. So in other words, you ask another LLM, you say, given this
 output that has now come out, is it semantically the same as the output you got before? The only
 thing we found to do there is basically ask an LLM to make that decision for us, which is kind
 of a, I don't know, I find this rather unsatisfactory. But it's kind of LLMs all
 the way down because eventually the LLM might be uncertain, then you have to ask another LLM,
 how certain was that LLM and so on. And eventually just say, it's probably good enough and you release
 the software or whatever. Okay, with that, I think we can call this meeting over. I have to rush
 back to be able to upload them all so that everyone can see this, even the panel. I'll try to
 get up before the end of the day. There were very good comments in the comments section,
 but we never got to them. What I recommend to attendees is that if your comment really is
 urgent and important, please raise your hand. Don't just put it there because otherwise it
 lies on luck to get to it. I want to thank all of the participants and encourage you to come
 next week because it's not over till it's over. Nice to meet you all. Bye.
