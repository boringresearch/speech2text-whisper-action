 Thank you everybody.
 Thanks so much to the organizers for the invitation.
 My name is Daniel Stark, I lead a team at both McGill's, the Faculty of Medicine and Student Science Apartments at the KGBI Institute, and what we're generally doing which is to take large data sets with interesting innovative data science and technology.
 We have a classic question. And today, the topic is going to be mostly focused on large language models, because historically, we have seen a large increase in interest in the ecosystem since convolution, the learning roughly, since 2010 2012.
 What we're currently witnessing I think it's good to say that we can view that as yet another wave of momentum in the systems, thanks to large language models.
 And it started out to be as predicting the next word on the internet turned into phenomena of meaning in of emergence, which even the developers of these kinds of systems didn't necessarily expect.
 So, this is just a picture of our team, which is composed of technical people, computer scientists, engineers, physicists, where we're trying to work closely with the main experts as you'll see, to address exciting questions.
 So, for the particular context of biomedical medicine, which is particularly, particularly challenging. What types of requirements do we actually have for our machine learning platforms for our data science platforms.
 And the key surprise to many sometimes it's that it's not just the excellence performance in classification tasks in continuous or competition tasks, as this is typically the case in a lot of areas where machine learning and deep learning are played.
 And a huge, huge emphasis is actually on being able to say why a certain model is working, how exactly gets to a particular decision point so we want to introspect all models sometimes people say in a post-mortem biopsy kind of way.
 We want to unbox the back box. And it's not just this, what large language models are also very useful for is anything that's related to integrating information from multiple different sources, this could be another point to consider.
 Large language models are probably going to be useful for a lot of different areas in biomedicine and that is because we have text data in all sorts of parts of our health provider systems in health research.
 One instantiation is probably electronic health records. So there is all sorts of ways in which we could apply large language models in hospitals and hospital derived kind of data.
 The big difference now is the limit of scale that has massively decreased. So, why have we been doing so much better in terms of scale since very recently, and I would argue that it's the particular kind in which large language models are actually
 trained which is the semi supervised way. So classically we really needed careful, often very laborious and slow annotation of all the various data points that we bring to bear in our training data.
 But that has really changed with the transformer since 2017, the bigger models, because now we can't really train one of those systems on the almost the entirety of the internet, based on a lot of assumptions, that's around two trillion tokens, and we do not need to provide any supervisory guidance to these learning systems.
 They simply learn intrinsic structure in a way that we do not fully understand, which then leads to all kinds of emergent properties. So, the quick definition, the working definition that I'm going to go here by, or transfer learning is going to be, we try to extract intrinsic structure
 larger amount of data, but then transition carryover the particular learning solution to another area typically more poor in data to derive advantages.
 So you will see this can take all sorts of forms. But it takes special relevance in neuroscience medicine because despite the big data movement, I think it's fair to say that a lot of areas of neuroscience, a lot of areas of medicine.
 We do not have the amount of data that we would like, or I would personally argue, we do not have the amount of data that we would actually need to answer the kinds of questions that we have in a satisfying way.
 So, and the question is now, how can we use general purpose data that's out there, that has maybe nothing to do with a particular task in neuroscience medicine to carry over this intrinsic structure.
 This average internet mind, this extracted semantic world knowledge that the large language model has extracted to a downstream task for our advantage.
 We have seen an emergence of a large series of scaling studies, how these transfer learning phenomena actually occur.
 The basic economics of modern training has been that the larger make the modern state that the book, or more specifically here is here.
 We see a small plot from the Kaplan 2020 2020 paper, and this really investigated three main parameters, the modest size in terms of parameters, the data size, as well as the amount of compute budget that we have, what they found is,
 you do avoid overbidding, you do keep learning interesting coherent structure, as you increase the data and the number of parameters, the modest size at the same time.
 So, and surprisingly, despite the monstrosity large language model really turned out to be highly data efficient. So, we can typically comfortably train on two to 20 times more tokens, then we have parameters in a model so that is what I call data efficient here.
 Of course, there are recent other developments, we learned, just in Europe last year that maybe this is a function of metrics, perhaps we had emergent phenomena already before the current era of large language models, or maybe there aren't even real emergence phenomena or reasoning and
 understanding the Opti systems, if you will.
 And paradoxically, we observed that now large language models keep getting always smaller again so the recent realization really seems to be that we do not necessarily need always larger, always higher parameter values, if we have always more higher quality
 or perhaps that's actually enough to really learn high quality large language model systems.
 So, let's motivate why it's actually so difficult. Once we have a working large language model system to make sense of why it's exactly what.
 So, probably a lot of you have seen these kinds of representations, symbolic schematics.
 We have essentially three steps.
 And the first one being the input of betting so different from other machine learning systems, we really need tokens, we really need
 bigger words kind of input representations which don't need to be texted actually works with sequences in general, which makes large language models, of course, interesting for kind of biological sequences, such as DNA sequences, protein sequences, or
 transcripts in itself. The first stages, we derive an embedding so real value vector for text sequences in our input data. So that is still kind of straightforward, but really starts to get much more complicated the moment we get to
 the attention part of a transformer there.
 So, this can be presented in all sorts of ways I just want to point your attention to the fact that the transformer is really a series of linear operations so it's not necessarily the part that is most difficult to introspect so that makes it a natural
 starting point for any agenda of making sense of how large language models for important reasons. So as the first approximation, you can think that what is the A matrix here, the confidence of the Q and K
 systems, the queries and the keys, is that this very much pertains to how information flows from a source token to a target token.
 And it is the O and the B matrices instead that are really about controlling which kinds of information are actually transferred between this source and target tokens.
 So that's just one way of simplifying a little bit how we think about the attention layer of a transformer block. And I said yet a little bit different, you can think of it as a QK circuit, as some call it here.
 That is because the K and the Q matrices almost act together and in the same vein, the output ability of the circuit in turn also acts in concert. So why don't we think of them as joint operations to hopefully make progress towards mechanistic interpretability.
 So we have another time, all of these, as you can see here, are linear operations. However, even just these attention circuits, it has been quite difficult to make progress understanding them.
 So I already mentioned most of these points, and that is despite the fact that the last part of the transformer block, the NLP, the modular perceptron, part of it, the fully connected forward neural network, it's even more penetrable.
 This part usually makes up roughly two thirds of the overall parameters of a large language model.
 And we know from the deep learning literature before large language models that it is quite difficult to really decipher, disentangle the meaning, the concepts, the unique aspects that are encoded, transported, represented through these neural layers.
 So, overall, we have several of those in a block, a series of those, typically, and that makes it even more challenging because the beddings that we learn in the first form they may actually, they trickle down with their tension, emphasis to the next layer, and so on.
 So, this stands in contrast to how we tried to operate with the last generation of natural language processing systems. So I'm thinking of a lot of Word2Vec, Word2MergeApp 2010, there we had a big emphasis on so-called semantic embeddings or semantic embedding vectors.
 So, a notion, a meaning of the word typically, as you see here on the bottom of the slide, the form that it took typically is that you represent them not as a string, but as a re-evaluated vector in a high-dimensional semantic coordinate system, where the position of this multi-dimensional coordinate system denotes the meaning.
 And that, in turn, means that we can actually do semantic vector operations once we have continuous vector spaces, which already hints at your salient spending semantic spaces in general.
 So, we can actually go and compute what semantic embedding vectors are how similar to each other, or how distant from each other, and this allows us to do all sorts of interesting downstream operations.
 However, this way of thinking is less prominent in the large language model literature at this point from my reading of the literature.
 So, as you can see, there's four sorts of operations. We can reframe, reconsider parts of the system in all sorts of ways, and I provided a few examples of what form this can take.
 So, back to the MLP layer, where we have the actual non-linear process integrations. Part of the reason why this is so challenging is what's called fully semantic. So, we do not necessarily have clean neuron interpretations.
 A single neuron in the MLP does not necessarily correspond to a single concept or a single notion, as we would like, that are directly accessible to us humans. That's apparently not been the case.
 And so we end up with a compressed representation of what is potentially a way of encoding more concepts or high level features, as some people call it, then you have hidden dimensions in the MLP part.
 So, it may be possible to reverse this compression, and there are ways to do this.
 For example, based on sparse overcompletionary learning, where you try to unpack hundreds and hundreds, potentially thousands of underlying features or concepts that MLP layers are representing, which you would not guess just looking at the integrations of the hidden neural network layer.
 Yet another way of thinking about it is perhaps there are so-called privileged bases into which we can rotate the hidden layer activations, such that the axis of our new core system, into which we want more MLP activations, may much more naturally align with concepts of notions that are accessible
 to us humans. So one strategy to make progress in algorithmic language models, which have been tried, and showed some initial progress, would be re-expressing the high dimensional activations in the MLP into another space.
 But the question is, how do we actually find these other spaces?
 However, if we can do this, we could overcome the curse of dimensionality, which is also a problem because for the investigator, it is typically a challenge to just even visualize the intermediate processing or modeling products that emerge as need to get information.
 How it flows through the system. So it's not just that, it's also quantitatively measuring interpretability has been difficult to automatize. So that means it is difficult, it requires human intervention, and it's slow.
 And so it would be great if we could come up with metrics that allow us to just approximate the degree to which we can generate human interpretable representations for what is going on in the end appealing layers of LLM's.
 So this takes me to why we are exactly using LLM's in all particular products that I want to talk to you today, and that is autism.
 So, second part of autism is a part of psychiatry. And as a lot of people know this very quick, there's a very special discipline inside of medicine, because we do not really necessarily have hard currency markers to detect whether a certain child,
 a certain individual with a suspicion of autism actually does carry this diagnosis or not. We cannot draw a block, and we don't have any other established trusted biomarkers that we could use to really be sure.
 So, what happens is, you really need people trained after medical school for years and years and years to specialize in finding out in encountering when they encounter somebody, okay does that individual in front of me, need the criteria for the diagnosis of autism or not.
 And just the definition of autism, to make things worse, has been a moving target so autism as a concept probably emerged around 1900, and it initially described a particularly strong disinterest in reality so it started to be described as patients,
 individuals who tend to be in their own heads, a lot of people. However, the definition of autism changed a couple of times.
 For example, in the 60s and 70s, it was changed yet another time, there was a much bigger emphasis on deficit and social interaction, which were not directly at the site, such as the beginning of this research area.
 And there was not this big emphasis on ignoring reality. So, what's important for this talk is that it's roughly for more than 50 years that in clinical practice and research, we see a strong emphasis on
 deficits in social interaction, that these clinicians trained for years, take a strong indicator that certain individual probably needs the criteria.
 Because a big question that is also lurking here is that of specificity, we are oftentimes not sure how to exactly come up with the criteria or combinations of criteria to define the diagnosis of autism, where subsets of these criteria do not also qualify for other candidate mental health divisions.
 So, the debate of specificity has been a long and vexing one, and we have not really converged on a particular interpretation of truth or consensus.
 But maybe we should really keep working on it because autism is a very widespread condition so more than 1% in our societies probably meet the criteria for autism.
 And that, of course, leads to needs in society that we need to meet to help these individuals. So, nevertheless, at this point in time, the clinical judgment, the clinician talking to the patients is still the best and perhaps the only way to certify this particular diagnostic label.
 How does this exactly work.
 So that's maybe clear to some of you, but perhaps not to everybody. And we have diagnostic manuals, which codify the criteria and the explanations of how diagnosis like autism should be done.
 And those are shared across hospitals, across healthcare institutions. And this is really how medical doctors also communicate with each other, how they have actually come to the conclusion to talking to the patients, often several times, not just in a single session.
 And it is very much based on lists of criteria that are laid down in these agreed upon incumbent diagnostic manuals, an important one of which is called DSM-5.
 So let me kind of explain a little bit of the broader context as well. So, over the last decades, we have seen a lot of improvements in what you can call science and truth biology.
 And of course, once we had these new technologies, a lot of people thought, okay, let's revisit autism. This time, with all this information, with this depth, we should be able to make a difference in autism and identify reliable markers of disease that would allow us to automatically detect autism.
 So one of those high throughput technologies that emerged and became widely available and cheap, less than $50 per subject, is common variant genetics or genome-wide association studies.
 However, these genome-wide association studies, they only led to explain a few percent, around two percent, of the observed variants in the autism phenotype.
 Well, if you use this to perform automatic classification, we probably cannot very much exceed 70 percent of a classification accuracy.
 But whether or not somebody carries this diagnosis or not, we, from a different perspective, we did not find autism-specific genes using genome-wide association studies, so forth.
 So another high throughput technology that investigators in biomedicine have certainly used is brain imaging using magnetic resonance imaging, MRI.
 And there's all sorts of flavors of resting state, the so-called, sorry, of MRI imaging, one modality, resting state imaging is particularly well-liked than one of them.
 But even teams, very professional machine learning, with years of experience in doing high throughput brain imaging and very carefully conducted data science workflows, they could not get much beyond 65 percent accuracy in classifying whose brain scan belongs to somebody who has been neurotypical and whose brain scan belongs to somebody who carries the diagnosis of autism.
 So this is the reason why still today, even after decades of research in these areas, with the large datasets from primary biology, we have not really found a way to circumvent clinical judgment as the main means to diagnose this individual.
 So what's going to be important for the motivation of this project is that we take a drastically different approach.
 Given the lack of progress in pathobiological approaches, especially GWAS brain imaging, we try to take a different route and we just go back and recognize a human individual talking to the patient is still the best case, the optimal scenario to come to reliable diagnosis.
 So why don't we apply machine learning to the diagnostic process itself?
 So can't we machine learn using our language models, the clinical decision making process of how a healthcare professional is exactly reasoning about whether the person in front of them should carry the diagnosis of autism or not, to revisit also the question of autism specific traits.
 So we don't do this in isolation. As I alluded to at the beginning, our tendency is to partner with the main experts in a particular project.
 Here in this case, we work with the University of Montreal seasoned psychiatrists who have been seeing hundreds of patients. So the hospital where we draw the data from actually sees more than 100 patients per year.
 So that's one out of three days there's a new patient brought in. And so it's fair to say that they have certainly seen a huge variety of clinical cases in this area.
 So they helped us through the process and the particular input data that we use for our LLM project here is the actual reports of the clinicians that they put into the system every time they see a particular patient.
 As I mentioned before, it's not usually possible to come to definitive whether or not somebody carries the diagnosis of autism just based on a single session.
 So we actually only use the text manifestations of how the clinicians document for themselves and for others what exactly happened when they met a certain potential patient.
 So we have more than 4000 of those reports. This is from more than 1000 patients each patient to be showing up several times.
 And typically the clinicians really need three, four or six, even six sessions to really be sure and come to a diagnosis based on the clinical symptoms that they have observed.
 So this is what they write down. We digitize, anonymize this, we use OCR to turn the analog information into digitized information.
 We had to do a number of cleaning steps to then finally design our language modeling architecture, which builds on a pre-trained LLM, this is what I alluded to at the beginning, that has already been trained on hundreds and thousands of text sources that by themselves do not necessarily have much to do with psychiatry.
 With psychiatry or neuroscience or any other aspects of what we are trying to do. The magic is that we have learned in general semantic representation.
 It could have been a world model, if you will, that we now bring to bear in this clinical data set by fine tuning it on the workforce that we actually observed. So you'll see that we add a specialized attention mechanism on top of it.
 But I'll get to this at a later stage. What's important to realize here at this point already is that we do not have a lot of data.
 We have thousands of reports, which are maybe one or two work pages long each, which by machine learning standards can certainly not be judged to be a massive data set. However, this amount of data is actually already enough.
 I mentioned that LLMs are quite data efficient, sample efficient, and this is really what we can confirm because fine tuning on this very small set of reports allow us to perform a series of baseline classification models.
 So that means we take the text information that we have in the reports, typically by a background voice representation, so only counting a sort of histogram, how many times does a certain word occur in each of the text reports actually using the sequence structure.
 However, using simple classifiers, we only got to 65%, which actually in fact is already as high as what brain imaging is able to do.
 Then we try classifiers with higher volume linearity, that's random forests, also based on the background voice representation, we get another almost 10% of gain, and last generation language, natural language process models took back.
 That's a Word2Vec variant.
 We also did not really get past 75%.
 However, after fine tuning on these reports, we can see that we are almost indistinguishable from 80% accuracy.
 So that then already means we can build a model, although it has millions of parameters, we can make it work on a small subset of reports, which is common in the medical community.
 We did not train this one from scratch, just based on these data, we really need this internet scale pre-training step.
 However, then we can show that we can outperform a series of standard model choices.
 We kind of hide a number of steps that we try to take, which did not lead us to make progress in terms of profitability of why this language model did perform so well, or how it exactly comes to the conclusion of using elements of the clinician's documented reports to reach accurate classification results.
 The individual report, in the end, belonged to an individual who actually does carry the diagnosis report, or only was suspected to carry this diagnosis.
 For example, we started off like many, of course, at the Word level.
 In the attention layer, this gives us attention matrices that have the size of vocabulary on both edges.
 And as you can imagine, this is pretty difficult to work with, to visualize. Nevertheless, we try it.
 We tried, for example, decomposing the states with supervised and unsupervised techniques. This did not really lead in our experience to sheer progress in making sense of what exactly is coherent in the attention scores at the Word level across hundreds and hundreds of reports, and in telling the patient groups apart.
 We also tried to perform part of the only versions of PCA to the attention scores of the hundreds of reports.
 Same observation. We got a lot of results, but we didn't really get clarity on how exactly
 the language model is taking its decision. And we also tried graph analysis, network analysis, because we were thinking that the Word level representations, they are naturally occurring in semantic webs where
 words are more or less close to each other. And perhaps we can represent this information in a directive acyclic graph and use the arsenal of graph theory tools to come to a more useful representation of what is going on in the internals of the model.
 This did not really lead to a lot of progress either. Similar but different, we also tried the nearest neighbor analysis. So we identified tiny intended words that the attention had flagged, plus which language or behavior, and then we tried to find their closest neighbors in the semantic space.
 And you see some examples here, but we made the same experience we had a lot of data, but not necessarily better on what is going on in the model.
 Well, there's all sorts of explanations that we could now
 make.
 However, I'm just going to summarize it this way by saying, perhaps we are not studying this phenomena at the right unit of investigation. So maybe we need to change the unit of inference, the unit in which we are trying to make sense from these high dimensional results.
 So, this is where we had the idea.
 Why don't we go back to the earlier insights in the NLP community, which is working with semantic embedding vectors.
 So, every word can be represented as a real value becker, but by aggregating what works in a certain sentence in a certain report, we can actually build a sentence level representation.
 And so we changed our approach and tried to go all in on the idea of building a specialized attention mechanism that actually provides more direct interpretability by acting at the single sentence level.
 And as you'll see, that's not the only way in which we can draw advantages.
 It actually has a whole series of advantages. So just to kind of recap, because this is one of the most commonly asked questions here for this project.
 We naturally obtained semantic embedding representations. We have 300 dimensional semantic spaces, we have 300 real values that represent where in the semantic system of DLL from the pre-training and fine-tuning, how exactly it instantiates the meaning.
 But we can meet across those to get a sentence level representation, which then in turn could be fed into a classifier to tell them apart. This is exactly what we did.
 So we extended the usual pre-trained framework that we started with, and we walked in an interpretability module, which is the red one here on the right, a single hand attention one, which operates in sentence units.
 Which then in turn is fed to the classifier, which achieves the reports.
 So the first question would be, what does large language models do? Is that actually related to the classification, or is the large language model just being run for random things?
 So what we see here is the combined energy layers, the activations from this system, whereas with the two principal component analysis, which means that now being in the sentence level, each dot in this lowering embedding representation that you can see here corresponds to a particular sentence.
 We performed coloring in this 2D space of the most explanatory dimensions as a function of the known diagnosis, but the diagnosis here in red group did not really influence the fitting of the PCA model itself.
 So what you can see is that in blue, the autism cases, which were later confirmed to actually be the genuine case of autism by the healthcare professionals and medical doctors, they actually do cluster in a particular subspace of this plot received for the riot.
 Most years turned by the most explanatory directional variation in the data.
 So this already shows that we can for a fact, learn an autism aware semantic space. So whatever it is what the large language model is doing, it has a close relationship to the outcome that we actually care about.
 And if you look at the dots, there's some overlap, but it doesn't seem to be too much overlap. So how can we try to dissect this even further?
 One way, the classic way is so-called surrogate models. They do not try to recapitulate the exact mechanics of the model fitting that happens in the actual model.
 We consider here that this may just be too complicated. Instead, we use a simpler, more interpretive model, in this case just the linear model, to mimic the predictions that the actual model, the large language model is doing.
 So even if the large language model is wrong in certain unseen reports, we will fit the linear model to actually mimic this behavior.
 So what we see is that we can fit layer-wise prediction models for each of 12 different layers of the overall large language model.
 And we see even in the earliest layer that we get around 75.75 area in the curve performance. And so already at the earliest stages of this model, we can discriminate the patients at a fairly high rate.
 And it gets better and better with every layer, with the 12th layer reaching performance up to 0.96 area under the curve.
 So having the deeper model is an important conclusion here, for effect actually does lead to better prediction performance. So we want to have a nested multi-layer LLM to really extract as much information as we can.
 And what's coming out of this 12th layer is then actually the input for, as you saw, for our specialized attention.
 So here's an example of the attention scores. What it looks like in one particular report, because, as you know, you get a different attention score matrix for every input observation.
 So this is why you see here example reports in each of these three plots.
 Notice that we do not have words on the axis anymore. This is what this usually looks like, because this is the result of our specialized attention module, where we now can see how does the large language model actually assign relevance, not to single sentences even, but to combinations of sentences.
 How does the large language model actually look at the reports in this particular case, with all the sentences that make up one particular report, to come to the conclusion whether or not this report goes through somebody with autism or just was suspected with autism, but it was ruled out.
 So I will just quickly mention that we do indeed see all sorts of words and notions and concepts in the sentences that are select here in the groups that have high relevance to what is discussed in the clinical field as relevant aspects in coming to a conclusion about this diagnosis.
 So, we use these highly attended sentences to go further and map out some of the most relevant words. So let me just emphasize again, those are not work level analysis.
 The trick from our project is that we represent everything as a sentence notion. However, once we know the most attended sentence with each of the reports, we can actually go and carry out just put the statistics on the ratios for certain word of most attended highlighted words between who ended up to carry the diagnosis and who didn't.
 So you can see that here at the top of the list, where it's letters flapping, that's the motor movement fingers, there are three top discriminators that occurred up to 20 and more times more often than the most attended sentences in the reports of the autism patients, compared to the suspected
 results. So there's all sorts of ways in which we can now look into the machinery, what's going on.
 But the common question for scientists is probably, how do you validate those results? How can we know that what you find there is actually real and not just artifacts of carefully conducted data set exercise.
 This is where the jacket me came up with the idea of performing an external validation in the following way.
 So, the machinery community tends to try to resolve modern interpretability, in my opinion, somewhat isolation of everything else that exists.
 So what we thought is that maybe in this particular application domain. There is a way to integrate already existing knowledge that is not part of what input output mapping function of a vitamin LLM.
 And they can actually provide us insight into what is what the model is doing. So dimension, you know, the DSM five diagnostic manuals. That's the catalog, this is the enshrined system that clinicians are using every day on planet.
 So, wouldn't it be great if we can relate the gear and items, which were not actually measured by themselves in any of these patients.
 And maybe actually bring this in contact with what is going on in the internets or. So, but briefly what's important for the interpretation is that we have two big clusters in the DSM systems for the diagnosis of autism.
 And the most important take home message was that the A section has this classical strong focus on social impairments, which has a big, big emphasis on how we study, diagnose and treat autism until today.
 Whereas this B section also contributes to the diagnosis, but has a much bigger emphasis.
 So first off, we tried to quantify in this semantic vector embedding operations sort of way, verbal descriptions of each of the established DSM criteria that I just mentioned.
 How do they actually relate to our autism aware semantic space? What we see is, we can actually map them into the sentences again, one dot B one sentence in our reports, we can view we find that it's actually the B criteria.
 So the section that's not social behavior related, it's those that are in the subspace in blue, which is actually leading to the diagnosis spot.
 So it is not the case that what is currently most emphasized, the social impairment indicators, which is A123, those ones are actually at the opposite end of the group.
 The way we quantify the relationship of DSM-5 criteria and semantic embeddings was cosine similarity. So of course, cosine similarity has been used before in NLP to quantify the similarity of semantic representations.
 But the point is here that we could not necessarily have done this at the work level, the way in which these elements are typically used in medical space, neuroscience, and so yet another way to look at this is that we can actually now take each of the seven DSM criteria.
 We revisit the semantic embeddings of the leading sentences, the most discriminatory, the most attention receiving sentences in each of the reports, and we can now quantify how similar is it in meaning to the known seven DSM criteria.
 And let me just say again, the trick is really that the DSM criteria have never been measured in these patients or in these clinical settings. However, we found a way to introduce them into our modeling pipeline, so that we can perform an asset test against established knowledge.
 So, a value of one is very similar semantic meaning, zero is there's no relationship, and minus one is the opposite. So we get this for the most attended sentences in both cases, the non-artisan groups.
 And as you can see, again, we have the criterion B1, B3, B4, the non-sourcing criteria, which are actually most similar to the autism diagnosis.
 We do not find it that our leading sentences are the most useful for autism diagnosis, but those are actually very semantically similar to the healthy ones.
 We confirmed these results using latent discriminant analysis, so we actually rerepresented each report just as the seven similarities of the leading sentence with established seven DSM criteria.
 So we only had seven numbers instead of the whole semantics of the reports that we initially had, and we could reach pretty high prediction accuracy in the ISN reports of around 0.98 area in the literature.
 So this shows that our large language model actually looking at from different perspectives.
 And with all sorts of ways to try to peek into what our refined, large language model is doing, not at the word level, but at the sentence level.
 But I would claim that we found a number of interesting ways to sneak peek into it.
 And the core conclusion is that we cannot confirm this 50 year old idea that there is a very strong emphasis on social deficits in what is the most important in coming to an accurate positive diagnosis based on the data that we analyze.
 So, instead, it really turns out that the stereotype repetitive behaviors, special interests and sensory behaviors that appear to be much more discriminatory, at least in how the large language model and tense to the semantic units that we find in
 text reports from collections on the actual settings, where they come to these groups.
 And that could mean that maybe we have to readjust some of these gold standard manuals, such as the DSM file.
 There's all sorts of ways in which we could take this to the next level.
 One way would be to a little bit de-convolve now the NAP activations of our final element even further.
 We saw from the PCA space that there is certainly structure that is closely related to the diagnosis.
 However, we could use further decomposition techniques, especially sparse over complete dictionary learning to try to see are there really perhaps thousands and thousands concepts or features that are actually instantiated in the key parts of all LLMs that contribute to the successful diagnosis.
 We could also perform all sorts of follow-up analyses from the attention at the ablation studies, if we have multi-cut systems, we can perform all sorts of modifications of pre-output layer activations, and so on and so forth.
 I sometimes describe this kind of a pictorial analysis, and both of these ways that I just mentioned, to the final privilege basis one and forms of ablation studies two, you could also look at this from the context of optimization.
 This is a functional training epochs looking for jumps or non-smooth transitions from one training iteration to the next one.
 One of the interpretable takeaways is it's not just about prediction accuracy for biomedicine and neuroscience. We are one of these areas where it is extremely important to come to conclusion also about how exactly did the system come to its prediction.
 From the AI perspective, this is of course extremely relevant for the AI safety debates, so any progress in the LLM spendability realm probably translates to benefits in the AI safety problems as well, and I hope I convinced you that sometimes it helps to simply re-express the problem, rotate the problem into a different space,
 recast what we're actually trying to find to make specialized versions of our architectures that actually have those sorts of downstream benefits, and I hope I convinced you that we did use external information to really validate what we have found, and the internals of our LLMs did align with established diagnostic criteria,
 and we could automatize these comparisons at scale without human intervention, which, as I mentioned at the beginning, is one of the weaknesses of this whole LLM spendability field at this point.
 So finally, some fire placement. If you're interested in these kinds of reflections, feel free to check out all this paper. With this, I thank you so much for your attention, and I think the organizers for inviting me.
 I can't say thank you to Zanillo, but I will say thank you to Jack. There are some questions already in the discussion.
 But I'll ask one, because I have a feeling that you hear this one a lot, or you will hear this one a lot. If you were not studying autism, but brain imaging.
 Clearly you wouldn't say I want to hear what clinicians say about these images. You want to use the images as the data. Is there any possibility of that in the autism scale?
 If you were here in the morning, it's the same thing as what Friedemann said about cross validation with real brain data.
 Yeah, well, I think, is this working? Yeah, okay. Yeah, I think, are you sort of getting at the aspect of, you know, using multimodal information using all this it's grounding. Yeah, I think for sure I think that would be, we actually had a few questions about this before of using things like brain imaging, and also sort of other biomarkers in conjunction with the text information to get more robust.
 Sort of predictions and interpretations and I think, yeah, I think that'd be really useful. It's just we don't we don't quite have the data set to do that at this point, but I think if you know if we want to collect that kind of data set I think it would be super helpful in this context.
 Okay, here's, here's some of the questions that came in. Everybody here is welcome to ask questions everybody out in the two other lands, the attendees and the panelists as well.
 So, should I read the question. Yeah.
 The first one here is could sentence level representation be performed on verbal production of the child, rather than the interpretation of the question. And could this help for their focus on that, which are stereotypical utterances or lack of neurotypical utterances.
 Yeah, so this is also another question that we get frequently is. So we're training on the clinician observations directly.
 Whereas you know maybe we could actually train on the, on the actual speech patterns of the child or the individual.
 Yeah, and I think, I think that would be, again, we don't have the data for that. At this point, but I think that would be, you know, super helpful to maybe even integrate things.
 I think that would probably give us even better ability and performance for sure.
 Yeah, maybe I should go to the next or.
 It's great. Yes, go ahead.
 But why could you come up here.
 I don't know what the what they were talking about like using the child's, like, speech patterns to diagnose all this I was thinking like I movement and I, like, I contact that would be like, you could track it.
 Like, you could have a contribution model to like, you can have like frames of the video, and you can analyze within the model.
 Yeah, yeah I know that that's that's another suggestion that we got for sure is, is incorporating eye tracking, because I think that from what I understand I'm not, you know, totally familiar with it but from what I understand there's a decent amount of data
 available on eye tracking and autism. So it would be very interesting to interpret that as well.
 So maybe just going back to the question here. So the same commenter asks single sentence necessarily the same one when it's.
 Okay, okay. Well, they asked a single sentence slide seems to mirror content analysis based on a dozen. The ADR assessment variables could we not train on data analysis directly from those scoring tools.
 Yes, again, that would be. That's something we're looking at as well because we do have the data for that.
 But it's just trying to incorporate it into into our pipeline. We can make it work exactly but. And also there's a lot of missing data in that in that analysis but we definitely like to do that as well.
 Sure, so I think one of these ones. So the next question here is, are there any models working on the diagnosis of ASD with adults have acquired some compensation skills with regards to social skills, although nonverbal assessments.
 Section A, the ADOS module four questions tend to flush these out.
 This is a good question. So we worked the demographic that we were working with mostly was children so I think there's a median age of around five.
 So this is really the data that we have is coming from children but again, there's sort of another aspect of diagnosing adults with autism, which, you know, carries with it, different sort of set of questions and things that clinicians look for.
 So yeah, I mean, again, we don't have the data set for that but it would be very interesting to look at that as well. For sure.
 I guess I'll go into another question online here and really slowly and enunciate.
 I am mirroring Stevens question when asking if we could not use a dose slash the AD are directly. So the gay studies are becoming relevant. Yeah, and again, that's, that's something we would definitely like to work with, if we, if we have have the data paired with the commission
 observations as well. So this is really interestingly this is one of the first data sets that we've come across as a proprietary data set that we've collected in Montreal.
 There's not a lot of.
 Now, the thing is, of course there's tons of health records available that we could mine potentially if we get access to that.
 But this is sort of a, you know, a preliminary investigation into using these clinical observations.
 So, this last question online here is, does the models accuracy vary as per the functioning of the patient. So how does the model fare with high functioning individuals. So that's a really good question we actually didn't look at that.
 It's sort of hard to assess you know which individuals are high functioning and which ones.
 So, the data that we got we didn't exactly have no direct variable for that. But I guess you know we could go back and look at the reports and sort of potentially put them on a scale of you know high functioning to low functioning, instead of
 just giving a binary prediction. So that could be interesting avenue to explore.
 But yeah, that would be interesting to know if the model itself.
 I would I would imagine that yes that is something that's hard to.
 It's hard for the model to disentangle. If the individual is, you know, very high functioning in many areas, there are less signals in the text that actually point to autism whereas sort of the lower functioning ones it's much more concrete you know they have
 trouble with speech they have a very obvious repetitive movements or ticks. So that's much easier to pick up obvious we're in academia and half of the room is asperger's.
 Yeah, I mean so I will say these. So these individuals that we, we study were much would be more on the,
 the lesser functioning side just because they were referred to a specialty office and clinic just to get, you know, a more formal diagnosis. So obviously if there was, you know, a lot of doubts, and we probably would not have them.
 We take care of our own, we don't send to clinics.
 Other questions.
 I'm here from there. If not, if not I have a last question for you. You could, you could speak of of LLM research is hearsay research because words are words and all you're getting is hearsay this said this and people said that
 they said the same word with that word. So if we call that hearsay research, you're actually working on another hearsay body. And the question is, isn't it time to also, for example, when you're looking for other medical records, instead of autism
 or maybe it's not there, there are not many direct measures for autism but if you went into cardiology, you'd surely have a lot more data to use than just what the clinicians say about their data.
 No, you're definitely right. I think this, the idea of grounding is very important in the medical domain for her.
 We just wanted to tackle something where LLMs are immediately applicable, which is autism because there's, there's simply nothing else we can use really, other than the clinician judgment and the question observations that are all hearsay.
 Exactly. So, so we wanted to actually directly look at that hearsay and say, you know, what are the aspects of this hearsay that lead them to diagnose.
 It's a terrific study. In fact, it's time for us to applaud you for the both the study and for your
 Thanks.
 Bye.
 [BLANK_AUDIO]
